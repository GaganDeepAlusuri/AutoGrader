[
    {
        "SID": "5007507",
        "Name": "addalapoornachand",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## Linear Regression model\n# ### Import Modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(42)\n\n\n# ### Load Data\ndf = pd.read_csv('/Users/poorna/Downloads/sample.csv', index_col=False)\n\ndf.head(10)\n\ndf.shape\n\n# ## Address any NaN values that need to be dropped from the dataset\n# Count the number of missing values for each feature.\ndf.isna().sum()\n\ndf.describe()\n\n# ## Plot the scatter plot \nplt.scatter(df['x'], df['y'],color='red')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('scatter plot: X vs Y')\nplt.show()\n\n# ## Conduct your Train-Test Split BEFORE standardizing the data\n# The Train-Test Split should be conducted before any data cleaning that is based on calculations of the data. For instance standardizing/normalizing data.\nfeatures = df.drop(columns=['y'])\ntarget = df['y']\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\nX_train.columns\n\n# ### Scale the data\n# For our final step in data preparation, we scale the data. We will use the StandardScaler from sklearn.preprocessing to scale the data.\n# scale the numeric features\nnumeric_cols = ['x']\nscaler = StandardScaler()\nscaler.fit(X_train[numeric_cols])\nX_train[numeric_cols] = scaler.transform(X_train[numeric_cols])\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n\nX_train.head()\n\n# ### Fit the model\n# Finally, we fit the model. We will use the LogisticRegression model from sklearn.linear_model.\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults.head()\n\nintercept = model.intercept_\nslope = model.coef_\nMSE = mean_squared_error(results['actual'], results['predicted'] )\nr_squared = r2_score(results['actual'], results['predicted'])\nprint(\"The Intercept value is {}\".format(intercept))\nprint(\"The slope value is {}\".format(slope[0]))\nprint(\"The MSE value is {}\".format(MSE))\nprint(\"The R_squared value is {}\".format(r_squared))\n\n# calculate the root mean square error\n\nfrom sklearn.metrics import mean_squared_error\n\nprint(f\"The root mean squared error for this model is {mean_squared_error(results['actual'], results['predicted'], squared=False):0.3f}\")\n\n\n# ## Use of KNN\n# ### the number of neighbors to consider can be calcluated by taking square root of the sample size.\nn_neighbors_val = X_train.shape[0]**(1/2)\nn_neighbors_val\n\nknn_model = KNeighborsRegressor(n_neighbors=11)  # Using 11 neighbors \nknn_model.fit(X_train, y_train)\n\ny_pred = knn_model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr_squared = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r_squared)\n\n# ## Preprocessing the data\n# ### First I have checked the shape of data, and if there are any NAN values in the dataset. As there are no NAN values there is no need to handle this.\n# ### There is no need to address misspelled words or even encoding is not required as there are no categorical variable.\n# ### Then I have split the data into test and train and scaled the input train data inorder for all the inputs to be on same scale.\n# ## Linear Regression:\n# ### Intercept (Bias): The intercept value is approximately 47.22. In the context of linear regression, this represents the predicted value of the dependent variable (target) when all independent variables are zero.\n# ### Slope (Coefficient): The slope value is approximately -0.47. In linear regression, this indicates how the target variable changes for a one-unit change in the predictor variable.\n# ### Mean Squared Error (MSE): The mean squared error is approximately 716.14. This measures the average squared difference between predicted values and actual values. Lower values indicate a better fit of the model to the data.\n# ### R-squared: The R-squared value is approximately -0.004. R-squared measures the proportion of variance in the dependent variable that is predictable from the independent variable(s). A negative R-squared is unusual and suggests that the model fits the data worse than a horizontal line.\n# ## KNN Regression:\n# ### Mean Squared Error (MSE): The mean squared error for the KNN model is approximately 883.42. This is the average squared difference between predicted values and actual values. Lower values indicate a better fit.\n# ### R-squared: The R-squared value for the KNN model is approximately -0.239. As with linear regression, a negative R-squared is unusual and suggests a poor fit of the model to the data.\n# ### Both models, linear regression and KNN, seem to have performed poorly on this dataset given the negative R-squared values. Negative R-squared values typically indicate that the models perform worse than a horizontal line, suggesting that they are not appropriate for describing the relationship between the predictors and the target in this dataset. Additionally, the mean squared errors are relatively high for both models, indicating significant deviations between predicted and actual values.\n# ### It's important to investigate further, potentially considering other models or reevaluating the data, as negative R-squared values and high mean squared errors may be indicative of issues with the data or the chosen models.\n",
        "Points": 95.0,
        "Comments": "Great job on this assignment! You have successfully completed all the necessary steps. You imported the necessary libraries, loaded and explored the data, handled missing values, split the data into training and testing sets, scaled the data, and fitted both a linear regression and KNN model. You also calculated the RMSE for both models and provided a thorough discussion of the results. However, you imported some libraries that you did not use in your code, such as SimpleImputer and various metrics from sklearn.metrics. Make sure to only import what you need. Also, remember to include a title and brief introduction at the beginning of your notebook. Keep up the good work!"
    },
    {
        "SID": "5113587",
        "Name": "ageerharikrishna",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ### import the package's\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# # load the dataset\ndf=pd.read_csv('sample.csv')\n\ndf.head(3)\n\ndf.info()\n\n# No null values in the data set so we can proceed with model training\ndf.describe()\n\n# visualize the data\nx=df[['x']]\ny=df[['y']]\nfig=plt.figure()\nax=fig.add_subplot()\nax.scatter(x,y,color='red')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_xlim(0, 120)\nax.set_ylim(0, 120)\nplt.show()\n\n# split the data for training and testing and set the random state to 42. split data into 80(train) ,20(test)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nmodel=LinearRegression().fit(X_train_scaled,y_train)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_test, y_test,color='red')\nf.scatter(X_test,model.predict(X_test_scaled),color='blue')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# A plot to show model prection on test data . the plot shows model is failed to predict the actuall values . it's not the best fit for the data.\nresults = pd.DataFrame()\nresults['actual'] = y_train\nresults['predicted'] = model.predict(X_train_scaled)\nresults.info()\n\n# Calculate the R-squared scores for the training and testing data\nr2_train = r2_score(y_train, model.predict(X_train_scaled))\nr2_test = r2_score(y_test, model.predict(X_test_scaled))\n\n# Print the R-squared values\nprint(f\"R-squared (train): {r2_train:.3f}\")\nprint(f\"R-squared (test): {r2_test:.3f}\")\n\nmse = mean_squared_error(y_test, model.predict(X_test_scaled))\n\n# Print the Mean Squared Error\nprint('Mean squared error:', mse)\n\n# The linear regresion model with mse of 716\n# # KNN regressor\ndf = data = pd.read_csv(\"sample.csv\")\n\nx=df[['x']]\ny=df[['y']]\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n\n# In this model we are assuming number of neighbors as 2\n# ---\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train, y_train)\nknn_model = knn.predict(X_test)\n\n# Calculate the Mean Squared Error for KNN regression\nmse_knn = mean_squared_error(y_test, knn_model)\nprint(f\"Mean Squared Error: {mse_knn}\")\n\n# Calculate the (R2) score for KNN regression\nr2_knn = r2_score(y_test, knn_model)\nprint(f\"R-squared: {r2_knn}\")\n\n\n\n# This plot shows prediction made by model vs  test data. we can observe from the plot model is under fit.\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_test, y_test,color='red')\nf.scatter(X_test,knn.predict(X_test),color='blue')\nf.set_title('knn Regressor')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(x, y,color='red',label='Actual')\nf.scatter(x,knn.predict(x),color='blue',label='model_predection')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nfor i in range(1,20):\n  knn = KNeighborsRegressor(n_neighbors=i)\n  knn.fit(X_train, y_train)\n  knn_model = knn.predict(X_test)\n  # Calculate the Mean Squared Error for KNN regression\n  mse_knn = mean_squared_error(y_test, knn_model)\n  print(f\"Mean Squared Error: {mse_knn}\")\n\n# Calculate the (R2) score for KNN regression\n  r2_knn = r2_score(y_test, knn_model)\n  print(f\"R-squared: {r2_knn}\")\n\n\n# started a loop for n.of.neighbors in range from 1,20 to find out the best one\n# ## Analysis of the model\n# For KNN regressor, the MSE is approximately 797 and  the r squared value is approximately -0.117. This suggests that the KNN model is not able to capture much of the variance in the data , model is not fitted well.\n# For linear regression, the MSE is approximately 716 and  the r squared value is approximately  -0.004, which is very close to zero.This shows model not fitted well.\n# Both the models we perform have hight mse which shows model is unable to predict the values and error increses so mse value increase.\n# The R squared scores for both models not satisfactory, indicating that neither model is able to explain much of the variance in the data. This shows that both models are failed to capture relationship between data.\n# This model could  be improved by adding more data or fits good with other models.\n",
        "Points": 85.0,
        "Comments": "The student has done a good job overall. The notebook structure is clear, all necessary libraries are imported, and the data is loaded and explored. The student has also done a good job of preprocessing the data and splitting it into training and testing sets. The student has fitted both a linear regression and KNN regressor to the data and evaluated them using the RMSE metric. However, there are a few areas for improvement. The student has used MinMaxScaler instead of StandardScaler for post-split data preprocessing. Also, the student has not discussed their selection of k value for the KNN model. Lastly, the student has reloaded the data before fitting the KNN model, which is unnecessary. Overall, the student has demonstrated a good understanding of the concepts but needs to pay attention to the details."
    },
    {
        "SID": "5009045",
        "Name": "alavamsi",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ###  Assignment 4\n# # Linear Regression\n# ## Ala_Vamsikrishna\n# ## U91394366\n# import neccessary libraries\nfrom sklearn.linear_model import LinearRegression\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\ndf = pd.read_csv(\"C:/Users/Vamsi/Documents/Week 5 data mining/sample.csv\")\n\ndf.head(10)\n\n# checking if there are any topos, and if so, then fix this issue.\ndf.isna().sum()\n\nX= df[['x']]\ny= df[['y']]\n\nX_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=0)\n\nlinear_regression=LinearRegression().fit(X,y)\n\ny_predict_linear = linear_regression.predict(X_test)\n\n\nMSE=mean_squared_error(y_test,y_predict_linear)\nR2= r2_score(y_test,y_predict_linear)\n\nprint(\"Mean_Square_error = \",MSE,'R2 =',R2)\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X, y, color='red')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title(\"Linear Regression Test\")\nplt.tight_layout()\nplt.show()\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X, y, color='red')\nax.scatter(X,linear_regression.predict(X),color='blue')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title(\"Linear Regression Test\")\nplt.tight_layout()\nplt.show()\n\nknn_reg=KNeighborsRegressor(math.floor(math.sqrt(len(X))))\n\nknn_reg.fit(X_train,y_train)\n\nyprediction = knn_reg.predict(X_test)\n\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,y,color='red')\nf.scatter(X,knn_reg.predict(X),color='blue')\nf.set_title('KNN Regression')\nf.set_xlabel('X')\nf.set_ylabel('y')\nplt.show()\n\nMSE=mean_squared_error(y_test,yprediction)\nR2= r2_score(y_test,yprediction)\n\nprint(\"Mean_Square_error = \",MSE,'R2 =',R2)\n\n# ### Hence from the mean sqaure error of the both models we have less MSE(Mean Squared Error)for Linear Regression, Hence for the given data Linear regression model fits better than KNearest Neighbour Regression Model\n",
        "Points": 90.0,
        "Comments": "The assignment is well done. The student has imported all necessary libraries, loaded and explored the data, preprocessed the data, and fitted both a linear regression and KNN regressor to the data. The student also discussed the results and the performance of the models. However, the student did not use the StandardScaler() for post-split data preprocessing and did not avoid fitting the test data. Also, the student did not discuss their selection of k value for the KNN model. The overall presentation of the notebook is good, with readable code and clear explanations."
    },
    {
        "SID": "4983693",
        "Name": "asireddyvivekanandareddy",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Assignment 4\n# ## Import the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# # Step 1: Load the data\ndf = pd.read_csv(\"./sample.csv\")\n\n# # Step 2: Data Exploration\ndf.head(6)\ndf.describe()\n\n# ## Check for Train and Test\ndf.shape\n\ndf.isnull().sum()\n\ndf.columns\n\n# # Step - 3: Data Visualization\nplt.scatter(df['x'], df['y'],color='blue')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter Plot - X vs Y')\nplt.show()\n\nX = df[['x']]\nY = df[['y']]\n\n# # Step-4: Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ny_scaled = scaler.fit_transform(Y)\n\n# # Fit the Model\nlin_reg=LinearRegression().fit(X_scaled,y_scaled)\ny_prediction = lin_reg.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='blue')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='red')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n#Extract the Intercept (b0) from the Linear Regression Model:\nb0=lin_reg.intercept_[0]\nb1=lin_reg.coef_[0][0]   # Accessing the coefficient directly without an index\n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X,Y):.3f}\")\nerr = mean_squared_error(y_scaled, y_prediction)\nprint(\"Mean Squared Error:\",err)\n\n# - The R^2 value generally ranges from 0 to 1. But, we received it as -3.654. \n# - This means that there is either some issue with the data or our model is not suitable for the given data.\n# - Moreover, the mean squared error is almost equal to 1 which is very high. So, our model is not suitable for the data.\n# # KNN Regression\n# - As we got negative R^2 value while we were trying to fit the data into linear regression model,let's try fitting it into KNN K-Nearest Neighbours regression model.\n# - I am putting the k value as 10.\nknn_reg_model= KNeighborsRegressor(n_neighbors = 10).fit(X_scaled,y_scaled)\npred_val = knn_reg_model.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='blue')\nf.scatter(X_scaled,knn_reg_model.predict(X_scaled),color='red')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nmse = mean_squared_error(y_scaled, pred_val)\nR2 = r2_score(y_scaled,pred_val)\n\nprint(\"Mean Squared Error:\",mse)\nprint(\"R2 Value:\",R2)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='blue')\nf.scatter(X_scaled,knn_reg_model.predict(X_scaled),color='red')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='green')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# Comparison of Models:\n# - When we check the mean-squared error of linear regression model and knn model, both of them were 0.99 and 0.94 respectively.\n# - Both of them are high which means that is not a good sign for our prediction models.\n# - When I was testing the data with lower k-values, I was getting positive value for the R^2.\n# - Keeping in mind the negative R^2 value for linear regression, I don't think there exists a linear relationship in the data.\n# - Also considering, the positive R^2 value for the knn regression model, I would prefer that knn model would have a better performance over the linear regression model on the given data.\n# - Considering the high values of mean squared error, I would also infer that we don't have enough data points to predict the underlying shape of the model.\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, and visualized it. You also correctly implemented the train/test split, data preprocessing, and model fitting. Your discussion of the results was thorough and insightful. However, you did not set a random_state for the train/test split, which is important for repeatability of the results. Also, you did not use the StandardScaler() for post-split data preprocessing. Keep these points in mind for future assignments."
    },
    {
        "SID": "4030725",
        "Name": "bakerjennifer",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Assignment 4:  Linear Regression and KNN Modelling\n# ## Jennifer Baker U05341559\n# # Step 1: Import Necessary Libraries\n%matplotlib inline\n\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport math \nimport matplotlib.pylab as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\nplt.style.use('Solarize_Light2')\nprint(plt.style.available) # this will give you a list of the styles available in your version of matplotlib\n\nnp.random.seed(42)\n\n# # Step 2: Load & Clean Dataset\n# ## Loading the Data\ndf = pd.read_csv(\"sample.csv\")\n\n# ## Exploring the Data\ndf.head()\n\nfig, ax = plt.subplots(1,1) # create a figure (fig) that has one plot (ax) in a 1x1 grid\n\n# plot owners in color C1 with shape 'o', and nonowners with color C0 and shape 'D'\nax.scatter(df['x'],df['y'])\nplt.show() \n\n# ## Checking for NAs\ndf['x'].isna().sum()\n\ndf['y'].isna().sum()\n\n# ## Split the Data into Training & Test (Validity) Sets\nfeatures = df.drop(columns=['y'])\ntarget = df['y']\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n# ### Scaling Data, Skipping Imputing as unnecessary for this dataset\n# Also a note: When there is only one column, we don't need to specify when using Scalar because it is already a vector\nX_train.head()\n\nscaler =  StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n\n#training = pd.concat([X_train, y_train] , axis= 1, join='outer')\n\n\n#training[['x'], 'y']] = training[['x', 'y']].apply(lambda iterator: ((iterator - iterator.mean())/iterator.std()).round(2))\n#training\n\n# ## For Linear Regression: \nmodel = LinearRegression(\n    n_jobs=-1       # use all processors\n)\n_ = model.fit(X_train, y_train)\n\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults.head()\n\nfrom sklearn.metrics import mean_squared_error\n\nprint(f\"The root mean squared error for this model is {mean_squared_error(results['actual'], results['predicted'], squared=False):0.3f}\")\n\n\n# # For KNN\n# Creating KNN Model\nneighbors = int(np.round(math.sqrt(len(df))))\nneighbors = neighbors+1\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\n# I have spent hours trying to figure out the valueerror below. I don't know why it is throwing the error. Everything I look up says it is supposed to be categorical, but that doesn't make sense. I think it has to do with scaling, but I'm not exactly sure why or how. \nfeatures = df.drop(columns=['y'])\ntarget = df['y']\n\n# split the data into validation and training set\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=1)\n\n# create a standard scaler and fit it to the training set of predictors\nscaler = StandardScaler()\nscaler.fit(X_train)\n\n# Transform the predictors of training and validation sets\nX_train = scaler.transform(X_train) \nX_test = scaler.transform(X_test) \n\nmodel = KNeighborsClassifier(n_neighbors=71,  metric='euclidean') # user euclidean distance\n\n# We could choose other distance metrics; for a list of other metrics...\n# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics\n# coverage of difference distance metrics is outside of this courses scope... but, you can experiment by changing the metric\n# for example...\n#knn = KNeighborsClassifier(n_neighbors=71,  metric='manhattan')\n\nmodel.fit(X_train, y_train)\n\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults.head(20)\n\nknnmodel = KNeighborsClassifier(n_neighbors = neighbors,  metric='euclidean') # user euclidean distance\n\nknnmodel.fit(X_train , y_train)\n\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults.head(20)\n\n",
        "Points": 85.0,
        "Comments": "The assignment is well-structured and the code is readable. The student has imported all necessary libraries, loaded and explored the data, and conducted pre-split data preprocessing. The student has also used the train/test split function and indicated the random_state for repeatability. The student has used the StandardScaler() for post-split data preprocessing and avoided fitting the test data. The student has fitted a linear regression model to the data and indicated the RMSE for the model. However, the student has not successfully fitted a KNN regressor to the data due to a ValueError. The student has also not discussed their selection of k value for the KNN model. The student has recapped their analysis and discussed the performance of the linear regression model using the RMSE metric. The overall presentation of the notebook is good, including code readability, comments, and markdown explanations. The student should work on fixing the ValueError for the KNN model and discussing the selection of k value for the KNN model."
    },
    {
        "SID": "5096845",
        "Name": "banakarrachananagaraj",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ### Assignemt 04\n# ### Import required packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(42)\n\n# ### Load the dataset\nnp.random.seed(1)\n\n \n\ndf = pd.read_csv(r'C:/Users/Rachana N Banakar/Downloads/Assignment_4_DM/sample.csv')\n\n \n\ndf.head(10)\n\n\n# ### investigate the dataset\nprint(\"Number of Rows and Number of Columns:\")\nprint(df.head(10))\n\ndf.plot.scatter(x='x', y='y', color='Blue')\nplt.title('Plot showing X vs Y')\n\n# ### Missing Values\nmissing_values = df.isnull().sum()\n\nprint(missing_values)\n\n# ### Visualizing  the data\nplt.scatter(df['x'], df['y'], color='Brown' )\nplt.xlabel('x of values')\nplt.ylabel('y of values')\nplt.title('Plot showing X and Y')\n\nplt.show()\n\n# ### Separate the features (x) and target variable (y)\n\nX = df[['x']]  # Here we have assumed 'x' is the feature column\nY = df['y']    # Here we have assumed 'y' is the target variable\n\n# ### Split the dataset to train and test\n# Splitting the data set into training and testing sets - A train-test split can help shrink the training set when working with small amounts of data. This reduction makes it more difficult for the model to recognize patterns in the data, leading to parameters that are not ideal. In such cases, the learnt parameters might not effectively depict the dataset's underlying relationships, which could affect the model's predictive capability.\n# As a result, it would be wise to avoid a train-test split given the lack of data. Instead, making use of the complete dataset for training enables the model to extract as much data from the available samples as possible, potentially producing a more robust and accurate model.\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.16, random_state=42)\n\n# ### Initialize and fit the Linear Regression model\nlin_model = LinearRegression()\nlin_model.fit(x_train, y_train)\n\n# Making the anticipation on the test set\ny_pred_lin = lin_model.predict(x_test)\n\n# Evaluating the Linear Regression model\nmse_lin = mean_squared_error(y_test, y_pred_lin)\nr2_lin = r2_score(y_test, y_pred_lin)\n\nprint(f\"In Linear Regression - Mean_Squared_Error: {mse_lin:.2f}\")\nprint(f\"In Linear Regression - R-squared: {r2_lin:.2f}\")\n\n# ### Visualize the Linear Regression\nplt.scatter(x_train, y_train, color='green', label='original plot')\nplt.plot(x_test, y_pred_lin, color='red', label='Linear_Regression_plot_which_was_expected')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Linear Regression of x and y')\nplt.show()\n\n# ### Visualize the KNN Regressor\n# Initialize and fit the KNN Regressor model A KNN regression model with 13 neighbors using scaled input features (X_scaled), and then it predicts the target variable (pred_val) using the same scaled features.\n\nKNN_model = KNeighborsRegressor(n_neighbors=8)  # we can adjust the number of neighbors (k) as needed\nKNN_model.fit(x_train, y_train)\n\n# Make predictions on the test set \ny_expect_KNN = KNN_model.predict(x_test)\n\n# Evaluating the KNN Regressor model\nmeanse_KNN = mean_squared_error(y_test, y_expect_KNN)\nr2_KNN = r2_score(y_test, y_expect_KNN)\n\nprint(f\"K-Nearest Neighbors (KNN) - Mean_Squared_Error: {meanse_KNN:.2f}\")\nprint(f\"K-Nearest Neighbors (KNN) - R-squared: {r2_KNN:.2f}\")\n\n# Visualizing the predictions\nplt.scatter(x_train, y_train, color='orange', label='Original_plot')\nplt.scatter(x_test, y_expect_KNN, color='black', label='KNN Regression which is expected')\nplt.plot(x_test, y_pred_lin, color='red', label='Linear_Regression_plot_which_was_expected')\nplt.xlabel('x_values')\nplt.ylabel('y_values')\nplt.legend()\nplt.title('K-Nearest Neighbors (KNN) Regression')\nplt.show()\n\n# ### Observations:\n# The worse performance of both models is indicated by the greater MSE. Here, the MSE values for both models are essentially equal. The linear regression model's poor fit is also shown by the model's negative R2 score. Performance of KNN is superior to that of linear regression overall. In sufficient data points and the complicated data shape may be to blame for the models' poor performance.\n",
        "Points": 95.0,
        "Comments": "Great job on the assignment! You have successfully completed all the required tasks. You imported the necessary libraries, loaded and explored the data, handled missing values, split the data into training and testing sets, and fitted both a linear regression and KNN regressor model. You also evaluated the models using the RMSE metric and provided a discussion of the results. However, you did not use the StandardScaler() for post-split data preprocessing. Make sure to include this step in future assignments to ensure your data is on the same scale. Keep up the good work!"
    },
    {
        "SID": "4882785",
        "Name": "chitralarakeshwaranjaneyulu",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## Assignment 04\n# ## Rakeshwaranjaneyulu Chitrala U96703037\n# ### Importing Libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# ### Loading datasets using pd.read_csv\ndf=pd.read_csv('C:/Users/Dell/Downloads/sample.csv')\n\nprint(df.head())\n\n# ### Here we are creating two dataframes x and y\nX=df[['x']]\ny=df[['y']]\n\n# ### Creation of Scatterplot for x and y dataframes \nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,y,color='orange')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# ### Creating linear regression model\nlin_reg=LinearRegression().fit(X,y)\n\n# ### Here we are representing the Linear Regression model using Scatterplot\nfig=plt.figure() \nf=fig.add_subplot()\nf.scatter(X,y,color='orange')\nf.scatter(X,lin_reg.predict(X),color='black')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# ### Determining the intercept and coefficient values\nb0=lin_reg.intercept_[0]\nb1=lin_reg.coef_[0][0]\n\n# ### Linear regression equation and R-squared score of the model\nprint(\"Y = {:.2f}+{:.2f}x\".format(b0, b1))\nprint(\"R Square R^2: {:.3f}\".format(lin_reg.score(X, y)))\n\n# ### Determining the Mean squared error value\ny_p = lin_reg.predict(X)\nmse = mean_squared_error(y, y_p)\nprint(f\"MSE: {mse}\")\n\n# ### KNN Regressor\nknn_data= df[['x','y']]\nknn_data\n\n# ### Normalizing the data using Minimum-Maximum scaling\nknn_data=(knn_data - np.min(knn_data))/(np.max(knn_data)-np.min(knn_data))\n\nknn_ind=knn_data.drop('y',axis=1)\nknn_dep=knn_data['y']\n\nind_train,ind_test,dep_train,dep_test = train_test_split(knn_ind,knn_dep,test_size=0.3,random_state=20)\n\n# ### Performing Regression\nknn_model=KNeighborsRegressor().fit(ind_train,dep_train)\nprd_val = knn_model.predict(ind_test)\nprd_val\n\npredict_df=pd.DataFrame({'Actual':dep_test,'Predicted':prd_val})\npredict_df.shape\n\n# ### Z-score normalization\npredict_df = (predict_df * np.std(df.y) + (np.mean(df.y)))\n\npredict_df.head()\n\nr2_score(predict_df.Predicted, predict_df.Actual)\n\nknn_model=KNeighborsRegressor(n_neighbors = 11).fit(X,y)\nprd_val = knn_model.predict(X)\n\n# ### Creating Scatterplot for KNN Regression\nfig=plt.figure() \nf=fig.add_subplot()\nf.scatter(X,y,color='blue')\nf.scatter(X,knn_model.predict(X),color='yellow')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\ny_p = knn_model.predict(X)  \nmse = mean_squared_error(y, y_p)\nprint(f\"MSE: {mse}\")\n\n# ### Summary\n# Here from the above we have used two data frames namely x and y which are used to train a linear regression,visualizing its fit and key parameters. Next, we standardize the dataset using Minimum-Maximum scaling.Then, we proceed to train a KNN regressor on the standardized data. After making predictions with this model, we denormalize the predictions to their original scale and evaluate its performance. Additionally, we train another KNN model with neighbors=11 on the original, non-standardized data. Interestingly, both models show comparable performance, as indicated by similar Mean Squared Error (MSE) values.\"\n",
        "Points": 95.0,
        "Comments": "Great job on the assignment! You have successfully completed all the tasks. You imported the necessary libraries, loaded and explored the data, preprocessed the data, and split it into training and testing sets. You also successfully fitted a linear regression and KNN regressor to the data, and evaluated their performance using the RMSE metric. Your discussion of the results was also thorough and insightful. However, you did not use the StandardScaler() for post-split data preprocessing, which resulted in a small deduction of points. Keep up the good work!"
    },
    {
        "SID": "5141685",
        "Name": "dadisettymadhukiran",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ASSIGNMENT 04\n\n# Madhu Kiran Dadisetty_U53552106\n\n# ### Importing Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\n\n\nnp.random.seed(42)\n\n# ### Load Data\ndf = pd.read_csv('C:/Users/Administrator/Downloads/sample.csv', index_col=False)\ndf\n\n# ### Explore the data\ndf.head()\nnum_rows = len(df)\nnum_columns = len(df.columns)\nprint(f\"Number of Rows: {num_rows}\")\nprint(f\"Number of Columns: {num_columns}\")\nprint(df.head(8))\n\ndf.plot.scatter(x='x', y='y')\n\n### Checking data types of columns\n\ndf.dtypes\n\ndf.describe()\n\ndf.shape\n\n# ### Checking for null values\nmissing_values = df.isnull().sum()\n\nprint(missing_values)\n\n# ### Visualize the data\nplt.scatter(df['x'], df['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of the Data')\nplt.show()\n\n\n# ### Separate out the inputs \nX = df[['x']].values\ny = df[['y']].values\n\n# ### Split the data in to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.11, random_state=42)\n\n# ### Fit the linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\ny_pred_r = lin_reg.predict(X_test)\n\n\nr_squared = lin_reg.score(X_test, y_test)\n\nprint('Linear Regression R-squared:', r_squared)\n\ny_pred_r = lin_reg.predict(X_test)\n\nprint('Linear Regression MSE:', mean_squared_error(y_test, y_pred_r))\n\n# Visualize the regression line\nplt.scatter(X_train, y_train, color='b', label='Actual')\nplt.plot(X_test, y_pred_r, color='r', label='Linear Regression Prediction')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Linear Regression')\nplt.show()\n\n# ### KNN Regressor\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train, y_train) \n\ny_pred_knnr = knn_reg.predict(X_test)\n\nr_squared = knn_reg.score(X_test, y_test)\n\nprint('KNN Regressor R-squared:', r_squared)\n\ny_pred_knnr = knn_reg.predict(X_test)\n\nprint('KNN Regressor MSE:', mean_squared_error(y_test, y_pred_knnr))\n\n# Visualize the predictions\nplt.scatter(X_train, y_train, color='g', label='Actual')\nplt.scatter(X_test,y_pred_knnr, color='r', label='KNN Regression Prediction')\nplt.xlabel('X')\n\n\nplt.ylabel('y')\nplt.legend()\nplt.title('K-Nearest Neighbors (KNN) Regression')\nplt.show()\n\n# ### Results and Model Performance:\n#In order to forecast the 'y' variable based on the 'x' variable, two regression models, linear regression and K-Nearest Neighbors (KNN) regression, were applied to the dataset.\n\nLinear Regression:\n\nR-squared (R\u00b2) value: 0.0117\nMean Squared Error (MSE): 650.54\n\nR-squared and MSE results showed that the linear regression model outperformed the KNN.However, the R-squared value is still relatively low, indicating limited explanatory power. The MSE indicates a fair degree of prediction accuracy. Although it is not a strong match, the regression line's fit to the data can be seen visually.\n\nK-Nearest Neighbors (KNN) Regression:\n\nR-squared (R\u00b2) value: -0.8107\nMean Squared Error (MSE): 1191.87\n\nWith a negative R-squared value, the KNN Regression model performed worse than Linear Regression and had a worse fit to the data. A higher MSE indicates less precise predictions. The KNN predictions scatter plot reveals how poorly it performs.\n\nIn summary, neither model offered a good fit to the data, however the Linear Regression model fared better than KNN in this situation.\n\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, preprocessed the data, split it into training and testing sets, and fitted both a linear regression and KNN regressor to the data. You also evaluated the models using the RMSE metric and discussed the results. However, you imported the LogisticRegression and SimpleImputer libraries but did not use them in your code. Remember to only import the libraries you need. Also, consider adding a brief introduction at the beginning of your notebook to provide context for your analysis."
    },
    {
        "SID": "5079175",
        "Name": "doddasasankreddy",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# # LOAD THE DATA \nfile1 = pd.read_csv(\"file.csv\")\n\nprint(file1.head())\n\n# # as we have x,y values lets split values and perform pre processing\nk=file1[['x']]\nm=file1['y']\nfile1.isnull().sum()\n\n# now lets graph the data and visualize it\na=file1.hist()\n\n# scale the data\nscaler = StandardScaler()\nfile1 = scaler.fit_transform(file1)\n\n# # now lets train and test the data 80% train,20% test\nk_train, k_test, m_train, m_test = train_test_split(k, m, test_size=0.2, random_state=42)\n\n# # Model building and training with LRE and KNN\n# linear_reg_model = LinearRegression()\nlinear_reg_model.fit(k_train, m_train)\n\n#  Create the KNN regressor with n number of neighbors (ie 3)\nknn_model = KNeighborsRegressor(n_neighbors=3)\nknn_model.fit(k_train, m_train)\n\n# # Model evaluation for both linear regression and knn\n#for knn\npred = knn_model.predict(k_test)\n# now lets find mean squared error (mse),mean absolute error (mae), r2 score\nmse = mean_squared_error(m_test, pred)\nmae = mean_absolute_error(y_test, pred)\nr2 = r2_score(m_test, pred)\n\nprint(\"KNN REGRESSOR METRICS\")\nprint(f\"mae: {mae}\")\nprint(f\"mse: {mse}\")\nprint(f\"r2: {r2}\")\n# for lre\npred2 = linear_reg_model.predict(k_test)\n# now lets find mean squared error (mse),mean absolute error (mae), r2 score\nmse1 = mean_squared_error(m_test, pred2)\nmae1 = mean_absolute_error(y_test, pred2)\nr21 = r2_score(m_test, pred2)\nprint(\"lre REGRESSOR METRICS\")\nprint(f\"mae1: {mae1}\")\nprint(f\"mse1: {mse1}\")\nprint(f\"r21: {r21}\")\n\n\n\n# KNN Model:\n# On average, it predicts values that are around 23.67 units off from the correct answers.\n# It tends to make relatively large prediction errors, with an average squared error of about 987.28.\n# Unfortunately, it doesn't capture the underlying data patterns well; it's actually worse than simply guessing the average value.\n# Linear Regression Model:\n# It predicts values that are, on average, about 23.04 units away from the correct answers.\n# Its prediction errors are smaller compared to KNN, with an average squared error of about 716.14.\n# However, similar to KNN, it also struggles to capture the data patterns and isn't significantly better than guessing the average value.\n# The Linear Regression model's R2 score is close to zero, indicating that it doesn't perform much better than simply using the mean of the target variable. Like the KNN Regressor, the Linear Regression model is not capturing the underlying patterns in the data.\n# The Linear Regression model has an MSE of approximately 716.14, which is lower than the KNN Regressor's MSE. This suggests that the Linear Regression model's predictions have smaller errors than those of the KNN Regressor.\n#  Both models have similar MAE values, indicating that they make predictions with a similar average absolute error. also both models did not perform as well as they should have been\n\n\n",
        "Points": 85.0,
        "Comments": "The student has done a good job overall. The code is well-structured and the student has demonstrated a good understanding of the concepts. However, there are a few areas that need improvement. The student has imported libraries that were not used in the code, which is not a good practice. Also, the student has scaled the entire dataset before splitting it into training and testing sets, which can lead to data leakage. The student should only scale the training data and then apply the same transformation to the test data. Lastly, the student has not provided a title or a brief introduction to the notebook, which would have made it more readable and understandable. Keep up the good work and consider these points for future assignments."
    },
    {
        "SID": "5090461",
        "Name": "gangupamusaisunil",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ### Import Libraries\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(42)\n\n# ### Load Data\ndf = pd.read_csv('sample.csv', index_col=False)\n\n\n\n# ### Explore the data\nnum_rows = len(df)\nnum_columns = len(df.columns)\nprint(f\"Number of Rows: {num_rows}\")\nprint(f\"Number of Columns: {num_columns}\")\nprint(df.head(10))\n\ndf.plot.scatter(x='x', y='y')\n\n# ### Address any NAN Values\nmissing_values = df.isnull().sum()\n\nprint(missing_values)\n\n# ### Visualize the data\nplt.scatter(df['x'], df['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of the Data')\nplt.show()\n\n\n# ### Separate out the inputs \nX = df[['x']].values\ny = df[['y']].values\n\n# ### Split the data in to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n\n# ### Fit the linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\ny_pred_r = lin_reg.predict(X_test)\n\n\nr_squared = lin_reg.score(X_test, y_test)\n\nprint('Linear Regression R-squared:', r_squared)\n\n\ny_pred_r = lin_reg.predict(X_test)\n\nprint('Linear Regression MSE:', mean_squared_error(y_test, y_pred_r))\n\n# Visualize the regression line\nplt.scatter(X_train, y_train, color='b', label='Actual')\nplt.plot(X_test, y_pred_r, color='r', label='Linear Regression Prediction')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Linear Regression')\nplt.show()\n\n# ### KNN Regression\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train, y_train)\n\ny_pred_knnr = knn_reg.predict(X_test)\n\nr_squared = knn_reg.score(X_test, y_test)\n\nprint('KNN Regressor R-squared:', r_squared)\n\n\n\ny_pred_knnr = knn_reg.predict(X_test)\n\nprint('KNN Regressor MSE:', mean_squared_error(y_test, y_pred_knnr))\n\n\n\n\n# Visualize the predictions\nplt.scatter(X_train, y_train, color='g', label='Actual')\nplt.scatter(X_test,y_pred_knnr, color='r', label='KNN Regression Prediction')\nplt.xlabel('X')\n\n\nplt.ylabel('y')\nplt.legend()\nplt.title('K-Nearest Neighbors (KNN) Regression')\nplt.show()\n\n# OBSERVATION:\n#Linear Regression:\nMSE: 624.17\nR^2 = 0.02\nLinear Regression fits the data reasonably well, with a lower MSE compared to KNN Regression.\n\n#K-Nearest Neighbors (KNN) Regression:\nMSE: 1168.71\nR^2 = -0.82\nKNN Regression provides a more flexible fit to the data but has a higher MSE than Linear Regression.\n\nIn this specific case, Linear Regression outperforms KNN Regression in terms of MSE.\n\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, handled missing values, split the data into training and testing sets, fitted both linear regression and KNN regression models, and evaluated their performance using RMSE. You also provided a good discussion of the results. However, you did not provide a title or brief introduction to your notebook, which would have been helpful for understanding the context of your analysis. Also, you imported the SimpleImputer library but did not use it in your code. Please ensure to only import the libraries you need. Keep up the good work!"
    },
    {
        "SID": "5144161",
        "Name": "gillnikita",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Linear vs KNN Regression\n# ## Fitting the models and comapring them\n# We will import the required packages for linear regresssion, Knn, scaling and metrics\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n\n\nnp.random.seed(42)\n\n# ## Load data\ndata_df = pd.read_csv('sample.csv')\ndata_df\n\n# ## Data processing - check values, data types, missing values\ndata_df.describe()\n\n# ### Checking the data types\ndata_df.info()\n\n# ### Checking for any NULL (missing) values in the data\ndata_df.isna().sum()\n\n# ### There are no missing values in the dataset so no need to drop or impute data here. Also, since the dataset is numerical, we are not checking for any misspelled words.\n# ### Plotting the data\n_ = data_df.hist()\n\n# ### Since we see here that X and Y are not on the same scale, so we need to perform scaling. Its done below after we train-test split the data.\n# ### Visualizing data before fitting the model\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(data_df['x'], data_df['y'], color='red')\n\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n# ### Applying Train-Test Split method\n#splitting before scaling the data\nX = data_df[['x']]\ny = data_df[['y']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\n\n# ### Data Cleaning : Scaling data by using StandardScaler from sklearn.preprocessing\n#scale the train data and the test data\nnumeric_cols = ['x']\nscaler = StandardScaler()\nscaler.fit(X_train[numeric_cols])\nX_train[numeric_cols] = scaler.transform(X_train[numeric_cols])\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\nnumeric_cols = ['y']\nscaler = StandardScaler()\nscaler = StandardScaler()\nscaler.fit(y_train[numeric_cols])\ny_train[numeric_cols] = scaler.transform(y_train[numeric_cols])\ny_test[numeric_cols] = scaler.transform(y_test[numeric_cols])\n\nX_train.head(10)\n\ny_train.head(5)\n\n# ## Linear Regression \n#apply linear regression to fit model to training dataset\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train, y_train)\ny_pred_linear = linear_reg.predict(X_test)\n\n# ### Plotting to visualize the linear regression model fit \nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_train,y_train,color='Green')\nf.plot(X_test,y_pred_linear,color='blue')\nf.set_title('Linear Regression Test')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# ### Metrics Calculation - R-squared, Mean Squared Error and RMSE\nlinear_r2 = r2_score(y_test, y_pred_linear)\nprint(\"Linear Regression R-squared:\", linear_r2)\n\nmeanerror = mean_squared_error(y_test, y_pred_linear)\n\n# Print MSE for Linear Regression\nprint(f\"Linear Regression - Mean Squared Error: {meanerror:.2f}\")\n\n\nlinear_rmse = np.sqrt(mean_squared_error(y_test, y_pred_linear))\nprint(\"Linear Regression RMSE:\", linear_rmse)\n\n# ## K-Nearest Neighbors (KNN) Regression using train-test split\nknn_reg = KNeighborsRegressor(n_neighbors=4)  # number of neighbors taken is 4\nknn_reg.fit(X_train, y_train)\ny_pred_knn = knn_reg.predict(X_test)\n\n# ### Plotting to visualize the KNN regression model fit \nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_train,y_train,color='Green')\nf.scatter(X_test,y_pred_knn,color='blue')\nf.set_title('KNN Regression Test')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# ### Metrics Calculation for KNN \n# Calculate R-squared for KNN Regression\nknn_r2 = r2_score(y_test, y_pred_knn)\n\n# Mean Squared Error (MSE) for KNN Regression\nmse_knn = mean_squared_error(y_test, y_pred_knn)\n\n#RMSE\nknn_rmse = np.sqrt(mean_squared_error(y_test, y_pred_knn))\n\nprint(\"R-squared:\", knn_r2)\nprint(f\"KNN Regression - Mean Squared Error: {mse_knn:.2f}\")\nprint(\"RMSE:\", knn_rmse)\n\n# ### Discussion of Models, Analysis and Results\n# ## Linear Regression:\n# R-squared (R2): -0.64\n# Mean Squared Error (MSE): 1.10\n# RMSE: 1.05\n# ## KNN Regression:\n# R-squared (R2): -0.20\n# Mean Squared Error (MSE): 1.12\n# RMSE: 1.17\n# ### Models Analysis by Metrics Comparison:\n# Just if we look at the metrics results of both and compare, in KNN, the Mean Squared Error (MSE) is approximately 0.20, while for Linear Regression, it is 0.64 These values imply that the KNN model's predictions exhibit a relatively higher level of error, suggesting that the model might not be capturing the data as effectively as Linear Regression does. Even MAE suggests that Linear regreesion is a better model than KNN. \n# Also, the negative sign in R-squared indicate that \n# It might be possible that the model's predictions are worse than if we were to use the average as the predicted value. Also possible if the dataset is small as the case here or there is noise in the dataset.\n# Linear regression, while also not performing well, seems to have a slightly better fit compared to KNN\n# ### Visualizing & discussing the comparison between 2 models\n# Comparison between both the Models with train data\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_train, y_train, color='red')\nax.scatter(X_train,knn_reg.predict(X_train),color='blue')\nax.scatter(X_train,linear_reg.predict(X_train),color='green')\n\n\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title(\"Comparison Between KNN and Linear Regression\")\nplt.tight_layout()\nplt.show()\n\n# Comparison between both the Models with test data\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_test, y_test, color='red')\nax.scatter(X_test,knn_reg.predict(X_test),color='blue')\nax.scatter(X_test,linear_reg.predict(X_test),color='green')\n\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title(\"Comparison Between KNN and Linear Regression\")\nplt.tight_layout()\nplt.show()\n\n# ### As per both the plots - train plot & test plot, we can see that the blue dots and green dots nearly coincide with the red dots (actual data). This clearly signifies that both models are poorly performing here with this dataset.\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on this assignment! You have successfully completed all the required steps. You imported the necessary libraries, loaded and explored the data, preprocessed the data, and split it into training and testing sets. You also correctly implemented both the linear regression and KNN regression models, evaluated them using the appropriate metrics, and provided a thorough discussion of the results. The only area for improvement is to ensure that you explain why you chose a specific k value for the KNN model. Keep up the good work!"
    },
    {
        "SID": "5072991",
        "Name": "gundlurusiddarthareddy",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # U13940323_SiddarthaReddy_Gundluru_Assignment04\n# Week05, Fitting a linear regression and KNN regressor \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ## Step 1: Load the given data that we will model\ndata = pd.read_csv(r'C:/Users/sidda/OneDrive/Documents/Data Mining/Assignment4/sample.csv')\n\ndata.head()\n\n# ## Step 2: Data Exploration\ndata.describe()\n\n# # Checking if we can split data to Train and Test \n# Having small amount of data, dividing it into training and testing sets can limit the training data even more. This can prevent the model from learning effectively, which may lead to less accurate results. So, when data is limited, it's often better to use all of it for training to get the most out of the information available and potentially build a more dependable model.\ndata.shape\n\ndata.isnull().sum()\n\n# Since there are no missing values in the dataframe, we can move forward with assurance. Given that the dataframe has just two columns, it's easy to assign one as the feature (X) and the other as the target (Y). This straightforward setup makes it easier to get the data ready for training and provides a clear understanding of how our feature relates to the target.\ndata.columns\n\n# ## Step 3: Visualizing the data\nplt.scatter(data['x'], data['y'],color='red')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter Plot of X vs Y')\nplt.show()\n\nX = data[['x']]\ny = data[['y']]\n\n# ## Step 4: Standardize the input data \nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ny_scaled = scaler.fit_transform(y)\n\n# # Linear Regression\nlin_reg=LinearRegression().fit(X_scaled,y_scaled)\n\ny_pred = lin_reg.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='blue')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n#Extract the Intercept (b0) from the Linear Regression Model:\nb0=lin_reg.intercept_[0]\n\nb1=lin_reg.coef_[0][0]   # Accessing the coefficient directly without an index\n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X,y):.3f}\")\nmse = mean_squared_error(y_scaled, y_pred)\nprint(\"Mean Squared Error:\",mse)\n\n# A negative R2 Score indicates that the model's predictions are less accurate than simply using the mean of the dependent variable. This means that the model isn't performing well and might not be the right fit for this data.\n# # KNN Regression\n# A KNN regression model with 13 neighbors using scaled input features (X_scaled), and then it predicts the target variable (pred_val) using the same scaled features.\nknn_model= KNeighborsRegressor(n_neighbors = 13).fit(X_scaled,y_scaled)\npred_val = knn_model.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,knn_model.predict(X_scaled),color='blue')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n\nmse = mean_squared_error(y_scaled, pred_val)\nR2 = r2_score(y_scaled,pred_val)\n\nprint(\"Mean Squared Error:\",mse)\nprint(\"R2 Value:\",R2)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,knn_model.predict(X_scaled),color='blue')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='green')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# The Higher MSE suggests that both models aren't performing well, as evidenced by their nearly identical MSE values.\n# Additionally, the negative R2 score for the linear regression model points to its inadequate fit.\n# Comparatively, the KNN model outperforms the Linear Regression.\n# The subpar performance of these models might be attributed to the data's complexity and a lack of sufficient data points.\n",
        "Points": 95.0,
        "Comments": "The student has done a good job on this assignment. The notebook structure is well organized, all necessary libraries are imported, and the data is loaded and explored correctly. The student has also done a good job in preprocessing the data, fitting the models, and evaluating them. However, the student did not indicate the random_state for repeatability in the train/test split, which is why points were deducted. The student's discussion of the results is thorough and insightful. The overall presentation of the notebook is excellent, with clear code and detailed comments. Keep up the good work!"
    },
    {
        "SID": "5074977",
        "Name": "kachamnikhil",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ### Assignment 04\n# ## Nikhil Kacham\n# ## U99023428\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n# ### Load the data\ndf=pd.read_csv('./sample.csv')\n\ndf.head(10)\n\nX=df[['x']]\nY=df[['y']]\n\n# ### explore the data\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,Y,color='brown')\nf.set_xlabel('x')\nf.set_ylabel('Y')\nplt.show()\n\n### Creating the linear regression model\n\n\nlinear_reg=LinearRegression().fit(X,Y)\n\n### Representing the Linear Regression model using Scatterplot\n\nfig=plt.figure() \nf=fig.add_subplot()\nf.scatter(X,Y,color='orange')\nf.scatter(X,linear_reg.predict(X),color='yellow')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nb0=linear_reg.intercept_[0]\nb1=linear_reg.coef_[0][0]\n\n### Linear regression equation and R-squared score of the model\n\nprint(\"Y = {:.2f}+{:.2f}x\".format(b0, b1))\nprint(\"R Square R^2: {:.3f}\".format(linear_reg.score(X, Y)))\n\n### Determining the Mean squared error value\n\nY_predict = linear_reg.predict(X)  \nmse = mean_squared_error(Y, Y_predict)\nprint(f\"MSE: {mse}\")\n\n### KNN Regressor\n\nKNN_data= df[['x','y']]\nKNN_data\n\n### Normalizing the data \n\nKNN_data=(KNN_data - np.min(KNN_data))/(np.max(KNN_data)-np.min(KNN_data))\n\nKNN_independent=knn_data.drop('y',axis=1)\nKNN_dependent=KNN_data['y']\n\n### splitting the data\n\nindependent_train,independent_test,dependent_train,dependent_test = train_test_split(KNN_independent,KNN_dependent,test_size=0.2,random_state=18)\n\n### Performing KNN Regression\n\nKNN_model=KNeighborsRegressor().fit(independent_train,dependent_train)\npredict_val = KNN_model.predict(independent_test)\npredict_val\n\npredict_df=pd.DataFrame({'Actual':dependent_test,'Predicted':predict_val})\npredict_df.shape\n\n###  normalization\n\npredict_df = (predict_df * np.std(df.y) + (np.mean(df.y)))\n\npredict_df.head()\n\n### calculate mean square error\n\ny_predict = KNN_model.predict(X)  \nmse = mean_squared_error(Y, y_predict)\nprint(f\"MSE: {mse}\")\n\nr2_score(predict_df.Predicted, predict_df.Actual)\n\nKNN_model=KNeighborsRegressor(n_neighbors = 10).fit(X,Y)\npredict_val = knn_model.predict(X)\n\n### Scatterplot for KNN Regression\n\nfig=plt.figure() \nf=fig.add_subplot()\nf.scatter(X,Y,color='brown')\nf.scatter(X,KNN_model.predict(X),color='yellow')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n###summary\n\n# In this code, i performed an analysis on a dataset and applied both Linear Regression and k-Nearest Neighbors (KNN) Regression models to predict a target variable 'y' based on the feature 'x'.\n# Both models performance is not that good.A high R-squared value close to 1 indicates that the linear regression model explains a large portion of the variance in 'y' based on 'x'.\n# I have created a KNN Regression model to predict y based on x.\n# For KNN regression model i got r2 value of -7 so this model is not good for fitting data.\n\n\n\n\n",
        "Points": 85.0,
        "Comments": "The assignment is well done with all the necessary steps followed. The data was loaded and explored, and both Linear Regression and KNN Regression models were applied. However, there are a few areas that need improvement. The notebook lacks a title and a brief introduction, which are important for setting the context of the analysis. Also, the train/test split was not done before applying the models, which is a crucial step in any machine learning task. Lastly, the StandardScaler() was not used for post-split data preprocessing. Keep these points in mind for future assignments."
    },
    {
        "SID": "5082779",
        "Name": "kaluvapallisuhit",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## Suhit_Kaluvapalli\n# ### U14988780\n# ### Import Libraries\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(42)\n\n# ### Load Data\ndf = pd.read_csv(\"C:/Users/suhit/Downloads/sample (1).csv\", index_col=False)\n\n\n\n# ### Explore the data\nnum_rows = len(df)\nnum_columns = len(df.columns)\nprint(f\"Number of Rows: {num_rows}\")\nprint(f\"Number of Columns: {num_columns}\")\nprint(df.head(8))\n\ndf.plot.scatter(x='x', y='y')\n\n# ### Address any NAN Values\nmissing_values = df.isnull().sum()\n\nprint(missing_values)\n\n# ### Visualize the data\nplt.scatter(df['x'], df['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of the Data')\nplt.show()\n\n\n# ### Separate out the inputs \nX = df[['x']].values\ny = df[['y']].values\n\n# ### Split the data in to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.23, random_state=42)\n\n# ### Fit the linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n\ny_pred_r = lin_reg.predict(X_test)\n\nprint('Linear Regression MSE:', mean_squared_error(y_test, y_pred_r))\n\n# Visualize the regression line\nplt.scatter(X_train, y_train, color='b', label='Actual')\nplt.plot(X_test, y_pred_r, color='r', label='Linear Regression Prediction')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Linear Regression')\nplt.show()\n\n# ### KNN Regressor\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train, y_train) \n\n\n\ny_pred_knnr = knn_reg.predict(X_test)\n\nprint('KNN Regressor MSE:', mean_squared_error(y_test, y_pred_knnr))\n\n\n\n\n# Visualize the predictions\nplt.scatter(X_train, y_train, color='g', label='Actual')\nplt.scatter(X_test,y_pred_knnr, color='r', label='KNN Regression Prediction')\nplt.xlabel('X')\n\n\nplt.ylabel('y')\nplt.legend()\nplt.title('K-Nearest Neighbors (KNN) Regression')\nplt.show()\n\n# ### OBSERVATION:\n# Linear Regression Model:\n# By considering trained data, a Linear Regression model was created and trained. On the basis of the test data, predictions were generated, and the Mean Squared Error (MSE) was calculated to assess the model's performance. A scatter plot and the regression line were used to visualize\u00a0the actual and anticipated values.\n# KNN Regressor Model:\u00a0\n# By considering trained data, a KNN Regressor model with 5 neighbours\u00a0was initialized\u00a0and trained. On the basis of the test data, predictions were generated, and the Mean Squared Error (MSE) was calculated to assess the model's performance. A scatter plot was used to visualize\u00a0the actual and projected values.\n# Both Linear Regression and KNN Regressor models are applied to the data.\n# Linear Regression yielded a Mean Squared Error (MSE) of approximately 696, indicating the average squared difference between actual and predicted values.\n# KNN Regressor, with 5 neighbors, resulted in a higher MSE of around 937, indicating higher prediction errors compared to Linear Regression in this case.\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, handled missing values, split the data into training and testing sets, fitted both linear regression and KNN regressor models, and evaluated them using the MSE metric. You also provided a good discussion of the results. However, you missed out on the post-split preprocessing step using StandardScaler(). Remember to scale your features for models that are sensitive to the scale of the input features like KNN. Keep up the good work!"
    },
    {
        "SID": "5151247",
        "Name": "karanamsandeep",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Assignment 4\n# ### Sandeep Karanam U94742075\n# ### Fitting Linear Regression and KNN Regressor on sample data and comparing the models performances including necessary preprocessing and data exploration.\n# ### Import Modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\n\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(42)\n\n# ### Load Data\ndf = pd.read_csv('./sample.csv', index_col=False)\n\ndf.head(10)\n\ndf.shape\n\n# ## Address any NaN values that need to be dropped from the dataset\n# Count the number of missing values for each feature.\ndf.isna().sum()\n\n# No missing values in the data\n# Seperate out the input m(X) and the target (y)\nX = df[['x']]\ny = df[['y']]\n\n# ## Explore the given data\n# Explore the data using a scatterplot.\nfig=plt.figure()\nax=fig.add_subplot()\nax.scatter(X,y,color='red')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title('Linear Regression Test')\nplt.tight_layout()\nplt.show()\n\n# Since we do not have much data we will not train test split model. we will the fit model on all the data we have and I do not see any need of standardisation or normalising the data. As we do not have any null values nothing to impute and no categorical columns so no need of converting into dummy variables. \n# ##  Partitioning data into training and validation sets\n# Split the dataset into training (80%) and validation (20%) sets. \n# Randomly sample 80% of the dataset into a new data frame `train_data`. The remaining 20% serve as validation.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n# ## Fit the Linear Regression Model\nlin_reg=LinearRegression()\n_ = lin_reg.fit(X_train,y_train)\n\nfig=plt.figure()\nax=fig.add_subplot()\n\nax.scatter(X_train,y_train,color='red')\nax.scatter(X_train, lin_reg.predict(X_train), color='blue')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title('Linear Regression Test')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Y={lin_reg.intercept_[0]:.2f} + {lin_reg.coef_[0][0]:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X, y):.4f}\")\n\n# ### check the linear regression model on the valid data after split\ny_prediction = lin_reg.predict(X_test)\nr2 = r2_score(y_test, y_prediction)\n\nprint(\"\\nlinear Regression Results after split:\")\nprint(f\"R^2 on test data: {r2:.4f}\")\n\n# ## Now we will try to fit the KNN Regressor model on this data\n# You've heard that a good starting point in determining a k value is to try the square root of the total observations. Since there are 143 observations, following this rule of thumb, we would select a k value of 12.\n# > If k is an even number, there is a possibility that an observation could have the same number of nearest neighbors being one class (they took the loan) as the number being another (did not take the loan). In the cases of a tie, the SKLearn implementation of k-nn will select the first found. Though the chances of this happening are low, it's better to avoid this by choosing an odd number.\nmodel = KNeighborsRegressor(n_neighbors=12,  metric='euclidean') # user euclidean distance\n\nmodel.fit(X_train,y_train)\n\nfig=plt.figure()\nax=fig.add_subplot()\n\nax.scatter(X_train,y_train,color='red')\nax.scatter(X_train, model.predict(X_train), color='blue')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title('KNN Regressor Test')\n\nplt.tight_layout()\nplt.show()\n\ny_pred = model.predict(X_test)\n\n# Calculate the R-squared (R2) score\nr2 = r2_score(y_test, y_pred)\n\n# Print the R-squared score\nprint(f\"R-squared (R2) Score:, {r2:.2f}\")\n\n# The linear regression model and the K-Nearest Neighbors (KNN) regressor seem to perform relatively poorly since they provide low R-squared (R2) values. Let's interpret the results:\n# 1. Linear Regression Model:\n# - Train Data: The linear regression model is given an R2 score of 0.0015 for training data. The model\u2019s R-square value is almost 0 indicating that it explains virtually none of the target variable\u2019s variance. It implies a weak linear dependence of x-features on Y.\n# - Test Data: The linear regression model also fails on test data and has negative R2 score of \u22120.0040. An R2 score less then zero implies that the model performs worse than a horizontal line (horizontal line has an R2 score of 0). This shows that the linear regression model is not appropriate for this data set.\n# 2. KNN Regressor:\n# - R2 = -0.23 for the KNN regressor. It is a higher R2 than that of the linear regression model but still in low range. Nevertheless, when R2 is negative, it indicates poorly the predicting power of KNN regressor for explaining the amount variance in the target. However, this may not be a suitable option for our dataset.\n# Therefore, in conclusion, these two models are not doing well with this dataset; the linear regression performs somewhat better than the KNN regressor. Such as this may not suit the datasets to which these models could apply, and other feature engeneration or model tweaking is needed to bring about the desired effect.\n",
        "Points": 85.0,
        "Comments": "The assignment is well done with clear explanations and good use of code. The data loading, exploration, and preprocessing were done correctly. The models were fitted and evaluated properly. However, there were a few areas that could be improved. The student did not use the StandardScaler() for post-split data preprocessing. Also, the student did not discuss the selection of k value for the KNN model. Lastly, the student did not provide the RMSE for both models, which was required in the assignment. Overall, good job, but remember to follow all the assignment instructions."
    },
    {
        "SID": "4996333",
        "Name": "kasturinagagayathiridevi",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## Assignment 4\n# ### Naga Gayathiri Devi\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndf = data = pd.read_csv(r'C:\\Users\\gayat\\OneDrive\\Documents\\DataMining_Assignments\\sample.csv')\n\ndf.head(5)\n\nprint(data.describe())\n\nprint(data.isnull().sum())\n\n# ### Visualize the data\nplt.scatter(data['x'], data['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of Data')\nplt.show()\n\nscaler = StandardScaler()\ndata['x'] = scaler.fit_transform(data[['x']])\n\nX = data[['x']]\ny = data['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\ny_pred_lr = lr.predict(X_test)\n\nplt.scatter(X, y,label='Actual')\nplt.scatter(X_test, y_pred_lr, color='black', label='Linear Regression')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Linear Regression ')\nplt.legend()\nplt.show()\n\nmse_lr = mean_squared_error(y_test, y_pred_lr)\n\nprint(f\"Mean Squared Error: {mse_lr}\")\n\nr2_lr = r2_score(y_test, y_pred_lr)\n\nprint(f\"R-squared: {r2_lr}\")\n\ndf = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred_lr\n})\n\n# Print the DataFrame\nprint(df.head())\n\ndf = data = pd.read_csv(r'C:\\Users\\gayat\\OneDrive\\Documents\\DataMining_Assignments\\sample.csv')\n\nscaler = StandardScaler()\ndata['x'] = scaler.fit_transform(data[['x']])\n\n### Split the scaled data into training and test set\nX = data[['x']]\ny = data['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n\n# KNN regression model\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\n# Calculate the Mean Squared Error for KNN regression\nmse_knn = mean_squared_error(y_test, y_pred_knn)\nprint(f\"Mean Squared Error: {mse_knn}\")\n\n# Calculate the (R2) score for KNN regression\nr2_knn = r2_score(y_test, y_pred_knn)\nprint(f\"R-squared: {r2_knn}\")\n\nplt.scatter(X, y,label='Actual')\nplt.scatter(X_test, y_pred_knn, color='black', label='KNN Regression')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('KNN Regression ')\nplt.legend()\nplt.show()\n\n\ndf = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred_knn\n})\n\n# Print the DataFrame\nprint(df.head())\n\n# ### Summary\n# When we evaluate linear regression and KNN regression models using metrics like Mean Squared Error (MSE) and R-squared, we gain insights into their performance. MSE quantifies how well the models predictions align with the actual values, with a lower MSE indicating a closer fit to the data. On the other hand, R-squared measures the proportion of variability in the target variable that the model explains, with a higher R-squared signifying a better fit.\n# In this particular scenario, the linear regression model outperforms the KNN regression model. This means that the linear regression model provides predictions with less error and greater accuracy. However, it's essential to recognize that these metrics are just one aspect of model assessment. We should also consider factors such as model complexity and the ease of interpreting results when choosing the most suitable model for a given problem.\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on the assignment! You have successfully completed all the tasks. You imported the necessary libraries, loaded and explored the data, preprocessed the data, split it into training and testing sets, fitted both linear regression and KNN regression models, and evaluated them using RMSE. You also provided a good discussion of the results. However, you did not discuss your selection of k value for the KNN model, which resulted in a small deduction of points. Keep up the good work!"
    },
    {
        "SID": "5069725",
        "Name": "kharmalejanhavianantprakash",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay,mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(42)\n\n\n# ## Load the data\ndf = pd.read_csv(\"C:/Users/janha/Downloads/sample.csv\")\n\ndf.head(10)\n\ndf.shape\n\n# ## Check for null values\ndf.isna().sum()\n\n# ## Plot the data\nX=df[['x']]\ny=df[['y']]\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(X,y,color='red')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n# ## Check the correlation\nimport seaborn as sns\nsns.heatmap(df.corr(),annot=True)\nplt.show()\n\n# ## Conduct train-test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.20, random_state=1)\nprint('Training   : ', X_train.shape)\nprint('Validation : ', y_test.shape)\n\n# ## Use Linear Regression model \nmodel = LinearRegression()\nlin_reg1 = model.fit(X_train, y_train)\n\ny_lin_pred = lin_reg1.predict(X_test)\ncompare = pd.DataFrame()\ncompare['actual'] = y_test\ncompare['prediction'] = y_lin_pred\ncompare.head()\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(X_train,y_train, color='red')\nax.scatter(X_train, lin_reg.predict(X_train), color='blue')\nax.set_title('Linear Regression')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n# ## Analyze the linear regression model's performance\nb0 = lin_reg1.intercept_[0]\nb1 = lin_reg1.coef_[0][0]\nr2 = lin_reg1.score(X_train, y_train)\nprint(f\"Y = {b0} + {b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X, y):.3f}\") \n\nmse = mean_squared_error(y_test,y_lin_pred)\nmae = mean_absolute_error(y_test,y_lin_pred)\nrmse = np.sqrt(mse)\nprint(mse)\nprint(mae)\nprint(rmse)\n\nfrom pathlib import Path\n\nplt.style.use('Solarize_Light2')\nprint(plt.style.available) # this will give you a list of the styles available in your version of matplotlib\n\nnp.random.seed(42)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# ## Set K value\nk_value = 3\n\ndef euclidean_distance(x1, x2, y1, y2):\n    return float(((x1-x2)**2 + (y1-y2)**2)**(1/2))\n\n_ = df.hist()\n\n# ## Create a new obvervation where we don't know the target value...\nimport matplotlib.pyplot as plt \n\ndf = pd.read_csv(\"C:/Users/janha/Downloads/sample.csv\")\nX = df['x']\ny = df['y']\n\nfig, ax = plt.subplots(1,1) # create a figure (fig) that has one plot (ax) in a 1x1 grid\n\n# plot owners in color C1 with shape 'o', and nonowners with color C0 and shape 'D'\nax.scatter(X,y, marker='o', color='C1')\n\n# add axis labels\nplt.xlabel('Input')  # set x-axis label\nplt.ylabel('Target')  # set y-axis label\n\n# iterate through each row and add the number of each observation to the scatter plot\nfor index, row in df.iterrows(): # look through each row of our data\n    ax.annotate(index, (row.x, row.y)) # add the number of the observation next to point\n\n## Create a test data point (one we will use to predict a target value for)\n# note that I'm choosing a point that is calculated from the mean and standard deviation of the sample.\nnew_value = pd.DataFrame([{'x': np.mean(df.x)+np.std(df.x), 'y': np.mean(df.y)+np.std(df.y)}])\nnew_value\n\nax.scatter(new_value.x, new_value.y, marker='*', label='New value', color='red', s=150)\nax.annotate('NEW', (new_value.x, new_value.y)) # add the number of the observation next to point\n\nplt.show() \n\n# ## Calculate the distance from the new point\ndistances = []\nfor index, row in df.iterrows():\n    distances.append(euclidean_distance(row.x, new_value.x, row.y, new_value.y ))\n    \ndf['distance'] = distances\n\ndf\n\ndf = df.sort_values(by=['distance'])\ndf\n\nk_nearest_neighbors = df.iloc[:k_value, :]\n\nk_nearest_neighbors\n\n# ## Plot the distance to each of the three closest observations\nfig, ax = plt.subplots(1,1) # create a figure (fig) that has one plot (ax) in a 1x1 grid\n\nX = df['x']\ny = df['y']\n# plot owners in color C1 with shape 'o', and nonowners with color C0 and shape 'D'\nax.scatter(X,y, marker='o', color='C1')\n\n# add axis labels\nplt.xlabel('Input')  # set x-axis label\nplt.ylabel('Target')  # set y-axis label\n\n# iterate through each row and add the number of each observation to the scatter plot\nfor index, row in df.iterrows(): # look through each row of our data\n    ax.annotate(index, (row.x, row.y)) # add the number of the observation next to point\n\nax.scatter(new_value.x, new_value.y, marker='*', label='Newvalue', color='red', s=150)\nax.annotate('NEW', (new_value.x, new_value.y)) # add the number of the observation next to point\n\nfor index, row in k_nearest_neighbors.iterrows():\n    x_values = np.array([row.x, new_value.x[0]]) # note, to remove a ragged array warning, new_household.Income[0] is used. \n    y_values = np.array([row.y, new_value.y[0]])\n    plt.plot(x_values, y_values, 'bo', linestyle=\"--\")\nplt.show()\n\n# ## Use KNN Regressor model\nclf = KNeighborsRegressor(7)\nclf.fit(X_train, y_train)\n\n\n# ## Analyze the knn regressor model's performance\ny_pred = clf.predict(X_test)\nprint(r2_score(y_test,y_pred))\nprint(mean_absolute_error(y_test,y_pred))\nprint(mean_squared_error(y_test,y_pred))\n\n\n# ### Linear Regression Model:\n# The MSE of approximately 585.59 represents the average squared difference between the true target values and the predicted values. \n# The MAE of approximately 18.86 represents the average absolute difference between the true target values and the predicted values. \n# ### KNN Regression Model:\n# The negative R-squared (R2) value of approximately -0.33 indicates that the KNN model is performing poorly.\n# The MAE of approximately 20.80 represents the average absolute difference between the true target values and the predicted values for the KNN model.\n# The MSE of approximately 718.64 represents the average squared difference between the true target values and the predicted values for the KNN model.\n# ## The Linear Regression model has a lower MSE and RMSE compared to the KNN Regression model, indicating better performance in terms of squared errors.\n# ## The KNN Regression model has a negative R-squared, suggesting that it doesn't explain the variance in the data well. \n# For both your linear regression and KNN models, accuracy is poor and mean square error is large for the following reasons:\n# ### Negative correlation between X and Y: \n# Looking at the correlation between X and Y which is negative indicates that there is a weak relationship between X and Y. \n# ### Small dataset:\n# A sample size of only 142 points can result in overfitting, which happens when models become too adept at learning the training data and become incapable of generalizing to new data.\n# ### Low accuracy:\n# Low accuracy indicates that the models are not learning the training data very well. This can be due to a number of factors, like noise in the data, overfitting, or underfitting.\n# Here are some things you can try to improve the performance of your models:\n# ### Improve the relationship between X and Y:\n# Try to add more features that are relevant to the target variable. This can help the model to get a more accurate relationship between the X and Y variables.\n# ### Use a larger dataset: \n# If at all possible, strive to gather additional information. This can help prevent overfitting by providing the models with more data to learn from.\n# ### Remove the outliers:\n# Removing the outliers can lead to improved model performance and can make the model's predictions more consistent and less sensitive to extreme values in the data.\n# ### Use a different model: \n# There are many different machine learning models available. Try using a different model,to see if you can improve the performance.\n\n\n",
        "Points": 85.0,
        "Comments": "The student has done a good job overall. The notebook structure is clear, all necessary libraries are imported, and the data is loaded and explored. The student has also conducted pre-split data preprocessing, used the train/test split function, and indicated the random_state for repeatability. The student has used the StandardScaler() for post-split data preprocessing and avoided fitting the test data. The student has fitted a linear regression and KNN regressor to the data, indicated the RMSE for both models, and discussed their selection of k value for the KNN model. The student has recapped their analysis and discussed the performance of the models using the RMSE metric. The overall presentation of the notebook, including code readability, comments, and markdown explanations, is good. However, the student has not provided a title and a brief introduction to the notebook, which results in a deduction of 15 points."
    },
    {
        "SID": "3911920",
        "Name": "kreitzerdevan",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Assignment 4\n# ### Devan Kreitzer\n# ### ISM6136.003\n# ### 10-04-2023\n# ------\n# ### Importing necessary libraries\n%matplotlib inline\n\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# ### Load the Data\n# This step loads the data using the `pandas` library \nsample_df = pd.read_csv('./data/sample.csv')\nsample_df.head(10)\n\n# The `shape` function shows us the rows and columns of our DateFrame.\nsample_df.shape\n\n# ### Explore the Data\n# By seeing the data types within our object, we can better understand if there are mismatching data types or if any feature needs to be changed. In our case, both of our *x* and *y* variables are float64 data types - which means the both contain numerical values. This will help our regressions work better.\nsample_df.dtypes\n\n# We can see that our data has no missing values, so there is no need to replace any NA's or impute any missing values.\nsample_df.isna().sum()\n\n# Next, we want to see the structure of the data.\n# | Column *x* | Column *y* |\n# | ----------- | ----------- |\n# | The values range from approximately 22.31 to 98.21. | The values range from approximately 2.95 to 99.49. |\n# | The mean is approximately 54.26. | The mean is approximately 47.83. |\n# | The standard deviation is approximately 16.77. | The standard deviation is approximately 26.94. |\nsample_df.describe()\n\n# These histograms give an idea of how the data points are distributed across different ranges for both *x* and *y*. The values of *x* seem to have multiple peaks, suggesting a multi-modal distribution. The distribution of *y* values appears to be relatively more uniform compared to *x*.\n_ = sample_df.hist()\n\n# I then wanted to explore the relationships between *x* and *y*, this is shown in the scatter plot below.\n# Plotting a scatterplot for x and y values\nplt.figure(figsize=(10, 6))\nplt.scatter(sample_df['x'], sample_df['y'], color='purple', alpha=0.6)\nplt.title('Scatterplot of x vs y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid(True)\nplt.show()\n\n# *rawr*\n# ### Splitting and Scaling the Data\n# I scaled the data to ensure all features have a similar scale, making algorithms like KNN, which rely on distances between data points, more effective and accurate. Additionally, scaling helps with interpretability in linear regression, as coefficients become directly comparable in terms of their impact on the response variable.\n# Splitting the data into independent and dependent variables\nX = sample_df[['x']]\ny = sample_df['y']\n\n# Splitting the data into training and testing sets (80% training and 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nX_train_scaled[:5], X_test_scaled[:5]  # Displaying the first 5 rows of scaled training and testing data\n\n# ## Linear Regression\n# ### Fit the model\n# Finally, we fit the model. We will use the LinearRegression model from sklearn.linear_model.\n# Initialize the Linear Regression model\nlinear_model = LinearRegression()\n\n# Fit the model to the training data\nlinear_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred_linear = linear_model.predict(X_test_scaled)\n\n# Calculate the mean squared error for the linear regression model\nmse_linear = mean_squared_error(y_test, y_pred_linear)\n\nmse_linear\n\n# The linear model's error score is about 716.14. This score tells us how far off our model's predictions are, on average, from the real values. In simpler terms, it's like measuring how often our guesses miss the target. The lower the score, the better our model is at predicting.\n# ### Plot the Regression\n# Predictions for visualization\nx_values = np.linspace(min(sample_df['x']), max(sample_df['x']), 400).reshape(-1, 1)\ny_pred = linear_model.predict(scaler.transform(x_values))\n\n# Plotting the data and the linear regression model\nplt.figure(figsize=(10, 6))\nplt.scatter(sample_df['x'], sample_df['y'], color='blue', label='Data points')\nplt.plot(x_values, y_pred, color='red', label='Linear Regression Line')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Linear Regression Model on Data')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# ## KNN Regression\n# ### Set k value\nk_value = 5 # we'll check these number of neighbors \n\ndef euclidean_distance(x1, x2, y1, y2):\n    return float(((x1-x2)**2 + (y1-y2)**2)**(1/2))\n\ndistance = euclidean_distance\n\n# ### Create a New Observation\n# We'll generate a random observation for the *x* feature, and we'll pretend we don't know its *y* value.\n# Create a new observation using mean and standard deviation of the sample\nnew_observation = pd.DataFrame([{'x': np.mean(sample_df['x']) + np.std(sample_df['x']), \n                                 'y': np.mean(sample_df['y']) + np.std(sample_df['y'])}])\n\n# Plotting as per the provided structure\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n# Scatter plot of all data points\nax.scatter(sample_df['x'], sample_df['y'], marker='o', color='C0')\n\n# Annotate each data point with its index\nfor index, row in sample_df.iterrows():\n    ax.annotate(index, (row['x'], row['y']))\n\n# Highlight the new observation\nax.scatter(new_observation['x'], new_observation['y'], marker='*', label='New Observation', color='red', s=150)\nax.annotate('NEW', (new_observation['x'].values[0], new_observation['y'].values[0]))\n\n# Axis labels and title\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatterplot with New Observation Highlighted')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Return the coordinates of the new observation\nnew_observation\n\n# ### Calculate Distances\ndistances_corrected = [euclidean_distance(row['x'], new_observation['x'].values[0], row['y'], new_observation['y'].values[0]) for _, row in sample_df.iterrows()]\n\n# Assign the distances to the dataframe and sort the dataframe by distance\nsample_df['distance_to_new_obs'] = distances_corrected\nsample_df_sorted = sample_df.sort_values(by=['distance_to_new_obs'])\n\n# Display the top rows of the sorted dataframe\nsample_df_sorted.head()\n\n# ### Identify k Nearest Neighbors\nk_nearest_neighbors = sample_df.nsmallest(k_value, 'distance_to_new_obs')\n\nk_nearest_neighbors[['x', 'y', 'distance_to_new_obs']]\n\n# ### Plot the Data\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n# Scatter plot of all data points\nax.scatter(sample_df['x'], sample_df['y'], marker='o', color='C0', label='Data Points')\n\n# Annotate each data point with its index\nfor index, row in sample_df.iterrows():\n    ax.annotate(str(index), (row['x'], row['y']), fontsize=8)\n\n# Highlight the new observation\nax.scatter(new_observation['x'].values[0], new_observation['y'].values[0], marker='*', label='New Observation', color='red', s=150)\nax.annotate('NEW', (new_observation['x'].values[0] + 1, new_observation['y'].values[0]), fontsize=8)\n\n# Connect the new observation to its 5 nearest neighbors\nfor _, neighbor in k_nearest_neighbors.iterrows():\n    ax.plot([neighbor['x'], new_observation['x'].values[0]], [neighbor['y'], new_observation['y'].values[0]], 'bo-', linestyle=\"--\", markersize=4)\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Scatterplot with New Observation and its Nearest Neighbors Connected')\nax.legend()\nax.grid(True)\nplt.show()\n\n# ## ***Findings***\n# In this assignment, we explored our data using two techniques. With Linear Regression, essentially to drawing a straight line through our data, we achieved an average error (MSE) of about 716.14. This means our model's predictions were not really close and had quite a bit of discrepancies. On the other hand, the KNN Regression, which makes predictions based on the 5 closest data points, gave us a more localized understanding, especially for data points that didn't fit the general trend. By evaluating these models' performance, we found that they complement each other, providing a comprehensive view of our data. As we move forward, we'll consider refining these models or exploring new ones for even sharper insights.\n",
        "Points": 95.0,
        "Comments": "Great job on the assignment! The notebook is well-structured and the code is clean and well-commented. The data loading, exploration, preprocessing, and model fitting were all done correctly. The discussion of results was also insightful. However, there was no explicit discussion on the selection of the k value for the KNN model. Remember to explain why you chose a particular k value in future assignments. Keep up the good work!"
    },
    {
        "SID": "4974601",
        "Name": "kunapareddychaitanyasai",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\n# # Importing the data\ndf = pd.read_csv('sample.csv')\ndf.head(8)\n\n# # Implementing Train and Test Data \ninput_x = df['x']\ntarget = df['y']\n\nX_train, X_test, y_train, y_test = train_test_split(input_x, target, test_size=0.3, random_state=56)\n\n\ndf_train= pd.DataFrame([y_train,X_train]).transpose()\ndf_test=pd.DataFrame([y_test,X_test]).transpose()\n\ndf_train.head()\ndf_test.head()\n\n# # Fitting a Linear Regression Model:\n\nmodel = LinearRegression(\n    n_jobs=-1      \n)\n\n\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(df_test[['x']], df_test[['y']], color='blue')\nax.scatter(df_test[['x']], reg_KNN.predict(df_test[['x']]), color='green')\nax.scatter(df_test[['x']], reg.predict(df_test[['x']]), color='red')\n\nax.set_title('KNN Regresor')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n# # Results for Linear Regression model:\n\nR2= reg.score(df_test[['x']], df_test[['y']])\nprint(f\"Coefficient of determination for the model is {R2}\")\nprint(f\"The root mean squared error for this model is {mean_squared_error(df_test[['y']], reg.predict(df_test[['x']]), squared=False):0.3f}\")\n\n# # KNN Model Fitting:\n#scale the X yales and Y values\nnumeric_cols=['y','x']\n\nscaler = StandardScaler()\nscaler.fit(df_train[numeric_cols])\ndf_train[numeric_cols] = scaler.transform(df_train[numeric_cols])\ndf_test[numeric_cols] = scaler.transform(df_test[numeric_cols])\ndf_train.head()\ndf_test.head()\n\nreg = model.fit(df_train[['x']], df_train[['y']])\n\nreg_KNN = KNeighborsRegressor(n_neighbors=15)\nreg_KNN.fit(df_train[['x']], df_train[['y']])\n\n# # Comparison of Results:\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(df_test[['x']], df_test[['y']], color='blue')\nax.scatter(df_test[['x']], reg_KNN.predict(df_test[['x']]), color='green')\nax.scatter(df_test[['x']], reg.predict(df_test[['x']]), color='red')\n\nax.set_title('KNN Regresor')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\nKNN_R2=reg_KNN.score(df_test[['x']], df_test[['y']])\nLM_R2=reg_KNN.score(df_test[['x']], df_test[['y']])\nKNN_MSE=mean_squared_error(df_test[['y']], reg_KNN.predict(df_test[['x']]), squared=False)\nLM_MSE=mean_squared_error(df_test[['y']], reg.predict(df_test[['x']]), squared=False)\nprint(KNN_MSE)\nprint(LM_MSE)\nprint(KNN_R2)\nprint(LM_R2)\n\n# The R-squared values for both models are negative, which is unusual. R-squared typically ranges from 0 to 1, and a negative R-squared suggests that the models are performing worse than a horizontal line. It might indicate that the models are not a good fit.\n",
        "Points": 85.0,
        "Comments": "The student has done a good job overall. The code is well-structured and the student has imported all the necessary libraries, loaded the data, and conducted the train/test split. The student has also fitted a linear regression and KNN regressor to the data and evaluated the models using the RMSE metric. However, there are a few areas for improvement. The student did not provide a title or brief introduction to the notebook. Also, the student did not explore the data by checking columns, missing values, and plotting the data. Lastly, the student did not discuss their selection of k value for the KNN model. These steps are important for understanding the data and the model's performance."
    },
    {
        "SID": "4279109",
        "Name": "maggiopatrick",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Assignment 4\n# This notebook will load data, process and transfom the data, then create a model using linear regression and K nearest neighbors techniques. \n# Author: Patrick Maggio  \n# Date: 10/2/2023\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing \nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(1)\n\ndf = pd.read_csv('../A4/A4data.csv')\nprint(df.head())\nprint(type(df))\n\ndf.dtypes\n\n#df['x'].unique()\n#df['y'].unique()\n#unique numbers have no value for visual anaylysis\n\ndf.describe()\n\n\ndf.info()\n\n\ndf.hist()\n\ndf.isnull().sum()\n\nfeatures = df[['x']]\ntarget = df[['y']]\nprint(type(features))\n\n# split the data into validation and training set\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=1)\n\n# create a standard scaler and fit it to the training set of predictors\n#scaler = preprocessing.StandardScaler()\n#scaler.fit(X_train)\n\n# Transform the predictors of training and validation sets\n#X_train = scaler.transform(X_train) \n#X_test = scaler.transform(X_test) \n\n\n#X_train = pd.DataFrame(X_train)\n#X_test = pd.DataFrame(X_test)\n#y_train = pd.DataFrame(y_train)\n#y_test = pd.DataFrame(y_test)\n\n\n# # Preprocessing Discussion\n# After exploring the data by looking at summary statistics, null values, and a visual representation of the data, I determined that further processing was not necessary. Due to the distribution of the data and relatively equal scale, normalization or standardization was not necesary to fit a proper model. If there were missing values, imputation may have been necessary since the sample was not large.\n\nmodel = KNeighborsRegressor(n_neighbors=5,  metric='euclidean')\n\nmodel.fit(X_train, y_train)\n\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults.head(10)\n\ny1 = results['actual']\ny2 = results['predicted']\nplt.scatter(X_test,y1)\nplt.scatter(X_test,y2)\n\n\nLinReg = LinearRegression()\n\nline = LinReg.fit(X_train, y_train)\n\nb1 = line.coef_\nb0 = line.intercept_\nprint(\"The equation for the line of best fit is: y = \" + str(b0) + \" + \" + str(b1) +\"x\")\n\nresults['line_pred'] = line.predict(X_test)\n\ny3 = results['line_pred']\nplt.scatter(X_test,y3)\nplt.scatter(X_test,y1)\nplt.scatter(X_test,y2)\nplt.legend(['Linear Regresion','Actual Values','KNN Prediction'])\n\n# # Model Discussion\n# This data had no catagorical elements so KNN regression was used instead of classification. Multiple iterations of the model were ran including using 5, 10, 20, and 50 neighbors to predict the outcome. The more neighbors used, the more the model resembled the linear regreassion. This model was not especially accurate. Perhaps more data would help the model be more accurate or another model could be used, such as a decision tree. It is evident that the KNN model predicted values with less of a range than the actuals. This could be beneficial in situations where the penalty for being outside a boundary is great such as in engineering when structures need to be within a stress boundary or in business when costs cannot exceed revenues.\n\n\n\n\n",
        "Points": 85.0,
        "Comments": "The assignment is well-structured and the code is readable. The student has done a good job in data loading, exploration, and model fitting. However, there are a few areas that need improvement. The student did not use the StandardScaler() for post-split data preprocessing. Also, the student did not indicate the RMSE for both models and did not discuss their selection of k value for the KNN model. Lastly, the student did not recap their analysis and discuss the performance of the models using the RMSE metric. Overall, good job, but remember to include all necessary steps in your analysis."
    },
    {
        "SID": "5128349",
        "Name": "mamidiprudhviteja",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# >Prudhvi Teja Mamidi\n# >U88005599\n# # Assignment _4\n# ## Problem statement\n# Fit a linear regression and KNN regressor to the data sample.csv, Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\n\nnp.random.seed(88005599)\n\n# ## Load and clean data\ndata= pd.read_csv('data/sample.csv')\ndata.head()\n\n# Here for the above data we assume Y as dependent variable and X as independent variable\ndata.shape\n\ndata.isna().sum() #no missing values\n\ndata.dtypes\n\ndata.describe()\n\n# Here we have 142 data points so for better performance of model we do train and test split with ratio of 0.2 for test size\n# ## Test and Train split\ninput_x = data['x']\ntarget = data['y']\n\nX_train, X_test, y_train, y_test = train_test_split(input_x, target, test_size=0.2, random_state=48)\n\n\ntype(X_train)\ntype(y_train)\n\ndata_train= pd.DataFrame([y_train,X_train]).transpose()\ndata_test=pd.DataFrame([y_test,X_test]).transpose()\n\ndata_train.info()\n\ndata_train.head()\ndata_test.head()\n\ndata_train.describe()\n\n# Here for above data we can observe that there is more difference bewtween min and max vales for both variables X and Y. But for linear regression there is no need for Normalisation as min amd max values fall between 3 Standard of deviation. \n# ## Linear Regression Model\n#training the model based on train Data \nmodel = LinearRegression(\n    n_jobs=-1       # use all processors\n)\nreg = model.fit(data_train[['x']], data_train[['y']])\n\n#Testing the model based on test data\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(data_test[['x']], data_test[['y']], color='green')\nax.scatter(data_test[['x']], reg.predict(data_test[['x']]), color='blue')\n\nax.set_title('Linear Regression')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n# ### Results of Linear regression Model\n#ressults of Linear Regression Model\nR2= reg.score(data_test[['x']], data_test[['y']])\nprint(f\"Coefficient of determination for the model is {R2}\")\nprint(f\"The root mean squared error for this model is {mean_squared_error(data_test[['y']], reg.predict(data_test[['x']]), squared=False):0.3f}\")\n\n# ## KNN regressor\n# ###  Standardisation of Data\n# As KNN deals with ecludian distance between the values  ans is non parameter regression so,it is better to Standardise the data to limit the range of values with meaan as 0\n#scale the X yales and Y values\nnumeric_cols=['y','x']\n\nscaler = StandardScaler()\nscaler.fit(data_train[numeric_cols])\ndata_train[numeric_cols] = scaler.transform(data_train[numeric_cols])\ndata_test[numeric_cols] = scaler.transform(data_test[numeric_cols])\n\ndata_train.head()\ndata_test.head()\n\n# ### Best N_neighbours value for KNN regressor based on MSE and r2\n# finding the best N-neibours \nKNN_results=pd.DataFrame()\nn_neighbours=list(range(1,55))\nTrain_r2=np.zeros(len(n_neighbours))\nTest_r2=np.zeros(len(n_neighbours))\nTrain_MSE=np.zeros(len(n_neighbours))\nTest_MSE=np.zeros(len(n_neighbours))\nfor i in range(len(n_neighbours)):\n    reg1 = KNeighborsRegressor(n_neighbors=i+1)\n    reg1.fit(data_train[['x']], data_train[['y']])\n    Train_r2[i]=reg1.score(data_train[['x']], data_train[['y']])\n    Test_r2[i]=reg1.score(data_test[['x']], data_test[['y']])\n    Train_MSE[i]=mean_squared_error(data_train[['y']], reg1.predict(data_train[['x']]), squared=False)\n    Test_MSE[i]=mean_squared_error(data_test[['y']], reg1.predict(data_test[['x']]), squared=False)\nKNN_results=pd.DataFrame({\n    'n_neighbours':list(range(1,55)),\n    'Train_r2':Train_r2,\n    'Test_r2':Test_r2,\n    'Train_MSE':Train_MSE,\n    'Test_MSE':Test_MSE\n})\n\n\nplt.plot(KNN_results['n_neighbours'],KNN_results['Test_MSE'],color='red', marker='o', label='Points')\n\nplt.plot(KNN_results['n_neighbours'],KNN_results['Test_r2'],color='blue', marker='o', label='Points')\n\nmin(KNN_results['Test_MSE'])\nbest_n_MSE=KNN_results[KNN_results['Test_MSE']==min(KNN_results['Test_MSE'])].iloc[0, 0]\nprint(best_n_MSE)\nbest_n_r2=KNN_results[KNN_results['Test_r2']==max(KNN_results['Test_r2'])].iloc[0, 0]\nprint(best_n_r2)\n\n# From above graph and results we can see that for n=15 the MSE value is minimum and R2 value is maximun for test data\nreg1 = KNeighborsRegressor(n_neighbors=15)\nreg1.fit(data_train[['x']], data_train[['y']])\n\n# ## Comparing results of Linear regression and KNN regressor\n#Testing the model based on test data\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(data_test[['x']], data_test[['y']], color='green')\nax.scatter(data_test[['x']], reg1.predict(data_test[['x']]), color='blue')\nax.scatter(data_test[['x']], reg.predict(data_test[['x']]), color='red')\n\nax.set_title('KNN Regresor')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\nKNN_R2=reg1.score(data_test[['x']], data_test[['y']])\nLM_R2=reg.score(data_test[['x']], data_test[['y']])\nKNN_MSE=mean_squared_error(data_test[['y']], reg1.predict(data_test[['x']]), squared=False)\nLM_MSE=mean_squared_error(data_test[['y']], reg.predict(data_test[['x']]), squared=False)\nprint(KNN_MSE,LM_MSE)\nprint(KNN_R2, LM_R2)\n\n# # Discussion of results of two models\n# Data: Given data plot of X and Y depics a figure which is neither linear or polynomial, So I believe neither of models predicts the Association of two variables X and Y.\n# Linear Regression:\n# As expected when we perfomed the linear regression by splitting the data to test and train we got the goodness of fit or coefficient of determination value as -0.005( means bad fit) and MSE = 28.90 (1.105 by scalling) which is very high so linear model is not proper fit for the given data.\n# KNN regressor: \n# it is a non parametric regressor so scalling plays a important role, when we performed the KNN with nof neighbours as 15 ( by considering the max Test_r2 and min Test_MSE) we got r2 = 0.025 and MSE= 1.07 slight improvement compared to linear regression but KNN will not be proper model for given data.\n# Conclusion:\n# When we realise Statistical values of data it seems to have somekind of  distribution but the plot of points depics a shape of an animal, so we should not build models only based on statistical values we have to visualise the data before predicting the model.\n\n\n",
        "Points": 95.0,
        "Comments": "The student has done a good job on this assignment. The notebook structure is well organized, all necessary libraries are imported, and the data is loaded and explored thoroughly. The student has also done a good job with preprocessing and splitting the data. The models are fitted correctly and evaluated using appropriate metrics. The discussion of results is also comprehensive and insightful. However, the student could improve on the overall presentation of the notebook. Some parts of the code could be more readable and the comments could be more descriptive. Also, the student has imported some libraries that were not used in the assignment, which is not a good practice. Overall, a very good job."
    },
    {
        "SID": "5128983",
        "Name": "mandadirevanth",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ### Revanth Mandadi\n# U16635207\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# ### Load the data\ndf = data = pd.read_csv(r\"C:\\Users\\revanths4\\OneDrive\\Desktop\\sample.csv\")\n\n# ### Data Exploration\nprint(data.head(10))\n\n\n\nprint(data.describe())\n\n\n\nprint(data.info())\n\n# ### Check for missing values\n\nprint(data.isnull().sum())\n\n\n# ### Visualize the data\n\nplt.scatter(data['x'], data['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of Data')\nplt.show()\n\n# ### Data Preprocessing(Standard Scaling)\n\nscaler = StandardScaler()\ndata['x'] = scaler.fit_transform(data[['x']])\n\n\nX = data[['x']]\ny = data['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n\n\n# ### Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n\n\n# ### Make predictions on the test set\ny_pred_lr = lr.predict(X_test)\n\n# ### PLot linear Regression\nplt.scatter(X, y,label='Actual')\nplt.scatter(X_test, y_pred_lr, color='green', label='Linear Regression')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Linear Regression ')\nplt.legend()\nplt.show()\n\n# ###  Calculate the MSE\n\nmse_lr = mean_squared_error(y_test, y_pred_lr)\n\n\n# ### Print result\nprint(f\"Mean Squared Error: {mse_lr}\")\n\n# ### Calculate R-square\nr2_lr = r2_score(y_test, y_pred_lr)\n\n# ### Print result\nprint(f\"R-squared: {r2_lr}\")\n\ndf = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred_lr\n})\n\n# Print the DataFrame\nprint(df.head())\n\n# ### KNN Regression\ndf = data = pd.read_csv(r\"C:\\Users\\revanths4\\OneDrive\\Desktop\\sample.csv\")\n\nscaler = StandardScaler()\ndata['x'] = scaler.fit_transform(data[['x']])\n\n\n### Split the scaled data into training and test set\nX = data[['x']]\ny = data['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n\n\n# KNN regression model\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\n\n# Calculate the Mean Squared Error for KNN regression\nmse_knn = mean_squared_error(y_test, y_pred_knn)\nprint(f\"Mean Squared Error: {mse_knn}\")\n\n# Calculate the (R2) score for KNN regression\nr2_knn = r2_score(y_test, y_pred_knn)\nprint(f\"R-squared: {r2_knn}\")\n\n\n\nplt.scatter(X, y,label='Actual')\nplt.scatter(X_test, y_pred_knn, color='red', label='KNN Regression')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('KNN Regression ')\nplt.legend()\nplt.show()\n\n\ndf = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred_knn\n})\n\n# Print the DataFrame\nprint(df.head())\n\n### Summary\n\n# According to MSE and R-squared metrics, the linear regression model is considered better than the KNN regression model.we can say my comparing mse values of linear regression and knn regression.\n# MSE calculates how close the predicted values are to the actual values, by finding the average of the squared differences. A lower MSE means that the data fits well.R-squared is a measure of how much of the target variable's variability is accounted for by the model. A higher R-squared means that the data fits better.\n# In this problem, the linear regression model performs better with less error and more accuracy compared to the KNN regression model. This means that the linear regression model matches the data better but it's important to remember that these measurements are just one way to assess a model and also important to think about other things like how complicated the model is and how easy it is to understand the results.\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully loaded the data, explored it, preprocessed it, and split it into training and testing sets. You have also correctly implemented both the Linear Regression and KNN Regression models and evaluated them using the MSE and R-squared metrics. However, you did not discuss your selection of k value for the KNN model. Remember to explain why you chose a particular k value in future assignments. Keep up the good work!"
    },
    {
        "SID": "4976197",
        "Name": "mekalamallemkondaiahvenkatadheeraj",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Assignment 4\n# running KNN and linear regression on sample data set\n# ## Importing libraraies\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nimport os\n\n%matplotlib inline\n\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Importing and skimming data \ndf = pd.read_csv('assignment_4_data.csv')\n\ndf.head(10)\n\n# Importing data from the sample file provided (renamed the file as 'assignment_4_data'). I  skimmed through the data and I believe that both the x and y columns are of the same scale so even when normalized the data will not undergo a drasric transformation or improve the working of any regression model. \nprint(df.shape) # provieds information about no. of rows and columns\nprint(df.head())\n\ndf.plot.scatter(x='x', y='y')\ndf.isna().sum()\n\n# the data is clean and has the shape of a t-rex, can see no discenable pattern or relationship between x and y.\n# # Train Test division\n# The size of the data set is small. So I will be using 90-10 split i.e 90% training and 10% test data.\nX=df[['x']]\ny=df[['y']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_train, y_train, color='red')\n\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n# scale the numeric features\nscaler = StandardScaler()\nscaler.fit(X_test)\nX_train1 = scaler.transform(X_train)\nX_test1 = scaler.transform(X_test)\n\nprint (X_test1)\n\n\nscaler1 = StandardScaler()\nscaler1.fit(y_train)\ny_train1 = scaler1.transform(y_train)\ny_test1= scaler1.transform(y_test)\n\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_train1, y_train1, color='red')\n\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n\n# After normalizing we can see that there is almost no change in the realtionship between x and y beacuse they almost form the same t-rex image. So i'll be using the same non standardised dataset. \n# # Linear Regression\nlin_reg=LinearRegression()\n_ = lin_reg.fit(X_train,y_train)\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_train, y_train, color='red')\nax.scatter(X_train, lin_reg.predict(X_train), color='blue')\n\n\n\nax.set_title('Linear Regression')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\nfig = plt.figure()\nax = fig.add_subplot()\n\ny_pred = lin_reg.predict(X_test)\n\n#ax.scatter(X_train, y_train, color='red')\n#ax.scatter(X_train, lin_reg.predict(X_train), color='blue')\nax.scatter(X_test,y_pred , color='green')\nax.scatter(X_test, y_test, color='brown')\n\n\nax.set_title('Linear Regression')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n# The above is a plot of the predicted test values and the actual test values(brown). The graph can be analysed by looking at any brown point and look for the point that is perfectly vertical to that point. The greeen point that you can look at is the predicted value, as you can see the brown points are all over the place and the green points are in a single line (because this is a linear regression model) and the ponints are really far from most of the points indicating that this isn't a good model for this data.\nb0 = lin_reg.intercept_[0]\nb1 = lin_reg.coef_[0][0]\nr2 = lin_reg.score(X_test, y_test)\n\nprint(f\"Y = {b0:.2f} + {b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X_test, y_test):.3f}\") \nprint('Linear Regression MSE:', mean_squared_error(y_test, y_pred))\n\n\n# The R2 is negative for test data implying the linear regression model is a bad fit for the data. The 'Y = 55.47 + -0.14x' is the predicted realtionship for the data. The MSE value is  742.726739216043 impliying most of the predicted points don't lie on the line and are far from it.\n# # KNN \n# importing packages specific to KNN\nfrom pathlib import Path\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport warnings #This is to keep future warning from cluttering\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# I am using The 5 nearest neighbours to calculate the possible Y value for a given x using KNN. I will also be using eucledian distance as I want the exact location rather than meniski.\nmodel =  KNeighborsRegressor(n_neighbors=5, metric='euclidean')\n\n#model = KNeighborsClassifier(n_neighbors=71,  metric='euclidean') # user euclidean distance\n\n# We could choose other distance metrics; for a list of other metrics...\n# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics\n# coverage of difference distance metrics is outside of this courses scope... but, you can experiment by changing the metric\n# for example...\n#knn = KNeighborsClassifier(n_neighbors=71,  metric='manhattan')\n\nmodel.fit(X_train, y_train)\n\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults.head(20)\n\nfig = plt.figure()\nax = fig.add_subplot()\n\n#ax.scatter(X_train, y_train, color='red')\n#ax.scatter(X_train, lin_reg.predict(X_train), color='blue')\nax.scatter(X_test, results['predicted'], color='green')\nax.scatter(X_test, results['actual'], color='brown')\n\n\nax.set_title('KNN')\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\n# The above plot is a representation of the predicted point and the actual point. To compare them take any brown point and look for a green point laying exactly vertical to it the green is the exact predicted point KNN gave us for the given parameters. Looking at the graph KNN also predicted points almost along a straight line just like the linear regression.\n\"\"\"\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults.head(20)\n\"\"\"\n\nrss = sum((results['predicted'] - results['actual'] )** 2)\n\ntss = sum((results['actual'] - results['actual'].mean() )** 2)\n\nprint (rss ,tss)\n\nR2 = 1 - (rss/tss)\n\nprint ('exact R2 value',R2)\n\nr2 = r2_score(results['actual'], results['predicted'])\n\nprint('KNN Regressor MSE:', mean_squared_error(results['actual'], results['predicted']))\nprint(f\"K-Nearest Neighbors (KNN) - R-squared: {r2:.2f}\")\n\n\n#results['actual'].mean()\n\n# Residuals sum of squares\n#rss = sum((results['predicted'] - results['actual']) ** 2)\n\n# Total sum of squares (proportional to the variance of the observed data)\n#tss = sum((results['actual'] - mean(results['actual'])) ^ 2)\n\n# Coefficient of determination R2\n#r_square = 1 - (rss/tss)\n\n# The KNN model at neighbours = 5 is also a bad fit for the model as you can see in the scatter plot and from the negitive R2 value and the high mean square value.\n# I am varying the number of neighbours to optimize my KNN based on R2 value and MSE\n\nfor i in range(1,30 ,1):\n    \n    model =  KNeighborsRegressor(n_neighbors=i, metric='euclidean')\n    model.fit(X_train, y_train)\n    results = pd.DataFrame()\n    results['actual'] = y_test\n    results['predicted'] = model.predict(X_test)\n    #results.head(20)\n    r2 = r2_score(results['actual'], results['predicted'])\n\n    print('for ' + str(i)+' KNN Regressor MSE:' + str(mean_squared_error(results['actual'], results['predicted'])))\n    print('for ' +str(i)+ ' K-Nearest Neighbors (KNN) - R-squared: ' + str(r2))\n\nprint('')\nprint('') \nprint('')\nprint('')\nprint('')\nprint('minkowski measurements')\nprint('')\nprint('')\nprint('')\n    \nfor i in range(1,30 ,1):\n    \n    model =  KNeighborsRegressor(n_neighbors=i, metric='minkowski')\n    model.fit(X_train, y_train)\n    results = pd.DataFrame()\n    results['actual'] = y_test\n    results['predicted'] = model.predict(X_test)\n    #results.head(20)\n    r2 = r2_score(results['actual'], results['predicted'])\n\n    print('for ' + str(i)+' KNN Regressor MSE:' + str(mean_squared_error(results['actual'], results['predicted'])))\n    print('for ' +str(i)+ ' K-Nearest Neighbors (KNN) - R-squared: ' + str(r2))\n    \n    \n\n# KNN for 15 neighbours has a positve R2 value and appears to be a relatively better fit from all the models we have looked through.\n# for 15 KNN Regressor MSE:709.7633469527229\n# for 15 K-Nearest Neighbors (KNN) - R-squared: 0.015617038732304356\n# But I belive this might ne a classic example of overfitting the model for testing data because once the testing data is again varied 15 might not be the best option to use.\n# Relative to the linear regression model KNN at n= 15 is a better model.\n# # Summary\n# I imported the dataset and skimmed through it examining the scales of the variables (i.e if the min and max are arouund the same and if the data given is continuous to make sure th emin and max aren't just outliers).\n# I checked the data for any NA's and other things I might have needed to clean.\n# Split the data into test and train at 90 - 10 split because the data set was realtively small. \n# I fit the train data to a linear regression model and used my test set to look at the models performance based the standard regression measures, R2 and MSE. \n# The model Owerwhelmingly underperformend and this judgement is base don the -ve R2 value, impling that the model can't fit the data or prdict any of the next values with out a large margin of error.\n# Next I made a KNN regression model and fit the train data to it I arbitarily set the k value at 5 and tested the model on train split. \n# The KNN model also underperformed as its R2 was also negative.\n# So I checked if I could optimize the model any more to better fit the data so I varied the Number of neighbours and the metric type from eucledian to minkowski and listed out the values.\n# # Conclusion\n# Both models don't have the ability to even closly predict Y values and are bad fits to the data set. Relative to the linear regression model KNN at n= 15  using eucledian distance is a better model.\n\n\n",
        "Points": 85.0,
        "Comments": "The student has done a good job overall. The notebook structure is clear, and the student has imported all necessary libraries. The data loading and exploration are done well, and the student has also done a good job with the train/test split. However, the student has made a mistake in the post-split preprocessing by fitting the StandardScaler on the test data, which is not correct. The models are fitted and evaluated correctly, and the student has provided a good discussion of the results. The overall presentation of the notebook is good, with clear code and explanations. However, the student should avoid fitting the scaler on the test data in future assignments."
    },
    {
        "SID": "5082739",
        "Name": "minukaprajayreddy",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\nnp.random.seed(42)\n\n# # Importing and Exploring the Data\ndf = pd.read_csv('./sample.csv', index_col=False)\ndf.head()\n\n\ndf.shape\n\ndf.info()\n\ndf.isna().sum()\n\ndf.describe()\n\n# Scaling is not necessary in this case as there is only one variable which is being used\n# Preprocessing and data exploration are important first steps for data analysis. To make sure the dataset is prepared for modeling, preprocessing resolves missing values, converting data, and addressing imbalances. They also make sure that categorical values are treated accordingly and every numerical data is on the same scale. Together, these actions increase data quality, knowledge, and the ability to create more reliable models.\n# # Plotting the data\nX = df[['x']]\ny = df[['y']]\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X, y, color='red')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nplt.tight_layout()\nplt.show()\n\n# # Splitting the data\n\nX_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.2, random_state=42)\n\n# # Linear Regression\nlin_reg_model = LinearRegression(\n    n_jobs=-1       # use all processors\n)\nlin_reg_model.fit(X_train, y_train)\n\n\ny_pred_lin_reg = lin_reg_model.predict(X_test)\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_test, y_test, color='red')\nax.plot(X_test,y_pred_lin_reg,color='blue')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title(\"Linear Regression Test\")\nplt.legend([\"Expected\", \"Predicted\"])\nplt.tight_layout()\nplt.show()\n\nprint(\"R^2: \",lin_reg_model.score(X_test, y_test))\nprint(\"RMSE: \",mean_squared_error(y_test, y_pred_lin_reg, squared=False))\n\n\n# Both R^2 and RMSE values are not satisfactory, negative r^2 here means that the model is performing worse than a horizontal line, overall from the values we can say that linear regression is not a good fit for this data model.\n# # Knn Regression\n# ## Finding the value of k which is most optimal\nfor k in range(1,21):\n    \n    knn_reg_model = KNeighborsRegressor(\n    n_jobs=-1,\n    n_neighbors= int(k) )\n    knn_reg_model.fit(X_train, y_train)\n    print(\"R^2: for k =\",k,\"is\",knn_reg_model.score(X_test, y_test))\n    \n                    \n\n\n# at k = 15 the model gas the lowest R^2 before increasing at k=16\n    \nknn_reg_model = KNeighborsRegressor(\n    n_jobs=-1,\n    n_neighbors= 15 )\n\nknn_reg_model.fit(X_train, y_train)\n\n\ny_pred_knn_reg = knn_reg_model.predict(X_test)\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_test, y_test, color='red')\nax.plot(X_test,y_pred_knn_reg,color='blue')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title(\"KNN Regression Test\")\nplt.legend([\"Expected\", \"Predicted\"])\nplt.tight_layout()\nplt.show()\n\nprint(\"R^2: \",knn_reg_model.score(X_test, y_test))\nprint(\"RMSE: \", mean_squared_error(y_test, y_pred_knn_reg, squared=False))\n\n# Both R^2 and RMSE values are not satisfactory, negative r^2 here means that the model is performing worse than a horizontal line, overall from the values we can say that Knn regression is not a good fit for this data model.\n# ## Comparting Both Models\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_test, y_test, color='red')\nax.plot(X_test,y_pred_knn_reg,color='blue')\nax.plot(X_test,y_pred_lin_reg,color='green')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nplt.legend([\"Expected\", \"KNN - Predicted\",\"Linear Regression - Predicted\"])\nplt.tight_layout()\nplt.show()\n\n# Both K-Nearest Neighbors (KNN) Regression and linear regression seem to be inappropriate for this dataset. Unsatisfactory model performance is indicated by the R^2 and RMSE measures, and there is little proof that the data show any sign of a detectable relationship. It's possible that these data points are more like random noise than a defined pattern that can be modeled with a regression model.\n",
        "Points": 95.0,
        "Comments": "Great job on this assignment! You have successfully imported the necessary libraries, loaded and explored the data, and split it into training and testing sets. Your implementation of both the Linear Regression and KNN Regression models was correct, and you correctly evaluated their performance using the RMSE metric. You also provided insightful discussions about your results. However, you did not use the StandardScaler() for post-split data preprocessing. Remember to scale your data when necessary to ensure optimal model performance. Keep up the good work!"
    },
    {
        "SID": "5109183",
        "Name": "namanagreeshma",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## Assignment 4\n# In this notebook we will fit a linear regression and KNN regressor to the loaded data.\n# ### Step 1 : Import the libraries \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ### Step 2: Load the given data that we will model\ndata = pd.read_csv(\"Downloads/sample.csv\")\n\n\ndata.head(10)\n\n# ### Step 3: Exploring the data\ndata.describe()\n\n# ### Step 4: Preprocessing \n# To train and test in this model we need to reconsider it because when handling smaller dataset, performing a train and test split can further reduce the size of the training set.\n# This is because when we split our already limited data into two subsets for training and testing, we are allocating a portion of your data exclusively for testing purposes, leaving us with less data for training the model.\n# For example, if you have a very small dataset, splitting it into an 80-20 or 70-30 ratio for training and testing can leave us with even fewer samples for training. While the testing set is essential for evaluating your model's performance, having too small a training set can make it challenging for your model to learn meaningful patterns and generalize well to new data.\ndata.isnull().sum()\n\n# There are no null values. \n# ### Step 5: Visualize the scatter plot \nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X, y, color='red')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title(\"Linear Regression Test\")\nplt.tight_layout()\nplt.show()\n\nX = data[['x']]\ny = data[['y']]\n\n# ### Step 6: Standardize the input data \nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ny_scaled = scaler.fit_transform(y)\n\n# ### Step 7: Performing Linear Regression\nlin_reg=LinearRegression().fit(X_scaled,y_scaled)\n\ny_pred = lin_reg.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\n\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='green')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nb0=lin_reg.intercept_[0]\nb1=lin_reg.coef_[0][0]  \n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X,y):.3f}\")\nmse = mean_squared_error(y_scaled, y_pred)\nprint(\"Mean Squared Error:\",mse)\n\n# The value of R^2 is negative, which indicates the performance to be bad. Overall, we can observe that Linear Regression model doesn't seem to be working well with the data.\n# ### Step 8: Performing KNN Regression\nknn_model= KNeighborsRegressor(n_neighbors = 13).fit(X_scaled,y_scaled)\npred_val = knn_model.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\n\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,knn_model.predict(X_scaled),color='blue')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nmse = mean_squared_error(y_scaled, pred_val)\nprint(\"Mean Squared Error:\",mse)\nR2 = r2_score(y_scaled,pred_val)\nprint(\"R^2 Value:\",R2)\n\n# ## Overall Comparision\nfig=plt.figure()\nf=fig.add_subplot()\n\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,knn_model.predict(X_scaled),color='blue')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='green')\nf.set_title('Comparision')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# ## Summary\n# Mean Sqaure Error: Basically measures the average squared difference between the predicted values and the actual values in the dataset.A higher value indicates a worse fit of the model to the data because it means that the model's predictions are farther away from the actual values. \n# High in both the models, indicating bad performance. \n# R^2: measures the proportion of the variance in the dependent variable that is predictable from the independent variables in a regression model. \n# Negative in Linear Regression Model, indicating bad performance. Postive in KNN Regression model.\n# Complex data shapes can be challenging for linear models like Linear Regression to fit because they assume a linear relationship between the independent and dependent variables.\n# In this scenario, they could be better models that fit the data. But b/w Linear Regression and KNN Regression,\n# KNN Regressions seems to perform better than Linear Regression. \n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, preprocessed the data, and split it into training and testing sets. You have also successfully fitted a linear regression and KNN regressor to the data and evaluated their performance using RMSE. Your discussion of the results is also well done. However, you did not indicate the random_state for repeatability in the train/test split, which is important for reproducibility of results. Also, the overall presentation of the notebook could be improved by adding more comments in the code and markdown explanations for better understanding. Keep up the good work!"
    },
    {
        "SID": "5007273",
        "Name": "nandamuruyashwanthbharadwaj",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# set the random seed\nnp.random.seed(42) # set this to ensure the results are repeatable. \n\n# ##Data load\ndf = pd.read_csv(r'C:/Users/hrush/Downloads/sample.csv')\n\n#Display the first 10 rows of the 'sample' dataframe\ndf.head(10)\n\n# ### Determine the number of rows and columns\n# determine the number of rows and columns\n\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n#structure of a DataFrame\ndf.info()\n\n# ### Data Exploration\n# #### Check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# ### Column Removal Based on Missing Data Threshold\ndf = df.dropna(axis=1, thresh=int(0.65*df.shape[0])) #axis=1 is column. thresh=int(0.65*df.shape[0]) means that if a column has less than 65% of the data, it will be dropped\n\n# ### Number of missing values in a observation (row) is high if any\ndf = df.dropna(axis=0, thresh=int(0.25*(df.shape[1]-1))) # axis=0 is row. thresh=int(0.25*(df.shape[1]-1)) means that if a row has less than 25% of the data, it will be dropped\n\ndf.shape \n\ndf.isna().sum() # check how many missing values are left in each column\n\nprint(df.head())\n\nprint(df.describe())\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,y,color='red')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# Separate the features (X) and target variable (y)\nX = df[['x']]  \ny = df['y']    \n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\n# Initialize and fit the Linear Regression model\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# ### Linear regression modeling.\n# Make predictions on the test set\ny_pred_lr = lr_model.predict(X_test)\n\n# Evaluate the Linear Regression model\nmse_lr = mean_squared_error(y_test, y_pred_lr)\nr2_lr = r2_score(y_test, y_pred_lr)\n\nprint(f\"Linear_Reg - Mean Squared Error: {mse_lr:.2f}\")\nprint(f\"Linear_Reg - R-square value: {r2_lr:.2f}\")\n\n## Data Visualization\n\n# Visualize the regression line\nplt.scatter(X_train, y_train, color='b', label='Actual')\nplt.plot(X_test, y_pred_lr, color='r', label='Linear Regression Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('Linear Regression')\nplt.show()\n\n# ### K-Nearest Neighbors (KNN) regression modeling.\n# Initialize and fit the KNN Regressor model\nknn_model = KNeighborsRegressor(n_neighbors=3)  # we can adjust the number of neighbors i.e k as needed\nknn_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_knn = knn_model.predict(X_test)\n\n# Evaluate the KNN Regressor model\nknn_mse = mean_squared_error(y_test, y_pred_knn)\nknn_r2 = r2_score(y_test, y_pred_knn)\n\nprint(f\"KNN - Mean Squared Error: {knn_mse:.2f}\")\nprint(f\"KNN - R-squared: {knn_r2:.2f}\")\n\n# ## Data Visualization\n# Visualize the predictions\nplt.scatter(X_train, y_train, color='b', label='Actual')\nplt.scatter(X_test, y_pred_knn, color='g', label='KNN Regression Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('K-Nearest Neighbors (KNN) Regression')\nplt.show()\n\n# ### Evaluation and comparison of both models.\n# Discussing the results and comparing the models' performance\nprint(\"All Results:\")\nprint(f\"Linear Reg- Mean Squared Error: {mse_lr:.2f}\")\nprint(f\"Linear Reg- R-squared: {r2_lr:.2f}\")\nprint(f\"K-Nearest Neighbors (KNN) - Mean Squared Error: {knn_mse:.2f}\")\nprint(f\"K-Nearest Neighbors (KNN) - R-squared: {knn_r2:.2f}\")\n\n# ### Summary\n# Linear Regression\n# Linear Reg- Mean Squared Error: 624.18\n# Linear Reg- R-squared: 0.02\n# KNN\n# K-Nearest Neighbors (KNN) - Mean Squared Error: 877.80\n# K-Nearest Neighbors (KNN) - R-squared: -0.37\n# Linear Regression -  It has low R squared value of 0.02 , thereby indicating the model can explain a small amount of variation in the target variable. Linear regression is not a good fit for this data.\n# Knn - This model also performs worse than linear regression on the data  as indicated by its negative r squared value. It's not able to exactly predict target variable based on the input variables.\n# Both these models perform badly on this data as the r squared value is low , This means both the models have problems identifying the key patterns in this data. \n# Preprocessing is very important as  it helps to ensure that the data is accurate, consistent, and complete. This is essential for obtaining reliable results from any analysis\n# Data Exploration is important because it helps to understand the data and identify patterns and relationships. This information can be used to develop hypotheses, refine research questions, and choose the appropriate modeling techniques.\n",
        "Points": 95.0,
        "Comments": "Great job on this assignment! You have successfully completed all the necessary steps for data loading, exploration, preprocessing, model fitting, and evaluation. Your code is clean and well-commented, making it easy to follow your process. You also provided a good discussion of your results. However, you imported some libraries that you did not use in your code, such as LogisticRegression, SimpleImputer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, and mean_absolute_error. Make sure to only import the libraries you need for your analysis. Keep up the good work!"
    },
    {
        "SID": "5084803",
        "Name": "nimmasandeepreddy",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# Import required libraries\n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\ndf = pd.read_csv(r\"C:\\Users\\Sandeep Reddy\\OneDrive\\Documents\\sample.csv\")\n\ndf.head(10)\n\ndf.shape\n\ndf.isna().sum()\n\n# Now we made sure that there are no missing values in our dataframe by using the above functions isnull. Since there are no missing values or rows there is no need to impute them or replace them with mean/median/mode aka central tendencies.\n# ## Performing a train-test split\n# Now, it's time to split our data into train and test datasets, usually this split is done in 70-30 ratio or 80-20 or 60-40. But anything between 70-30 to 75-25 is considered optimal. Now we split the data into 70% train and 30% test datasets.\ntrain_data, test_data = train_test_split(df, test_size=0.30, random_state=42)\nprint('Training   : ', train_data.shape)\nprint('Validation : ', test_data.shape)\n\n# We usually impute any missing values after the split, however we didn't have any missing values we'll skip this step and proceed to standardizing the data, which is final part of data preprocessing.\ntrain_data.head()     \n\ntest_data.head()\n\n# Since we have different indices this might be a problem while joining columns to predict the accuracy so I would like to remove all the indices and add the new indices starting from 0 to both train and test datasets.\ntrain_data.reset_index(drop=True, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)\n\n# ## Scale the data\ntrain_data_scaled = train_data\ntest_data_scaled = test_data\n\nscaler = StandardScaler()\ntrain_data_scaled = pd.DataFrame(scaler.fit_transform(train_data_scaled),\n                       index=train_data_scaled.index, columns=train_data_scaled.columns)\n\ntest_data_scaled = pd.DataFrame(scaler.transform(test_data_scaled),\n                       index=test_data_scaled.index, columns=test_data_scaled.columns)\n\ntrain_data_scaled.head()\n\ntrain_data.head()\n\ntest_data_scaled.head()\n\ntest_data.head()\n\n# ## Fitting a Linear Regression Model\n# Predicting using a simple Linear Regression Model.\n\nmodel = LinearRegression()\n\ninputs = ['x']\ntarget = ['y']\npredicted = ['PREDICTED']\n\nX_train = train_data_scaled[inputs].copy()\ny_train = train_data_scaled[target].copy()\n\nX_test = test_data_scaled[inputs].copy()\ny_test = test_data_scaled[target].copy()\n\n# Step 2: Train the model (on the training data)\nmodel.fit(X_train, y_train)\n\n# Step 3: Use the model to predict the target values\ny_test[predicted] = model.predict(X_test)\ny_test.head()\n\nplt.scatter(y_test['y'], y_test['PREDICTED'])\nplt.xlabel(\"Actual Values (y_test)\")\nplt.ylabel(\"Predicted Values (y_predicted)\")\nplt.title(\"Actual vs. Predicted Values\")\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nmean_squared_error(y_test['y'], y_test['PREDICTED']) \nprint(f\"The mean squared error for this model is : {mean_squared_error(y_test[target], y_test[predicted], squared=False):0.3f}\")\n\nr2 = r2_score(y_test['y'], y_test['PREDICTED']) \nprint(f\"The R-squared score for this model is : {r2}\")\n\nmae = mean_absolute_error(y_test['y'], y_test['PREDICTED'])\nprint(f\"Mean Absolute Error (MAE) for this model is : {mae}\")\n\n\n# ## KNN Model Fitting\nmse_values = []\n\nk = 35\n\ninputs = ['x']\ntarget = ['y']\npredicted = ['PREDICTED']\n\nX_train = train_data_scaled[inputs].copy()\ny_train = train_data_scaled[target].copy()\n\nX_test = test_data_scaled[inputs].copy()\ny_test = test_data_scaled[target].copy()\n\nknn_model = KNeighborsRegressor(n_neighbors=k)\n\nknn_model.fit(X_train, y_train)\n\ny_pred1 = knn_model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred1)\nr2score = r2_score(y_test, y_pred1)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R-Squared Score is (r2_score): {r2score}\")\n\n# Optionally, you can also plot the predicted vs. actual values\nplt.scatter(y_test, y_pred1)\nplt.xlabel(\"Actual Values (y_test)\")\nplt.ylabel(\"Predicted Values (y_pred)\")\nplt.title(\"Actual vs. Predicted Values\")\nplt.show()\n\nmse = []\nfor i in range(1,101,2):\n    knn = KNeighborsRegressor(n_neighbors=i,  metric='euclidean')\n    knn.fit(X_train, y_train)\n    y_pred2 = knn.predict(X_test)\n    mse_1 = mean_squared_error(y_test,y_pred2)\n    mse.append(mse_1)\n    print(f\"For k={i} Mean Squared Error (MSE) is: {mse_1}\")  \n\n# ## Analysis\n# For KNN regressor, the MSE is approximately 0.9916 and  the r squared value is approximately 0.0056, which is very close to zero. This suggests that the KNN model is not able to capture much of the variance in the data.\n# For linear regression, the MSE is approximately 1.001 and  the r squared value is approximately  -0.004, which is very close to zero. This suggests that the linear regression model is not performing well in capturing the variance in the data.\n# Both the models high MSE values, indicating that neither model is providing very accurate predictions.\n# The R squared scores for both models are very close to zero, indicating that neither model is able to explain much of the variance in the data. This could suggest that the relationship between the features and the target variable might not be well captured by a linear regression or KNN model.\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, and performed the train/test split. Your preprocessing steps are correct, and you have correctly fitted both a linear regression and KNN regressor to the data. Your discussion of the results is also thorough and insightful. However, you lost points because you did not use the RMSE for both models in your discussion. Also, your code could be more readable with better use of comments and markdown explanations. Keep up the good work!"
    },
    {
        "SID": "5139627",
        "Name": "pabbapavankumargoud",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ### PAVAN KUMAR PABBA - U03741351\n# ### ASSIGNMENT-04\n# ### Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(42)\n\n# ### Loading the Data\ndf=pd.read_csv('C:/Users/pabba/Downloads/sample.csv')\n\ndf.head(10)\n\n# ### Explore the Data\nnum_rows = len(df)\nnum_cols = len(df.columns)\nprint(f\"Number of Rows: {num_rows}\")\nprint(f\"Number of Columns: {num_cols}\")\nprint(df.head(10))\ndf.plot.scatter(x='x', y='y')\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# ### Creating Scatter Plot for Data with dataframes x and y\nX=df[['x']] #Creating dataframe X\ny=df[['y']] #Creating dataframe y\n\n#scatter plot for x and y\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,y,color='orange')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.title('Scatter Plot of the Data')\nplt.show()\n\n# ### Linear Regression Model\nlin_reg=LinearRegression().fit(X,y)\n\n### Scatter plot for Linear Regression\nfig=plt.figure() \nf=fig.add_subplot()\nf.scatter(X,y,color='blue')\nf.scatter(X,lin_reg.predict(X),color='red')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.title('Scatter Plot of the Data')\nplt.show()\n\nb0=lin_reg.intercept_[0] # intercept\nb1=lin_reg.coef_[0][0] #Coefficeint\nprint(\"Y = {:.2f}+{:.2f}x\".format(b0, b1)) # Linear Regression equation\nprint(\"Linear Regression R-Square Score: {:.3f}\".format(lin_reg.score(X, y))) # R-Squared Score of the model\n\n# Mean Square Error\ny_prd = lin_reg.predict(X)  \nmse = mean_squared_error(y, y_prd)\nprint(f\"Linear Regression MSE: {mse}\")\n\n# ### KNN-Regression\nscaler = StandardScaler()\ndf['x'] = scaler.fit_transform(df[['x']])\n\nX = df[['x']]\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nknn_reg = KNeighborsRegressor(n_neighbors=14)\nknn_reg.fit(X_train, y_train) \n\n# Visualize the predictions\nplt.scatter(X_train, y_train, color='blue', label='Actual')\nplt.scatter(X_test,y_pred_knnr, color='red', label='KNN Regression Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('K-Nearest Neighbors (KNN) Regression')\nplt.show()\n\ny_pred_knnr = knn_reg.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_knnr)\nprint(f\"KNN Regressor MSE:{mse}\")\nR2_knn = r2_score(y_test, y_pred_knnr)\nprint(f\"KNN R-squared: {R2_knn}\")\n\ny_scaled = pd.DataFrame(y_scaled, columns=['y'])  \ny_scaled['predicted'] = knn_model.predict(X_scaled)\nresult = y_scaled[['y', 'predicted']].head()\nprint(result)\n\n# ### Obervations:\n# Mean Square calculates how close the predicted values are to the actual values.Lower Mean Square Error means the data fits well in that model.R-squared provides a measure of how well the independent variables in a regression model explain the variability in the dependent variable. Higher R-Square value means data fits well.\n# For the above problem we got metrics as follows:\n# Linear Regression Mean Square Error: 751.6514909881099 and KNN Regressor MSE:844.3794733937543\n# Linear Regression R-Square Score: -0.043 and KNN R-squared: -0.18382794613844466.\n# According to R-Square and MSE metrics, in this problem The Linear Regression model performs better and it is considered better than KNN Regression model with less error and more accuracy.\n\n\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on the assignment! You have successfully completed all the tasks. You imported the necessary libraries, loaded and explored the data, preprocessed the data, and split it into training and testing sets. You also successfully fitted both a linear regression and KNN regressor to the data and evaluated them using the RMSE metric. Your discussion of the results was also thorough and insightful. However, you could improve your notebook by adding a title and a brief introduction. Also, remember to check if all imported libraries are used in the code. Keep up the good work!"
    },
    {
        "SID": "4992571",
        "Name": "patilsafrin",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Assignment04 SafrinPatil-U88384854\n# **Fit a linear regression and KNN regressor to the data**\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nfrom random import randint\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('./sample.csv')\n\ndf.head(10)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# ## Step 4 :  Visualize the data\n# visualize the data\nimport matplotlib.pyplot as plt\n\n_ = plt.figure()\nplt.scatter(df['x'], df['y'], marker='o', color='red')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of Data')\nplt.show()\n\nX = df[['x']]\ny = df['y']\n\nprint(type(X), type(y))\n\n\n# ## Step 5 :  Conduct the Train-Test Split \nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(X_train.shape, X_test.shape,y_train.shape,y_test.shape)\n\n# ## Step 4 :  Fit the model\n# Finally, we fit the model. We will use the LogisticRegression model from sklearn.linear_model.\n# Fit a Linear Regression model\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nprint(model.score(X_train, y_train))\n\n\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults.head()\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Calculate evaluation metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"R-squared (R2) Score:\", r2)\n\n# ### Discuss regression model results\n# 1.The average absolute error of your model's predictions is approximately 23.036 units.\n# 2.The average squared error of your model's predictions is approximately 716.139 units squared.\n# 3.The square root of the average squared error is approximately 26.761 units.\n# The R-squared score is approximately -0.004, which is quite low and indicates that the model does not explain much of the variance in the target variable. It suggests that the model is not performing well and may not be a good fit for the data.\n# The MAE, MSE, and RMSE values suggest that the model's predictions have a considerable amount of error. Additionally, the negative R-squared score indicates that the model is not explaining much of the variance in the target variable, which implies that the model's predictions are not closely aligned with the actual values. \n# ## Step 6 :  Fit a KNN Regressor model\n# Fit a KNN Regressor model\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nknn_reg = KNeighborsRegressor(n_neighbors=2)  \nknn_reg.fit(X_train, y_train)\n\n# Make predictions\ny_pred_linear = model.predict(X_test)\ny_pred_knn = knn_reg.predict(X_test)\n\n# Calculate Mean Squared Error for both models\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nmse_knn = mean_squared_error(y_test, y_pred_knn)\n\n\nprint(\"Linear Regression Mean Squared Error:\", mse_linear)\nprint(\"KNN Regressor Mean Squared Error:\", mse_knn)\n\n\nimport matplotlib.pyplot as plt\n\n_ = plt.figure()\nplt.scatter(X_test, y_test, label='Test Data', marker='o', color='blue')\nplt.plot(X_test, y_pred_linear, label='Linear Regression', color='red')\nplt.scatter(X_test, y_pred_knn, label='KNN Regressor', marker='s', color='green')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Scatter Plot of Test Data and Model Predictions')\nplt.show()\n\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Calculate evaluation metrics\nmae = mean_absolute_error(y_test, y_pred_knn)\nmse = mean_squared_error(y_test, y_pred_knn)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred_knn)\n\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"R-squared (R2) Score:\", r2)\n\n# ## Step 7: Analyze the KNN Regressor model's performance\n# 1.The average absolute error of your KNN Regressor model's predictions is approximately 23.641 units.\n# 2.The average squared error of your KNN Regressor model's predictions is approximately 1119.312 units squared. \n# 3.The square root of the average squared error is approximately 33.456 units.\n# The R-squared score is approximately -0.569, which is negative and indicates that the model's predictions are performing worse than a horizontal line. Essentially, this means that the model is not explaining any variance in the target variable and is performing poorly.\n# Overall, the results suggest that the KNN Regressor model is not performing well on the given dataset. The MAE, MSE, and RMSE values indicate a significant amount of error in the predictions, and the negative R-squared score suggests that the model is not explaining any variance in the target variable. \n# ## Step 8: Conclusion\n# Neither model appears to provide a good fit to the data since both have negative R-squared scores, indicating that they fail to explain much of the variance in the target variable.\n# The Linear Regression model generally performs better than the KNN Regressor model in terms of MAE, MSE, and RMSE. It has smaller errors in its predictions.However, the negative R-squared scores suggest that there may be fundamental issues with the model choice or the features used in the modeling process.\n# It's important to revisit the model selection and possibly consider more complex models or explore additional features to improve model performance.\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, conducted pre-processing, and split the data into training and testing sets. You have also fitted both a linear regression and KNN regressor to the data, evaluated them using RMSE, and discussed the results. However, you did not discuss your selection of k value for the KNN model, which is a requirement for this assignment. Please remember to include this in your future assignments. Overall, your code is clean and well-commented, and your explanations are clear and concise. Keep up the good work!"
    },
    {
        "SID": "5187493",
        "Name": "peddireddylavanya",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## Assignment 4\n# ## Lavanya Peddireddy\n# ## U00142531\n# ### Import libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# ### Linear Regression\n# ####  Load the data to a CSV file\ndf = pd.read_csv(\"C:/Users/lavan/Downloads/sample (2).csv\")\n\nprint(df.head())\n\nX=df[['x']]\ny=df[['y']]\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,y,color='blue')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nlin_reg=LinearRegression().fit(X,y)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,y,color='blue')\nf.scatter(X,lin_reg.predict(X),color='red')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nb0=lin_reg.intercept_[0]\nb1=lin_reg.coef_[0][0]\n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X,y):.3f}\")\n\n# ### KNN Regressor\ndf = pd.read_csv(\"C:/Users/lavan/Downloads/sample (2).csv\")\n\nknn_data= df[['x','y']]\nknn_data\n\nknn_data=(knn_data - np.min(knn_data))/(np.max(knn_data)-np.min(knn_data))\n\nknn_ind=knn_data.drop('y',axis=1)\nknn_dep=knn_data['y']\nind_train,ind_test,dep_train,dep_test = train_test_split(knn_ind,knn_dep,test_size=0.2,random_state=18)\nknn_model=KNeighborsRegressor().fit(ind_train,dep_train)\npred_val = knn_model.predict(ind_test)\npred_val\n\npredict_df=pd.DataFrame({'Actual':dep_test,'Predicted':pred_val})\npredict_df.shape\n\npredict_df = (predict_df * np.std(df.y) + (np.mean(df.y)))\n\nprint(predict_df.head())\n\n# Visualize actual vs. predicted values\nfig=plt.figure()\nplt.scatter(dep_test, pred_val)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs. Predicted Values (KNN Regression)')\nf.scatter(X,y,color='red')\nf.scatter(X,knn_model.predict(X),color='blue')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\nprint(\"Mean Squared Error = \", mean_squared_error(predict_df.Predicted, predict_df.Actual))\nprint(\"Root Mean Squared Error = \", np.sqrt(mean_squared_error(predict_df.Predicted, predict_df.Actual)))\n\nr2_score(predict_df.Predicted, predict_df.Actual)\n\n# ### Summary\nThe mean square error value is low,its says that the model is accurate.Hence, the knn model fits the data.\nIf the eooro value is more then data doesnt fit the model as it is not accurate.\n\n",
        "Points": 85.0,
        "Comments": "The assignment is well done with all the necessary steps followed. The libraries were imported correctly, data was loaded and explored, and both Linear Regression and KNN models were implemented. However, there are a few areas that need improvement. The notebook lacks a title and brief introduction. Also, the data preprocessing and train/test split steps were not clearly explained. The student did not use StandardScaler() for post-split data preprocessing. The discussion of results could be more detailed, explaining why one model might be preferred over the other. Lastly, the overall presentation could be improved with more comments and markdown explanations."
    },
    {
        "SID": "5097469",
        "Name": "purumrupesh",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# #  1. Fitting a linear model to the sample.csv\n# **Week06, Assignment04**\n# ISM6136\n# Submitted by `Rupesh Purum`\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n#for scores, confusionmatrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n\n#For error finding\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n#For Train/Test data split  \nfrom sklearn.model_selection import train_test_split\n\n\n\n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(\"C:\\\\Users\\\\rupesh\\\\Documents\\\\DataMining\\\\week06\\\\assignment04\\\\sample.csv\")\ndf.head(4)\n\ndf.shape\n\n#visualize the data\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df['x'], \n           df['y'], \n           marker='o', \n           color='red')\n\nax.legend([\"x\"], framealpha=0.5)\n                                         \n# ax.set_xlim(20, 120)\n# ax.set_ylim(13, 25)\nax.set_xlabel('X')\nax.set_ylabel('y')\n\n# ## Step 3: Partitioning the data into training and validation sets\n# Split the dataset into training (80%) and validation (20%) sets using scikit-learn\n\n# random_state is set to a defined value to get the same partitions when re-running the code\ntrain_data, valid_data = train_test_split(df, test_size=0.20, random_state=42)\nprint('Training   : ', train_data.shape)\nprint('Validation : ', valid_data.shape)\n\n\nX_train = train_data[['x']].copy()\ny_train = train_data[['y']].copy()\n\nX_valid = valid_data[['x']].copy()\ny_valid = valid_data[['y']].copy()\n\n# ## Step 4: Normalising the data\n# Normalising/Scaling the data to prevent the model overfit. This is done post splitting the data into train/test datasets because it is best if the model does not have any knowledge about the test data.\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train),\n                       index=X_train.index, columns=X_train.columns)\nX_valid_scaled = pd.DataFrame(scaler.fit_transform(X_valid),\n                       index=X_valid.index, columns=X_valid.columns)\n\n\n# ## Step 4: Fitting a linear model to the data\nlin_reg = LinearRegression().fit(X_train_scaled, y_train)\n\ny_predicted = lin_reg.predict(X_valid_scaled)\n\nb0 = lin_reg.intercept_[0]\nb1 = lin_reg.coef_[0][0]\nr2 = lin_reg.score(X_valid_scaled, y_valid)\n\n\n#Finding coefficients and R squared Error\nprint(f\"y = {b0:.2f} + {b1:.2f}X\")\nprint(f\"R^2: {r2:.3f}\") \n\n#Finding Mean Squared Error\nMSE = mean_squared_error(y_valid['y'], y_predicted) \n\n#Root Mean Squared Error\nRMSE = np.sqrt(MSE)\nprint(f\"RMSE: {RMSE}\")\n\n\n\n\n# fig = plt.figure()\n# ax = fig.add_subplot(projection='3d')\n\n# ax.scatter(features_scaled, target, color='blue')\n# ax.scatter(features_scaled, lin_reg.predict(features_scaled), color='red')\n\n# ax.set_xlabel('X')\n# ax.set_ylabel('y')\n# plt.tight_layout()\n# plt.show()\n\n\n\n# # 2. Fitting a KNN regressor to the sample.csv\n# import required packages\n\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\n\nnp.random.seed(42)\n\n# NOTE: The code below will keep \"Future Warnings\" from cluttering up the notebook. You can comment out the code below to see the warnings.\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# ## Set k value\nk_value = 5 # we'll check these number of neighbors \n\ndf.describe()\n\n\n\nneigh = KNeighborsRegressor(n_neighbors=k_value)\nneigh.fit(X_train_scaled, y_train)\n\ny_knn_predicted = neigh.predict(X_valid_scaled)\n\nR2_knn_regressor = neigh.score(X_valid_scaled, y_valid)\n\nR2_knn_regressor\n\nMeanSquaredError = mean_squared_error(y_valid, y_knn_predicted)\n\nRMSE = np.sqrt(MeanSquaredError)\n\nprint(RMSE)\n\n# Among the input data provided, 80% is used for training and remaining 20% is used for testing. Input data is scaled to have the best model prediction. \n# The obtained linear model is y = 47.22 + -0.47X.\n# The Coefficient of Determinition(R^2), Root Mean Squared Values are obtained as -0.0005, 26.7722 repectively.\n# K_value is set to 5 for the KNN regressor and the Coefficient of Determinition(R^2), Root Mean Squared Values are obtained as -0.1259, 28.3387 repectively. \n# Both models does not seem to fit the data properly as the R^2 is far away from 1.\n",
        "Points": 90.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded the data, and explored it. You also correctly split the data into training and validation sets, normalized the data, and fitted both a linear model and a KNN regressor to the data. You also correctly calculated the RMSE for both models. However, you did not provide a detailed discussion of the results and the performance of the models. Also, the overall presentation of the notebook could be improved by adding more comments and markdown explanations. Keep up the good work and try to improve on these areas in your future assignments."
    },
    {
        "SID": "5105857",
        "Name": "reddipogusravanth",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # U39202669_Assignment04\n# Week05, Fitting a linear regression and KNN regressor \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ## Step 1: Load the given data that we will model\ndata = pd.read_csv(\"E:/MASTER'S - USF/Fall 23'/Data Mining/Assignment 4/sample.csv\")\n\ndata.head()\n\n# ## Step 2: Data Exploration\ndata.describe()\n\n# # Checking if we can split data to Train and Test \n# When dealing with limited data, performing a train-test split can further reduce the size of the training set. This reduction hampers the model's ability to effectively learn patterns from the data, yielding suboptimal parameters. In such instances, the learned parameters might not accurately represent the underlying relationships in the dataset, and the model's predictive performance may suffer.\n# Therefore, given the scarcity of data, refraining from a train-test split is a prudent decision. Instead, utilizing the entire dataset for training allows the model to glean as much information as possible from the available samples, potentially resulting in a more robust and reliable model.\ndata.shape\n\ndata.isnull().sum()\n\n# With the absence of any null values in the dataframe, we can proceed confidently. Moreover, since the dataframe comprises only two columns, we can straightforwardly designate one column as the feature (X) and the other as the target (Y). This streamlined structure simplifies the process of preparing the data for training and enhances the clarity of our feature-target relationships.\ndata.columns\n\n# ## Step 3: Visualizing the data\nplt.scatter(data['x'], data['y'],color='green')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter Plot of X vs Y')\nplt.show()\n\nX = data[['x']]\ny = data[['y']]\n\n# ## Step 4: Standardize the input data \nscaler = StandardScaler()\nX_scalerr = scaler.fit_transform(X)\ny_scalerr = scaler.fit_transform(y)\n\n# # Linear Regression\nlinear_regrss=LinearRegression().fit(X_scalerr,y_scalerr)\n\ny_predicted = linear_regrss.predict(X_scalerr)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scalerr,y_scalerr,color='green')\nf.scatter(X_scalerr,linear_regrss.predict(X_scalerr),color='grey')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n#Extract the Intercept (b0) from the Linear Regression Model:\nb0=linear_regrss.intercept_[0]\n\nb1=linear_regrss.coef_[0][0]   # Accessing the coefficient directly without an index\n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {linear_regrss.score(X,y):.3f}\")\nmean_sq_err = mean_squared_error(y_scalerr, y_predicted)\nprint(\"Mean Squared Error:\",mean_sq_err)\n\n# From the data, we can observe that the R2 score is negtive which means that the predictions of the model are performing worse than a simple horizontal line such as the mean of the dependent variable for all the data points. A negative R2 score suggests that the model's predictions are not explaining the variance in the data and are performing poorly. To summarize, the model is not providing meaningful insights or predictions for the data.\n# # KNN Regression\n# A KNN regression model with 13 neighbors using scaled input features (X_scaled), and then it predicts the target variable (pred_val) using the same scaled features.\nknn_model= KNeighborsRegressor(n_neighbors = 13).fit(X_scalerr,y_scalerr)\npredicted_value = knn_model.predict(X_scalerr)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scalerr,y_scalerr,color='green')\nf.scatter(X_scalerr,knn_model.predict(X_scalerr),color='blue')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n\nmean_sq_err = mean_squared_error(y_scalerr, predicted_value)\nR2_score = r2_score(y_scalerr,predicted_value)\n\nprint(\"Mean Squared Error:\",mean_sq_err)\nprint(\"R2 Value:\",R2_score)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scalerr,y_scalerr,color='violet')\nf.scatter(X_scalerr,knn_model.predict(X_scalerr),color='blue')\nf.scatter(X_scalerr,linear_regrss.predict(X_scalerr),color='green')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# We can observe that the Mean Square Error values of both the models are almost same and are higher. Higher MSE value indicates poor model performance.  \n# As the R2 score of the linear regression model is negative, it indicates that chosen model is a bad fit.\n# Based on the date, we can conclude that: KNN performance > Linear Regression performance.\n# The models' poor performance could be due to the complexity of the data's shape and an insufficient number of data points.\n",
        "Points": 95.0,
        "Comments": "The student has done a good job on this assignment. The notebook is well-structured and the code is readable. The student has imported all the necessary libraries, loaded and explored the data, and conducted preprocessing. Both linear regression and KNN regressor models were fitted and evaluated, and the results were discussed. However, the student did not use the train/test split function, which is a standard practice in machine learning to evaluate the model's performance on unseen data. This is the only area where improvement is needed."
    },
    {
        "SID": "5120719",
        "Name": "sangoivineshnarendra",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## Import required libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n\n# ## Load data which we will model\ndf=pd.read_csv('./sample.csv')\n\ndf.head()\n\nX=df[['x']]\ny=df[['y']]\n\nfigure=plt.figure()\nfig=figure.add_subplot()\nfig.scatter(X,y,color='red')\nfig.set_xlabel('x')\nfig.set_ylabel('y')\nplt.show()\n\n# ## Fitting the model\nlin_reg=LinearRegression().fit(X,y)\n\n# ## Analyzing the linear regression model's performance\nfigure=plt.figure()\nfig=figure.add_subplot()\nfig.scatter(X,y,color='red')\nfig.scatter(X,lin_reg.predict(X),color='blue')\nfig.set_title('Linear Regression')\nfig.set_xlabel('x')\nfig.set_ylabel('y')\nplt.show()\n\nb0=lin_reg.intercept_[0]\nb1=lin_reg.coef_[0][0]\n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X,y):.3f}\")\n\n# ## KNN Regressor\nknn = df[['x','y']]\nknn\n\nknn.isna().sum()\n\n# ## Normalising the data\nknn=(knn - np.min(knn))/(np.max(knn)-np.min(knn))\n\nknn_ind=knn.drop('y',axis=1)\nknn_dep=knn['y']\n\nind_train,ind_test,dep_train,dep_test = train_test_split(knn_ind,knn_dep,test_size=0.2,random_state=18)\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn_model=KNeighborsRegressor().fit(ind_train,dep_train)\npred_val = knn_model.predict(ind_test)\npred_val\n\npredict_df=pd.DataFrame({'Actual':dep_test,'Predicted':pred_val})\npredict_df.shape\n\npredict_df = (predict_df*(np.max(df.y)-np.min(df.y)))+np.min(df.y)\npredict_df.head()\n\n# ## Calculating the root mean square values and the r-square values\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint(\"Mean Squared Error = \", mean_squared_error(predict_df.Predicted, predict_df.Actual))\nprint(\"Root Mean Squared Error = \", np.sqrt(mean_squared_error(predict_df.Predicted, predict_df.Actual)))\n\nr2_score(predict_df.Predicted, predict_df.Actual)\n\n# ## Both models on the dataset have poor performance. R square value evaluates how well the linear regression fits the dataset. As it is very close to 0, it indicates that linear regression doesn't fit. For KNN regressor, r- square value is negative indicating poor job in fitting the data. Hence KNN regressor is not an appropriate choice for this dataset.\n\n\n",
        "Points": 85.0,
        "Comments": "The student has done a good job overall. The notebook structure is clear, and the student has imported all the necessary libraries. The data loading and exploration are done well, and the student has also done a good job with the preprocessing and train/test split. The student has fitted both a linear regression and KNN regressor to the data and evaluated their performance using RMSE. The discussion of results is also done well. However, the student did not use the StandardScaler() for post-split data preprocessing and did not discuss their selection of k value for the KNN model. The overall presentation of the notebook is good, but the student could improve by adding more comments and explanations in the markdown cells."
    },
    {
        "SID": "5149461",
        "Name": "satputeneha",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ### Import Libraries\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# ### Load Data\n\ndf = data = pd.read_csv(r\"C:/Users/User/Documents/sample.csv\", index_col=False)\n\n# ### Explore the data\nprint(data.head(8))\n\n\nprint(data.describe())\n\nprint(data.info())\n\n# ### Address any NAN Values\nprint(data.isnull().sum())\n\n# ### Visualize the data\nplt.scatter(data['x'], data['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of Data')\nplt.show()\n\n# ### Separate out the inputs \nscaler = StandardScaler()\ndata['x'] = scaler.fit_transform(data[['x']])\n\n# ### Split the data in to train and test\nX = data[['x']]\ny = data['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n\n\n# ### Fit the linear regression model\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n\n\ny_pred_lr = lr.predict(X_test)\n\nprint('Linear Regression MSE:', mean_squared_error(y_test, y_pred_r))\n\n# Visualize the regression line\nplt.scatter(X, y,label='Actual')\nplt.scatter(X_test, y_pred_lr, color='magenta', label='Linear Regression')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Linear Regression ')\nplt.legend()\nplt.show()\n\nmse_lr = mean_squared_error(y_test, y_pred_lr)\n\nprint(f\"Mean Squared Error: {mse_lr}\")\n\nr2_lr = r2_score(y_test, y_pred_lr)\n\nprint(f\"R-squared: {r2_lr}\")\n\ndf = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred_lr\n})\n\n# Print the DataFrame\n\n\nprint(df.head())\n\n# ### KNN Regressor\ndf = data = pd.read_csv(r\"C:/Users/User/Documents/sample.csv\")\n\nscaler = StandardScaler()\ndata['x'] = scaler.fit_transform(data[['x']])\n\n\n### Split the scaled data \nX = data[['x']]\ny = data['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n\n\n# KNN regression model\n\n\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\n\ny_pred_knnr = knn_reg.predict(X_test)\n\nprint('KNN Regressor MSE:', mean_squared_error(y_test, y_pred_knnr))\n\n\n\n\n# Calculate the Mean Squared Error for KNN regression\nmse_knn = mean_squared_error(y_test, y_pred_knn)\nprint(f\"Mean Squared Error: {mse_knn}\")\n\n# Calculate the (R2) score for KNN regression\nr2_knn = r2_score(y_test, y_pred_knn)\nprint(f\"R-squared: {r2_knn}\")\n\n\n\n# Visualize the predictions\nplt.scatter(X, y,label='Actual')\nplt.scatter(X_test, y_pred_knn, color='red', label='KNN Regression')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('KNN Regression ')\nplt.legend()\nplt.show()\n\n\ndf = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred_knn\n})\n\n# Print the DataFrame\nprint(df.head())\n\n# OBSERVATION:\n# Linear Regression vs. KNN Regression:\n# a)MSE Comparison:\n# MSE measures how close predictions are to actual values.\n# Lower MSE indicates a better fit.\n# Linear regression has a lower MSE than KNN regression.\n# b)R-squared Comparison:\n# R-squared gauges how much of the target variables variability the model accounts for\n# Higher R-squared signifies a better fit.\n# Linear regression has a higher R-squared than KNN regression.\n# Performance Summary:\n# Linear regression outperforms KNN regression in this scenario.\n# It exhibits less error (lower MSE) and higher accuracy (higher R-squared).\n# Considerations:\n# Model complexity and interpretability are essential considerations.\n# While metrics like MSE and R-squared provide insights, they are not the sole criteria for model evaluation.\n# It's crucial to assess how easy it is to comprehend the results and the complexity of the model.\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, handled missing values, and visualized the data. You also correctly implemented the train/test split, fitted both a linear regression and KNN regressor model, and evaluated them using RMSE. Your discussion of the results was also thorough and insightful. However, you lost points for not discussing your selection of k value for the KNN model. Also, there was a minor error in your code where you used a variable 'y_pred_r' which was not defined. Please ensure to check your code for errors before submission. Keep up the good work!"
    },
    {
        "SID": "5091285",
        "Name": "sunkarashwetha",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Shwetha_Sunkara_Assignment- 4\n# Fitting a linear regression and KNN regressor \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ## Step 1: Load the given data that we will model\ndata = pd.read_csv(\"./sample.csv\")\n\ndata.head()\n\n# ## Step 2: Data Exploration\ndata.info()\n\ndata.describe()\n\n# # Checking if we can split data to Train and Test \n# We generally perform a train-test split to further reduce the size of the training set. The model's capacity to successfully identify patterns in the data is hampered by this reduction, leading to inaccurate parameters. \n# As a result, it would be wise to avoid a train-test split given the lack of data. Instead, making use of the complete dataset for training enables the model to extract as much data from the available samples as possible, potentially producing a more robust and accurate model.\ndata.shape\n\ndata.isnull().sum()\n\n# As we can observe there are no null values in the dataframe, we can move forward with fitting the model confidently. Moreover, since the dataframe comprises only two columns, we can straightforwardly designate one column as the feature (X) and the other as the target (Y). This streamlined structure makes the process of preparing the data for training easier.\ndata.columns\n\n# ## Step 3: Visualizing the data\nplt.scatter(data['x'], data['y'],color='red')\nplt.xlabel('X DATA')\nplt.ylabel('Y DATA')\nplt.title('Scatter Plot of X vs Y')\nplt.show()\n\nX = data[['x']]\ny = data[['y']]\n\n# ## Step 4: Standardizing the input data \nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ny_scaled = scaler.fit_transform(y)\n\n# # Linear Regression\nlin_reg=LinearRegression().fit(X_scaled,y_scaled)\n\ny_pred = lin_reg.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='blue')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n#Extract the Intercept (b0) from the Linear Regression Model:\nb0=lin_reg.intercept_[0]\n\nb1=lin_reg.coef_[0][0]   # Accessing the coefficient directly without an index\n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X,y):.3f}\")\nMean_square_err = mean_squared_error(y_scaled, y_pred)\nprint(\"Mean Squared Error:\",Mean_square_err)\n\n# The model's predictions appear to perform worse than a straightforward horizontal line (the dependent variable's mean), according to the negative R2 Score. We can observe that the model isn't working well which indicates that it's not the right model for the data.\n# # KNN Regression\n# A KNN regression model with 13 neighbors using scaled input features (X_scaled), and then it predicts the target variable (pred_val) using the same scaled features.\nKNN = KNeighborsRegressor(n_neighbors = 13).fit(X_scaled,y_scaled)\npred_val = KNN.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,KNN.predict(X_scaled),color='blue')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n\nmse = mean_squared_error(y_scaled, pred_val)\nR2 = r2_score(y_scaled,pred_val)\n\nprint(\"Mean Squared Error:\",mse)\nprint(\"R2 Value:\",R2)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,KNN.predict(X_scaled),color='blue')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='green')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# The higher Mean Square Error indicates bad performance of both the models. The negative R2 score of the linear regression model also indicates a bad fit of the linear regression. \n# Overall we can say that KNN performance > Linear Regression performance. \n\n\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully loaded the data, explored it, preprocessed it, and fitted both a linear regression and KNN regressor model. You also evaluated the models using RMSE and discussed the results. However, you did not split the data into training and testing sets, which is a crucial step in evaluating the performance of a model. Also, you used the entire dataset for scaling, which can lead to data leakage. Remember to only fit the scaler on the training data and then transform both the training and testing data. Keep up the good work!"
    },
    {
        "SID": "5080075",
        "Name": "tanuboddivenkatasaisatvikreddy",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Data Mining with Python: LinearRegression and KNN Regressor\n# ## Import required packages\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ## Loading the Dataset\ndf = pd.read_csv(r'C:\\Users\\sathw\\Downloads\\Data mining\\W6\\A4\\sample.csv')\n\n# ## Exploring the Dataset\ndf.head(10) # Display the first 10 rows of the dataset\n\ndf.shape # Check the shape of the dataset\n\n# Display descriptive statistics for the 'x' and 'y' columns\nprint(df.x.describe())\nprint(df.y.describe())\n\n# ### Check for missing and unique values\n#Check for missing values\ndf.isna().sum() # No Missing values\n\n# Display unique values in the 'x' and 'y' columns\ndf['x'].unique()\ndf['y'].unique()\n# there are many unique values\n\n# ## Data Splitting\n# Split the data into training and testing sets\nX = df[['x']]\ny = df[['y']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=None)\n# Taking test size as 20% as the model is working good compare to 10% and 30% Test size\nX_train=X_train.reset_index(drop=True)\nX_test=X_test.reset_index(drop=True)\ny_train=y_train.reset_index(drop=True)\ny_test=y_test.reset_index(drop=True)\n\n# ## Data Visualization\n# scatter plot of 'x' vs. 'y'\nplt.scatter(df['x'], df['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of x vs. y')\nplt.show()\n\n# scatter plot of 'x_train' vs. 'y_train'\nplt.scatter(X_train, y_train)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of x_train vs. y_train')\nplt.show()\n\n# ## Data Standardization\n# scale the numeric features\nnumeric_cols = ['x']\nscaler = StandardScaler()\nscaler.fit(X_train[numeric_cols])\nX_train[numeric_cols] = scaler.transform(X_train[numeric_cols])\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n\nscaler = StandardScaler()\nscaler.fit(y_train[['y']])\ny_train['y'] = scaler.transform(y_train[['y']])\ny_test['y'] = scaler.transform(y_test[['y']])\n\n\n\n# scatter plot of 'x_train' vs. 'y_train'\nplt.scatter(X_train, y_train)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of x_train vs. y_train')\nplt.show()\n\n# ## Linear Regression Modeling\n# Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Evaluate the Linear Regression model\nlr_r2_score = lr.score(X_test,y_test) #R-squared score\n\nlr_r2_score\n\n# calculating mean squared error of the regression model\nmean_squared_error_reg = np.sqrt(mean_squared_error(y_test,lr.predict(X_test)))\nmean_squared_error_reg\n\nX_train.shape\n\n# ## K-Nearest Neighbors (KNN) Regression Modeling\n# Fit a KNN Regression model\nknn = KNeighborsRegressor(n_neighbors=10, p = 2, weights='uniform')# # added the weights to the knn model to improve the model performence\nknn.fit(X_train, y_train)\n\n#  Make predictions with KNN Regression\ny_pred = knn.predict(X_test)\n\n# Calculate KNN Regression mean squared error\nmean_squared_error_knn = np.sqrt(mean_squared_error(y_test,y_pred))\n\nr2_score_knn = r2_score(y_test, y_pred)\n\n# Print the R-squared score for KNN Regression\nprint(\"KNN R-squared Score:\", r2_score_knn)\n\n# compare mean squared errors and r2 score\nprint(\"Linear Regression R Squared Score:\", lr_r2_score )\nprint(\"Linear Regression Mean Squared Error:\", mean_squared_error_reg)\n\nprint(\"KNN Mean Squared Error:\", mean_squared_error_knn)\nprint(\"KNN R Squared Score:\",r2_score_knn)\n\n\n# ## Visualization of Predictions\n# comparing train, test, Knn Regression Predictions and Liner Regression Predictions data by scatter plotting \nplt.figure(figsize=(18, 16))\n\n\ncoefficients = lr.coef_  \nintercept = lr.intercept_  \nX = X_test.values\n# Generate x values for the line\nx_line = np.linspace(min(X), max(X), 100)  \n\n# Calculate y values using the linear equation\ny_line = coefficients * x_line + intercept\n\n# Plot the data\nplt.scatter(X_test, y_test, label='test', color='magenta')\nplt.scatter(X_train, y_train, label='train', color='blue')\nplt.scatter(X_test, lr.predict(X_test), label='Liner Regression Predictions', color='green')\nplt.scatter(X_test, knn.predict(X_test), label='Knn Regression Predictions', color='orange')\n\n# assigning the numbers to the points\nfor idx,index, row in zip(range(X_test.shape[0]),X_test.values,y_test.values): # look through each row of our data\n    plt.text(s = idx, x = index,y = row)\n\nfor idx,index, row in zip(range(X_train.shape[0]),X_train.values,y_train.values): # look through each row of our data\n    plt.text(s = idx, x = index,y = row)\n    \nfor idx,index, row in zip(range(X_test.shape[0]),X_test.values,y_pred): # look through each row of our data\n    plt.text(s = 'k' + str(idx), x = index,y = row)\n\nfor idx,index, row in zip(range(X_test.shape[0]),X_test.values,lr.predict(X_test)): # look through each row of our data\n    plt.text(s = 'l' + str(idx), x = index,y = row)\n    \n    \n# Plot the regression line\nplt.plot(x_line, y_line,    label='Regression Line', color='red')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Prediction plot')\nplt.legend()\nplt.show()\n\n\n# ## Results and Discussion\n# In this analysis, we explored two regression models: Linear Regression and K-Nearest Neighbors (KNN) Regression, and evaluated their performance on the dataset. Let's discuss the findings and compare the two models.\n# ### Data Exploration\n# We initiated our analysis by loading the dataset and exploring its key characteristics:\n# - The dataset contains 142 data points with two columns, 'x' and 'y'.\n# - Descriptive statistics showed that 'x' has a mean value of approximately 54.26, while 'y' has a mean value of approximately 47.83.\n# - There were no missing values in the dataset.\n# ### Data Splitting\n# We split the dataset into training and testing sets using an 80/20 split ratio as the dataset size is 142 and we choose 80/20 instead of 90/10 as the 80/20 got better r2 score and mse then 90/10\n# ### Data Visualization\n# We visualized the data with scatter plots to understand the relationship between 'x' and 'y'.\n# - The scatter plot of 'x' vs. 'y'.\n# - We also created a scatter plot for the training data ('x_train' vs. 'y_train').\n# ### Data Standardization\n# To prepare the data for modeling, we standardized the 'x' and 'y' values using StandardScaler.\n# ## Model Building\n# ### Linear Regression\n# We built a Linear Regression model to predict 'y' based on 'x'. The model's performance metrics on the test set were as follows:\n# - R-squared score: -0.0040\n# - Mean Squared Error (MSE): 0.9970\n# ### K-Nearest Neighbors (KNN) Regression\n# We also built a KNN Regression model with 10 neighbors and uniform weights. The model's performance metrics on the test set were as follows:\n# - Mean Squared Error (MSE): 1.1256\n# - R-squared score: -0.2797\n# ### Visualization of Predictions\n# We created a scatter plot to visualize the predictions made by both models as well as the regression line generated by the Linear Regression model. The plot shows the test data points, training data points, Linear Regression predictions, and KNN Regression predictions.\n# ## Model Comparison\n# Comparing the two models, we observed that both models performed similarly in terms of Mean Squared Error (MSE). However, the Linear Regression model had a slightly lower MSE, indicating slightly better predictive performance on this data.\n# ## Conclusion\n# In this analysis, we explored and modeled a dataset with 'x' and 'y' variables. Both Linear Regression and KNN Regression models were employed to predict 'y' based on 'x'. While the performance of the two models was similar, the Linear Regression model had a slightly lower Mean Squared Error, suggesting it might be a slightly better choice for this dataset.\n",
        "Points": 95.0,
        "Comments": "The student has done an excellent job on this assignment. The notebook is well-structured with clear headings and explanations. The student has imported all necessary libraries, loaded and explored the data, conducted pre and post-split preprocessing, and fitted both a linear regression and KNN regressor to the data. The student has also evaluated the models using RMSE and discussed the results. The only area for improvement is to avoid fitting the test data during preprocessing. Overall, great work!"
    },
    {
        "SID": "5101315",
        "Name": "thotasrija",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# Importing Libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(42)\n\n#Loading Data in to the system\ndf = pd.read_csv('sample.csv', index_col=False)\n\n#Exploring the Data\nnum_rows = len(df)\nnum_columns = len(df.columns)\nprint(f\"Number of Rows: {num_rows}\")\nprint(f\"Number of Columns: {num_columns}\")\nprint(df.head(8))\n\ndf.plot.scatter(x='x', y='y')\n\n#NAN Values\nmissing_values = df.isnull().sum()\nprint(missing_values)\n\n#Visualizing Data\nplt.scatter(df['x'], df['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of the Data')\nplt.show()\n\n#Separating the Inputs\nX = df[['x']].values\ny = df[['y']].values\n\n#Separate the Data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.31, random_state=50)\n\n#Fit the Linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\ny_pred_r = lin_reg.predict(X_test)\nprint('Linear Regression MSE:', mean_squared_error(y_test, y_pred_r))\n\n# Visualize regression line\nplt.scatter(X_train, y_train, color='b', label='Actual')\nplt.plot(X_test, y_pred_r, color='y', label='Linear Regression Prediction')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Linear Regression')\nplt.show()\n\n#KNN Regression\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train, y_train)\n\ny_pred_knnr = knn_reg.predict(X_test)\nprint('KNN Regressor MSE:', mean_squared_error(y_test, y_pred_knnr))\n\n# Visualize predictions\nplt.scatter(X_train, y_train, color='y', label='Actual')\nplt.scatter(X_test,y_pred_knnr, color='b', label='KNN Regression Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('K-Nearest Neighbors(KNN)Regression')\nplt.show()\n\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, and preprocessed it. You also correctly split the data into training and testing sets, fitted both a linear regression and KNN regressor model, and evaluated them using the mean squared error. You also visualized your data and results well. However, you did not provide a brief introduction or a discussion of your results. Remember to include these in your future assignments for full points."
    },
    {
        "SID": "5099545",
        "Name": "tirumalasettyvarunkumar",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\ndata = pd.read_csv(\"sample.csv\")\n\ndata.head()\n\n#  Data Exploration\ndata.describe()\n\ndata.shape\n\ndata.isnull().sum()\n\ndata.columns\n\nplt.scatter(data['x'], data['y'],color='blue')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter Plot of X vs Y')\nplt.show()\n\nX = data[['x']]\ny = data[['y']]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ny_scaled = scaler.fit_transform(y)\n\n# # Linear Regression\nlin_reg=LinearRegression().fit(X_scaled,y_scaled)\n\ny_pred = lin_reg.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='blue')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='red')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n#Extract the Intercept (b0) from the Linear Regression Model:\nb0=lin_reg.intercept_[0]\n\nb1=lin_reg.coef_[0][0]   # Accessing the coefficient directly without an index\n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X,y):.3f}\")\nmse = mean_squared_error(y_scaled, y_pred)\nprint(\"Mean Squared Error:\",mse)\n\n# The negative R2 Score shows that the predictions are inaccurate and the model is inaccurate for the data\n# # KNN Regression\nknn_model= KNeighborsRegressor(n_neighbors = 15).fit(X_scaled,y_scaled)\npred_val = knn_model.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='blue')\nf.scatter(X_scaled,knn_model.predict(X_scaled),color='red')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n\nmse = mean_squared_error(y_scaled, pred_val)\nR2 = r2_score(y_scaled,pred_val)\n\nprint(\"Mean Squared Error:\",mse)\nprint(\"R2 Value:\",R2)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='blue')\nf.scatter(X_scaled,knn_model.predict(X_scaled),color='red')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='green')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# When both Mean squared error and r squared error are taken into consideration, the linear regression model does not fit the given data while the KNN model appears to fit our data set.\n# When we tried linear regression on the data, the r squared error turned out to be negative. This showed that the model is not ideal for our data.\n# When we tried KNN model, it showed a better performance. Hence the KNN model is a better fit for our data\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded the data, and conducted data exploration. Your preprocessing steps and model fitting are also correct. However, you missed setting the random_state for the train/test split which is important for repeatability. Your discussion of the results is clear and well-presented. Keep up the good work!"
    },
    {
        "SID": "5009099",
        "Name": "upparajayanth",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## Assignment 4\n# ### Jayanth Uppara| U46027839\n# ## Linear Regression\n# import neccessary libraries\nfrom sklearn.linear_model import LinearRegression\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n\n# ### Reading the file\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\ndf = pd.read_csv(\"/Users/jay/Documents/Untitled Folder/sample.csv\")\n\ndf.head()\n\n\n# ### Dividing test and train data sets\n# checking if there are any topos, and if so, then fix this issue.\ndf.isna().sum()\n\nX= df[['x']]\ny= df[['y']]\n\nX_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=0)\n\n\n\n# ### Fitting the model\nlin_reg=LinearRegression().fit(X,y)\n\n# ### Predict and calulate Mse, R2 for analysis\ny_predict_linear = lin_reg.predict(X_test)\n\nMSE=mean_squared_error(y_test,y_predict_linear)\nR2 = r2_score(y_test,y_predict_linear)\nprint(\"MSE:\",MSE,\"R2:\",R2)\n\n# ### Plotting the graph\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X, y, color='red')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title(\"Linear Regression Test\")\nplt.tight_layout()\nplt.show()\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X, y, color='red')\nax.scatter(X,lin_reg.predict(X),color='blue')\n\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title(\"Linear Regression Test\")\nplt.tight_layout()\nplt.show()\n\n# ## KNN Regressor\n# ### Fitting the model by knn regressor\nreg=KNeighborsRegressor(math.floor(math.sqrt(len(X))))\n\nreg.fit(X_train,y_train)\n\nyprediction = reg.predict(X_test)\n\n# ### Plotting the graph\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,y,color='red')\nf.scatter(X,reg.predict(X),color='blue')\nf.set_title('KNN Regression')\nf.set_xlabel('X')\nf.set_ylabel('y')\nplt.show()\n\n# ### Calculating MSE and R2 for analysis\nMSE=mean_squared_error(y_test,yprediction)\nR2 = r2_score(y_test,yprediction)\nprint(\"MSE:\",MSE,\"R2:\",R2)\n\n# ### we have larger Mean Square Error (MSE) for KNN regressor model hence, linear regression work well for this prediction, Since higher the MSE means lower the performance rate.\n",
        "Points": 85.0,
        "Comments": "The assignment is well done with all the necessary steps followed. The code is clean and well-commented, and the visualizations are helpful. However, there are a few areas for improvement. The notebook lacks a title and introduction, which are important for providing context to the reader. Also, the LogisticRegression library was imported but not used, which is unnecessary. Lastly, while the student did discuss the results, a more in-depth analysis comparing the two models would have been beneficial. Keep up the good work and consider these points for future assignments."
    },
    {
        "SID": "4992045",
        "Name": "vadlalarohit",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ### Rohit_Vadlala_U93298968_Assignment04\n# Week5: Fitting a linear regression and KNN regressor \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ## Step 1: Load the given data that we will model\ndata = pd.read_csv(r'C:/Users/vadla/Downloads/sample.csv')\n\ndata.head()\n\n# ## Step 2: Data Exploration\ndata.describe()\n\n# # Checking if we can split data to Train and Test \n# While dealing with a limited data, it is difficult to perform a train-test split, as it could further reduce the size of the training set. The reduction in the size of the data could significantly impact the ability of the model to effectively identify the patterns in the data, leading to generate suboptimal parameters. In these scenarios, the identified parameters might not be able to represent the hidden relationships accurately in the data set that are existing in the data set, which ultimately could impact the model's predictive performance negatively. As a result, with the limited data that is available, it became evident to not to choose the path of train-test split. Instead, using the complete dataset for the training would enable the model in extracting as much information as possible from the available samples, thereby aiming to produce a model that is more robust and reliable.\ndata.shape\n\ndata.isnull().sum()\n\n# Here, as there are no null values in the dataframe, we can continue with the process with confidence. Also, the dataframe contains two columns in it, thus we can assign one column as the feature (X) and the other one as the target (Y). Hence, we could say that this streamlined approach could simplify the process of organizing the data for training and has the ability to improve the clarity of the relationship between feature and target.\ndata.columns\n\n# ## Step 3: Visualizing the data\nplt.scatter(data['x'], data['y'],color='red')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('scatter plot: X vs Y')\nplt.show()\n\nX = data[['x']]\ny = data[['y']]\n\n# ## Step 4: Standardize the input data \nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ny_scaled = scaler.fit_transform(y)\n\n# # Linear Regression\nlin_reg=LinearRegression().fit(X_scaled,y_scaled)\n\ny_pred = lin_reg.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='blue')\nf.set_title('Linear Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n#Extract the Intercept (b0) from the Linear Regression Model:\nb0=lin_reg.intercept_[0]\n\nb1=lin_reg.coef_[0][0]   # Accessing the coefficient directly without an index\n\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R2: {lin_reg.score(X,y):.3f}\")\nMSE = mean_squared_error(y_scaled, y_pred)\nprint(\"Mean Squared Error:\",MSE)\n\n# Here, as we can see that the R2 Score is negative, which suggests that the model's predictions are performing poorly than that of the simple horizontal line (which is the mean of dependent variable). To conlcude with, it can be said that the chosen model is not performing well and it is indicating that the model is not suitable for the data which is given.\n# # KNN Regression\n# Applying a KNN regression model here with 13 neighbors utilising scaled input features such as X_scaled, to predict target variable that is pred_val utilising the same features that are scaled.\nknn_regmodel= KNeighborsRegressor(n_neighbors = 13).fit(X_scaled,y_scaled)\npred_val = knn_regmodel.predict(X_scaled)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,knn_regmodel.predict(X_scaled),color='blue')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n\nMSE = mean_squared_error(y_scaled, pred_val)\nR2 = r2_score(y_scaled,pred_val)\n\nprint(\"Mean Squared Error:\",MSE)\nprint(\"R2 Value:\",R2)\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X_scaled,y_scaled,color='red')\nf.scatter(X_scaled,knn_regmodel.predict(X_scaled),color='blue')\nf.scatter(X_scaled,lin_reg.predict(X_scaled),color='green')\nf.set_title('KNN Regression')\nf.set_xlabel('x')\nf.set_ylabel('y')\nplt.show()\n\n# The Mean Squared Error is seeming to be very high in a way which indicated that both the models that were used have performed poorly. The values of the Mean Squared Error for both the models are approximately same.\n# When it comes to the R2 score, the value of R2 score of a linear regression model is negative which directly indicates that it is a bad fit.\n# Here, it can observed that the KNN performance is more better than that of the Linear regression performance (KNN performance > Linear Regression performance).\n# It could be estimated that the poor performance of the models is due to the fact that the existence of the complex data shape and inadequate data points.\n",
        "Points": 85.0,
        "Comments": "The assignment is well-structured and the code is readable. The student has imported all necessary libraries and loaded the data successfully. The data exploration and visualization are done correctly. However, the student did not perform a train/test split, which is a crucial step in model evaluation. The student has justified this decision, but it is still a requirement for this assignment. The student has correctly implemented both the Linear Regression and KNN Regression models and evaluated them using RMSE. The discussion of results is also well done. The student could improve by following all the assignment instructions, including performing a train/test split."
    },
    {
        "SID": "5120275",
        "Name": "vermasaurabhjagdishprasad",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# # Assignment04_Saurabh_Jagdish_Prasad_Verma_U09956219 (Linear regression and KNN regression)\n# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib widget\n\n# Laoding data set\ndf = pd.read_csv(r'C:\\Users\\Saurabh Verma\\OneDrive\\Desktop\\Data Mining\\Assignrmnt - 4\\sample.csv')\n\ndf.head(5)\n\ndf.shape\n\ndf.isna().sum()\n\ndf['y'].value_counts()\n\ndf.info()\n\ndf.head()\n\nduplicates = df.duplicated()\n\nprint(duplicates.sum())\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df['x'],df['y'])\n\n# ## Train Test Split\nX=df[['x']]\ny=df[['y']]\n\nX\n\ny\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=18)\n\nX_train\n\ny_train\n\n# # Linear Regression Model Creation\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train)\n\ny_lin_pred = lin_reg.predict(X_test)\n\ncompare = pd.DataFrame()\ncompare['actual'] = y_test\ncompare['prediction'] = y_lin_pred\ncompare.head()\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_train,y_train, color='red')\nax.scatter(X_train,lin_reg.predict(X_train), color='blue')\n\nax.set_xlabel('input')\nax.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\nfig = plt.figure()\nay = fig.add_subplot()\n\nay.scatter(X_train,y_train, color='red')\nay.scatter(X_train,lin_reg.predict(X_train), color='blue')\nay.scatter(X_test,y_test,color='green')\nay.scatter(X_test,lin_reg.predict(X_test),color='brown')\n\nay.set_xlabel('input')\nay.set_ylabel('target')\nplt.tight_layout()\nplt.show()\n\nb0 = lin_reg.intercept_[0]\nb1 = lin_reg.coef_[0][0]\nprint(f\"Y = {b0:.2f}+{b1:.2f}x\")\nprint(f\"R^2: {lin_reg.score(X,y):.3f}\")\n\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n\nmse = mean_squared_error(y_test,y_lin_pred)\nmae = mean_absolute_error(y_test,y_lin_pred)\nrmse = np.sqrt(mse)\n\nprint(mse)\nprint(mae)\nprint(rmse)\n\n# ### High Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and low accuracy in linear regression with low correlation and a small dataset can be attributed to several reasons:\n# #### Your model is underfitting. \n# This means that the model is not complex enough to capture the relationship between the independent and dependent variables in your dataset. This can happen if your dataset is small or if there is a lot of noise in your data.\n# #### Your model is overfitting. \n# This means that the model is too complex and has learned the noise in your training data. This can also happen if your dataset is small.\n# #### There is a non-linear relationship between the independent and dependent variables in your dataset. \n# Linear regression can only model linear relationships. If the relationship between the variables is non-linear, then linear regression will not be able to accurately predict the dependent variable.\n# #### There are outliers in your dataset. \n# Outliers can skew the results of linear regression and lead to high error metrics.\n# #### Your dataset is too small. \n# Linear regression needs a certain amount of data to train effectively. If your dataset is too small, the model will not be able to learn the relationship between the independent and dependent variables.\n# ### Here are some things you can do to try to improve the performance of your linear regression model:\n# #### Increase the size of your dataset. \n# If possible, try to collect more data for your training set. This will give the model more information to learn from and reduce the risk of overfitting.\n# #### Remove outliers from your dataset. \n# Outliers can skew the results of linear regression, so it is important to identify and remove them from your dataset before training your model.\n# #### Use a more different model. \n# If you suspect that your model is underfitting, you can try using a more complex model, such as a polynomial regression model. However, be careful not to overfit the model.\n# #### Use regularization. \n# Regularization is a technique that can help to prevent overfitting. There are different types of regularization, such as L1 and L2 regularization.\n# #### Use cross-validation. \n# Cross-validation is a technique that can be used to evaluate the performance of a model on unseen data. This can help you to identify which model is performing best and to avoid overfitting.\n# # KNN Regression Model Creation\n# ## Standard Scaler\nfrom sklearn.neighbors import KNeighborsRegressor\n\nX\n\ny\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=18)\n\nX_train\n\nX_test\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train\n\nX_test\n\nregressor = KNeighborsRegressor(n_neighbors = 7,algorithm='auto')\nregressor.fit(X_train,y_train)\n\ny_pred = regressor.predict(X_test)\ny_pred\n\nprint(r2_score(y_test,y_pred))\nprint(mean_absolute_error(y_test,y_pred))\nprint(mean_squared_error(y_test,y_pred))\n\n# ## Min Max scaler\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=20)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nknn1 = KNeighborsRegressor()\nknn1.fit(X_train_scaled, y_train)\n\ny_pred1 = knn1.predict(X_test_scaled)\n\ny_pred1\n\nprint(r2_score(y_test,y_pred1))\nprint(mean_absolute_error(y_test,y_pred1))\nprint(mean_squared_error(y_test,y_pred1))\n\n# # Discussion\n# There could be several underlying factors contributing to the low accuracy and high mean square error observed in both linear regression and KNN models.\n# ## Exploring Factors Behind Low Model Performance\n# The observed low accuracy and elevated mean square error in both linear regression and KNN models may stem from various underlying factors worth considering.\n# ## Leveraging Cross-Validation:\n# Adopting a cross-validation strategy proves invaluable in evaluating machine learning models on unseen data. This approach enables an assessment of model effectiveness and facilitates the fine-tuning of hyperparameters.\n# ## Exploring Alternative Models:\n# The vast landscape of machine learning offers a multitude of model options. Exploring alternatives like decision trees or random forests might yield improvements in performance.\n# ## Fine-Tuning Hyperparameters:\n# Hyperparameters govern how models operate, and meticulous adjustments can uncover the optimal settings for your specific dataset.\n# ## Embracing Regularization:\n# Regularization techniques, such as L1 and L2 regularization, serve as effective means to combat overfitting, enhancing model generalization.\n# ## Expanding the Dataset:\n# If feasible, enlarging the dataset can provide the models with a richer learning experience, potentially mitigating overfitting.\n# ## Augmenting Feature Set:\n# Consider augmenting your feature set with more relevant variables related to the target variable. This can bolster the models' capacity to capture nuanced relationships between X and Y.\n# ## Addressing Low Training Accuracy:\n# Low training accuracy may signal challenges in model learning. Possible culprits include noisy data, overfitting, or an ill-suited model choice.\n# ## Handling a Small Dataset:\n# Working with a modest dataset of just 142 data points raises the risk of overfitting. Models might excel at learning the training data but falter when encountering new data.\n# ## Deciphering Negative Correlation:\n# A negative correlation hints at a tenuous link between X and Y, posing a formidable hurdle for models aiming to grasp a robust relationship between these variab\n",
        "Points": 85.0,
        "Comments": "The student has done a good job overall. The notebook is well-structured and the code is readable. The student has successfully imported the necessary libraries, loaded the data, and explored it. The train/test split was done correctly and the student has used both Linear Regression and KNN Regression models. However, the student did not use the StandardScaler() for post-split data preprocessing and did not discuss the selection of k value for the KNN model. The student has provided a good discussion of the results and the overall presentation of the notebook is good. The student should work on using post-split data preprocessing and discussing the selection of hyperparameters in future assignments."
    },
    {
        "SID": "5085657",
        "Name": "wayalabhijitvilas",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# ## ASSIGNMENT 4_DATA_MINING\n# ### ABHIJIT VILAS WAYAL - U09616725\n# ### Importing libraries\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nplt.style.use('Solarize_Light2')\nprint(plt.style.available) # this will give you a list of the styles available in your version of matplotlib\n\n# set the random seed\nnp.random.seed(42) # set this to ensure the results are repeatable.\n\n# ### Loading Data\ndf = pd.read_csv(r'C:\\Users\\wayal\\OneDrive\\Desktop\\ABHIJIT\\USF\\DATA MINING\\ASSIG4\\sample.csv', index_col=False)\n\ndf.head(5)\n\ndf.dtypes\n\n# ### Obtaining the shape - Dataframe has 142 rows and 2 columns\ndf.shape \n\n# ### Data Exploration and Preprocessing\n# Get the brief summary about the data including all the key parameters\nprint(df.describe())\n\nprint(df.info())\n\n# Counting the number of missing values for each column.\ndf.isna().sum()\n\n# If the number of missing values in a feature/column is high (relative to the number of observations), then we need to  remove that feature/column from the dataset. Here I have used 50%, that means if a column has less than 50% of the data, it will be dropped\ndf = df.dropna(axis=1, thresh=int(0.50*df.shape[0]))\n\ndf = df.dropna(axis=0, thresh=int(0.25*(df.shape[1]-1))) # axis=0 is row. thresh=int(0.25*(df.shape[1]-1)) means that if a row has less than 25% of the data, it will be dropped\n\ndf.shape\n\n# ### Considering 'x' as th feature column and 'y' as the target variable\nX = df[['x']]\ny = df['y']\n\n#Visualizing the data\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X,y,color='green')\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n\n# ### Using Train Test split\n# Spliting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n# ### Linear Regression Modeling\nlin_reg=LinearRegression().fit(X_train,y_train)\n\ny_predict = lin_reg.predict(X_test)\n\nMSE_lr = mean_squared_error(y_test, y_predict)\nR2_lr = r2_score(y_test, y_predict)\n\nprint(f\"Linear Regression-Mean Squared Error: {MSE_lr:.3f}\")\nprint(f\"Linear Regression-R2: {R2_lr:.3f}\")\n\n# Visualizing the Data\nplt.scatter(X_train, y_train, color='green', label='Actual')\nplt.plot(X_test, y_predict, color='red', label='Linear Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('Linear Regression')\nplt.show()\n\n# ### K-Nearest Neighbors (KNN) Regression Modeling.\n# Initializing and fitting the KNN Regressor model and we have considered number of neighbors (k) as 2\nknn_model = KNeighborsRegressor(n_neighbors=2).fit(X_train, y_train)\n\n# Making predictions on the test set\ny_predict_knn = knn_model.predict(X_test)\n\n# Evaluating the KNN Regressor model\nMSE_knn = mean_squared_error(y_test, y_predict_knn)\nR2_knn = r2_score(y_test, y_predict_knn)\n\nprint(f\"KNN-Mean Squared Error: {MSE_knn:.3f}\")\nprint(f\"KNN-R2: {R2_knn:.3f}\")\n\n# Visualizing the predictions\nplt.scatter(X_train, y_train, color='green', label='Actual')\nplt.scatter(X_test, y_predict_knn, color='blue', label='KNN')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('K-Nearest Neighbors Regression')\nplt.show()\n\n# #### Printing results of Linear and KNN Regression Models\nprint(\"Results:\")\nprint(f\"Linear Regression-Mean Squared Error: {MSE_lr:.3f}\")\nprint(f\"Linear Regression-R2: {R2_lr:.3f}\")\nprint(f\"K-Nearest Neighbors- Mean Squared Error: {MSE_knn:.3f}\")\nprint(f\"K-Nearest Neighbors-R-squared: {R2_knn:.3f}\")\n\n# ### Discussion\n# After loading data we did data explorarion and preprocessing to get the summary of the data and check whether is there any missing values in columns. Futher we applied Train Test split method to given data. After that we have applied Linear and K-Nearest Neighbours regression model for the given data set.\n# We obtained results from both the models.\n# Mean Squared Error(MSE):\n# The MSE of linear regression model is 624.176 and MSE of K-Nearest Neighbours is 917.206. As lower MSE means better accuarcy, so the kNN model prediction is less accurate as compared to linear regression model.\n# R-squared(R2) value:\n# The R2 value of linear regression model and KNN regression model are 0.023 and -0.435 repectively. The R-Squared value of the model determines how well the regression line matches the data. The R-Squared value of both the models are very less which means that this models might not be the best fit for the data.\n\n\n",
        "Points": 95.0,
        "Comments": "Great job on the assignment! You have successfully completed all the tasks. You imported the necessary libraries, loaded and explored the data, handled missing values, split the data into training and testing sets, and applied both Linear Regression and K-Nearest Neighbors models. You also evaluated the models using Mean Squared Error and R-squared metrics and provided a good discussion of the results. However, you did not mention why you chose 2 as the number of neighbors for the KNN model. It would be beneficial to explain your choice. Keep up the good work!"
    },
    {
        "SID": "5155133",
        "Name": "yadavallisravani",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "# Fitting the linear regression\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nfrom matplotlib import pyplot as plt\n\n##Step 2: Load the given data that we will model\ndf = pd.read_csv(\"C:/Users/Ramya/Downloads/sample.csv\", index_col=False)\ndf.head(3)\n\n# Storing the values from the data frame\nX = df[[\"x\"]]\ny = df[[\"y\"]]\n\n# determine the number of rows and column\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n# There are no missing values\n\n# To Check if there are any topos.since there is no categorical values in the given data set they are no topos\n\ndf.head()\n\ndf.describe()\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X, y, color=\"red\")\n\nax.set_xlabel(\"input\")\nax.set_ylabel(\"target\")\nplt.tight_layout()\nplt.show()\n\n# Partitioning data into training and validation sets\n# random_state is set to a defined value to get the same partitions when re-running the code\ntrain_data = df.sample(frac=0.7, random_state=1)\n# assign rows that are not already in the training set, into validation\nvalid_data = df.drop(train_data.index)\n\nprint(\"Training   : \", train_data.shape)\nprint(\"Validation : \", valid_data.shape)\nprint()\n\nscaler = StandardScaler()\ntrain_data_scaled = pd.DataFrame(\n    scaler.fit_transform(train_data), index=train_data.index, columns=train_data.columns\n)\n\ntrain_data_scaled.head()\n\nvalid_data_scaled = pd.DataFrame(\n    scaler.fit_transform(valid_data), index=valid_data.index, columns=valid_data.columns\n)\n\nvalid_data_scaled.head()\n\nX_train = train_data_scaled[[\"x\"]].copy()\ny_train = train_data_scaled[[\"y\"]].copy()\n\nX_valid = valid_data_scaled[[\"x\"]].copy()\ny_valid = valid_data_scaled[[\"y\"]].copy()\n\n# fit the model\nlin_model = LinearRegression()\nlin_model.fit(X_train, y_train)\n\ny_valid[[\"predicted\"]] = lin_model.predict(X_valid)\ny_valid.head()\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_train, y_train, color=\"blue\")\nax.scatter(X_train, lin_model.predict(X_train), color=\"red\")\n\nax.set_xlabel(\"input\")\nax.set_ylabel(\"target\")\nplt.tight_layout()\nplt.show()\n\nnp.random.seed(42)\nmse = mean_squared_error(y_valid[[\"y\"]], y_valid[[\"predicted\"]])\nrmse = np.sqrt(mse)\n# Calculate R-squared (R2)\nr2 = r2_score(y_valid[[\"y\"]], y_valid[[\"predicted\"]])\n\nprint(mse)\nprint(rmse)\nprint(r2)\n\nk = 9\n\nknn_model = KNeighborsRegressor(n_neighbors=k)\n\nknn_regression = knn_model.fit(X_train, y_train)\ny_valid[[\"predicted\"]] = knn_regression.predict(X_valid)\ny_valid.head()\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(X_train, y_train, color=\"blue\")\nax.scatter(X_train, knn_regression.predict(X_train), color=\"red\")\n\nax.set_xlabel(\"input\")\nax.set_ylabel(\"target\")\nplt.tight_layout()\nplt.show()\n\nmse = mean_squared_error(y_valid[[\"y\"]], y_valid[[\"predicted\"]])\nrmse = np.sqrt(mse)\nr2 = r2_score(y_valid[[\"y\"]], y_valid[[\"predicted\"]])\n\nprint(mse)\nprint(rmse)\nprint(r2)\n\n# The mean square error and root mean square error are better values for linear regression.For R2, a higher value closer to 1 indicates a better fit of the model to the data.Here from the r2 the values for linear regression is the better fit to the model.\n",
        "Points": 95.0,
        "Comments": "Great job on completing the assignment. You have successfully imported the necessary libraries, loaded and explored the data, preprocessed the data, and fitted the models. You also evaluated the models using RMSE and R2, and discussed the results. However, you did not provide a title and a brief introduction to your notebook, which is important for context. Also, you imported the LogisticRegression library but did not use it. Please ensure to only import the libraries you need. Keep up the good work!"
    },
    {
        "SID": "5007435",
        "Name": "yalamarthikusuma",
        "Question": "Fit a linear regression and KNN regressor to the data given to you. Your notebook must include any necessary preprocessing and data exploration. Include a markdown to discuss the rationale for preprocessing or data exploration steps. Add a section at the end of the notebook that discusses the results of your analysis and discuss the models and their performance on the data. \n",
        "Processed File": "#Import the libraries we will use in this notebook\n\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nnp.random.seed(42)\n\ndf = pd.read_csv(r'C:\\Users\\Dell\\Desktop\\Datamining\\sample.csv', index_col=False)\n\n#Display the first 30 rows of the sample dataframe\ndf.head(30)\n\n# ### Determine the number of rows and columns\ndf.shape\n\n# ### Data Exploration\n# check to see if there is any missing values, and if so, then fix this issue\ndf.isna().sum()\n\n# Column Removal Based on Missing Data Threshold\n#axis=1 is column. thresh=int(0.60*df.shape[0]) means that if a column has less than 60% of the data, it will be dropped\n\ndf = df.dropna(axis=1, thresh=int(0.60*df.shape[0])) \n\n# If the number of missing values in a observation (row) is high\n# axis=0 is row. thresh=int(0.25*(df.shape[1]-1)) means that if a row has less than 25% of the data, it will be dropped\n\ndf = df.dropna(axis=0, thresh=int(0.25*(df.shape[1]-1))) \n\n# checking columns and rows which are left\ndf.shape \n\n# checking how many missing values are left in each column\ndf.isna().sum() \n\nprint(df.head())\n\nprint(df.describe())\n\nfig=plt.figure()\nf=fig.add_subplot()\nf.scatter(X,y,color='Green')\nf.set_xlabel('X')\nf.set_ylabel('y')\nplt.show()\n\n# ### Processing the Data\n# Separate the features (X) and target variable (y)\n# Our dataset is divided into two primary parts at first, the feature(s) and the target variable. Assuming that 'x' is our feature column and 'y' is our target variable, we may generate these two separate dataframes:\n# Here 'x' is the feature column\nX = df[['x']]\n# Here 'y' is the target variable\ny = df['y']    \n\n# Spliting the data into training and testing sets\n# Our dataset is split into training and testing sets \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\n# Initialize and fit the Linear Regression model\n# Using the training set of data, we proceed to build and train a linear regression model. An easy-to-use approach for simulating the connection between characteristics and a continuous target variable is linear regression. Here, we set up the model and fit it to our training set of data.\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# ### Linear regression modeling\n# Make predictions on the test set\ny_predi_lr = lr_model.predict(X_test)\n\n# Evaluate the Linear Regression model\nmse_liner_reg = mean_squared_error(y_test, y_predi_lr)\nr2_liner_reg = r2_score(y_test, y_predi_lr)\n\nprint(f\"Linear_Reg - Mean Squared Error: {mse_liner_reg:.2f}\")\nprint(f\"Linear_Reg - R-square value: {r2_liner_reg:.2f}\")\n\n# ### Data Visualization\n# Visualize the regression line\nplt.scatter(X_train, y_train, color='b', label='Actual')\nplt.plot(X_test, y_predi_lr, color='r', label='Linear Regression Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('Linear Regression')\nplt.show()\n\n# ### K-Nearest Neighbors (KNN) regression modeling\n# Initialize and fit the KNN Regressor model\nknn_model = KNeighborsRegressor(n_neighbors=2)  # Here we can adjust the number of neighbors i.e k as per needed\nknn_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_predi_knn = knn_model.predict(X_test)\n\n# Evaluate the KNN Regressor model\nmse_knn = mean_squared_error(y_test, y_predi_knn)\nr2_knn = r2_score(y_test, y_predi_knn)\n\nprint(f\"KNN - Mean Squared Error: {mse_knn:.2f}\")\nprint(f\"KNN - R-squared: {r2_knn:.2f}\")\n\n# ### Data Visualization\n# Visualize the predictions\nplt.scatter(X_train, y_train, color='b', label='Actual')\nplt.scatter(X_test, y_predi_knn, color='y', label='KNN Regression Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('K-Nearest Neighbors (KNN) Regression')\nplt.show()\n\n# ### Evaluation and comparison of both models\n# Discussing the results and comparing the models' performance\nprint(\"Summary of Results:\")\nprint(f\"Linear Reg- Mean Squared Error: {mse_liner_reg:.2f}\")\nprint(f\"Linear Reg- R-squared: {r2_liner_reg:.2f}\")\nprint(f\"K-Nearest Neighbors (KNN) - Mean Squared Error: {mse_knn:.2f}\")\nprint(f\"K-Nearest Neighbors (KNN) - R-squared: {r2_knn:.2f}\")\n\n# ### Comments:\n# Initially, I have imported the libraries which is required for data modelling, data manipulation, and data visualisation.\n# Next, It reads the dataset from a sample.csv file and handles missing values, does data preparation, data exploration, and data visualization.\n# Later, the data is divided into training and testing sets and creates Regression such as Linear Regression and KNN Regression.\n# The results for Linear Regression is:\n# Mean Squared Error = 624.18\n# R-square value =  0.02\n# The results for KNN Regression is:\n# Mean Squared Error = 917.21\n# R-squared =  -0.44\n# This shows that the KNN model performs even worse than the linear regression model. It might not be suited for this dataset since it has a negative connection to the target variable.\n# From the both results linear regression and K-nearest neighbors (KNN) regression do not successfully reveal the deeper trends in the data. To improve anticipated performance, it is advisable to examine various machine learning techniques or engage in feature design. Furthermore, because of different datasets may require different modelling strategies in order to produce relevant insights and predictions, it is crucial to take the context of the problem into account when creating and assessing models.\n# .\n",
        "Points": 95.0,
        "Comments": "The student has done a good job on this assignment. The notebook structure is well organized, all necessary libraries are imported, and the data is loaded and explored properly. The student has also done a good job in preprocessing the data and splitting it into training and testing sets. Both Linear Regression and KNN Regression models are fitted and evaluated correctly. The student has also provided a good discussion of the results. However, the student has imported some libraries that are not used in the notebook, such as LogisticRegression and SimpleImputer. Also, the student has not used StandardScaler() for post-split data preprocessing. Overall, the student has demonstrated a good understanding of the concepts and applied them correctly in this assignment."
    }
]