{
    "SID": [
        "5007507",
        "160259344",
        "160253344",
        "160254036",
        "160270318",
        "160263268",
        "4882785",
        "160189646",
        "160276660",
        "160274116",
        "160266292",
        "160272512",
        "160261604",
        "160271550",
        "160271062",
        "160270806",
        "160196434",
        "160273704",
        "160271780",
        "160067740",
        "160244266",
        "160259148",
        "4976197",
        "160258444",
        "160247166",
        "160181742",
        "5084803",
        "160261394",
        "159987052",
        "5187493",
        "160278322",
        "160274328",
        "5120719",
        "160275402",
        "160275192",
        "160240660",
        "160265552",
        "160263772",
        "160251008",
        "160269432",
        "5120275",
        "160196312",
        "160273100",
        "160266936"
    ],
    "S_NAME": [
        "addalapoornachand",
        "ageerharikrishna 5113587",
        "alavamsi 5009045",
        "asireddyvivekanandareddy 4983693",
        "bakerjennifer 4030725",
        "banakarrachananagaraj 5096845",
        "chitralarakeshwaranjaneyulu",
        "dadisettymadhukiran 5141685",
        "doddasasankreddy 5079175",
        "gangupamusaisunil 5090461",
        "gillnikita 5144161",
        "gundlurusiddarthareddy 5072991",
        "kachamnikhil 5074977",
        "kaluvapallisuhit 5082779",
        "karanamsandeep 5151247",
        "kasturinagagayathiridevi 4996333",
        "kharmalejanhavianantprakash 5069725",
        "kreitzerdevan 3911920",
        "kunapareddychaitanyasai 4974601",
        "maggiopatrick 4279109",
        "mamidiprudhviteja 5128349",
        "mandadirevanth 5128983",
        "mekalamallemkondaiahvenkatadheeraj",
        "minukaprajayreddy 5082739",
        "namanagreeshma 5109183",
        "nandamuruyashwanthbharadwaj 5007273",
        "nimmasandeepreddy",
        "pabbapavankumargoud 5139627",
        "patilsafrin 4992571",
        "peddireddylavanya",
        "purumrupesh 5097469",
        "reddipogusravanth 5105857",
        "sangoivineshnarendra",
        "satputeneha 5149461",
        "sunkarashwetha 5091285",
        "tanuboddivenkatasaisatvikreddy 5080075",
        "thotasrija 5101315",
        "tirumalasettyvarunkumar 5099545",
        "upparajayanth 5009099",
        "vadlalarohit 4992045",
        "vermasaurabhjagdishprasad",
        "wayalabhijitvilas 5085657",
        "yadavallisravani 5155133",
        "yalamarthikusuma 5007435"
    ],
    "RAW_FILE": [
        "D:\\AutoGrader\\src\\autograder\\submissions\\addalapoornachand_5007507_160264296_assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\ageerharikrishna_5113587_160259344_assignment3_HARIKRISHNA.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\alavamsi_5009045_160253344_VamsikrishnaAla_U91394366 ass3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\asireddyvivekanandareddy_4983693_160254036_assignment3_starting_template.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\bakerjennifer_4030725_160270318_Jennifer_Baker_U05341559.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\banakarrachananagaraj_5096845_160263268_Assgnt3_U84731572_Rachana_N_B.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\chitralarakeshwaranjaneyulu_4882785_160196178_Assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\dadisettymadhukiran_5141685_160189646_MadhuKiranDadisetty_Assignment3_U53552106.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\doddasasankreddy_5079175_160276660_sasankreddy_dodda.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\gangupamusaisunil_5090461_160274116_Sunil_assignment3_U63410293.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\gillnikita_5144161_160266292_assignment3_NikitaGill_U49022008.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\gundlurusiddarthareddy_5072991_160272512_DataMining_Assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\kachamnikhil_5074977_160261604_Nikhil_Kacham_starting_template.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\kaluvapallisuhit_5082779_160271550_Suhit_Kaluvapalli_A3_U14988780.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\karanamsandeep_5151247_160271062_assignment3_Sandeep_Karanam_U94742075.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\kasturinagagayathiridevi_4996333_160270806_Gayathri_Assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\kharmalejanhavianantprakash_5069725_160196434_assignment3_starting_template.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\kreitzerdevan_3911920_160273704_DevanKreitzer_assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\kunapareddychaitanyasai_4974601_160271780_assignment3_starting_template1 (2).ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\maggiopatrick_4279109_160067740_A3_Patrick.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\mamidiprudhviteja_5128349_160244266_assignment3_anwers.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\mandadirevanth_5128983_160259148_RevanthMandadi_U16635207_assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\mekalamallemkondaiahvenkatadheeraj_4976197_160268238_assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\minukaprajayreddy_5082739_160258444_assignment3_starting_template_Prajay.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\namanagreeshma_5109183_160247166_GreeshmaNamana_U24918180_Assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\nandamuruyashwanthbharadwaj_5007273_160181742_Assignment3_Yashwanth_Bharadwaj_Nandamuru_U17339996.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\nimmasandeepreddy_5084803_160261560_Assignment 3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\pabbapavankumargoud_5139627_160261394_Pavan_U03741351_Assignment03.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\patilsafrin_4992571_159987052_assignment3_SafrinPatil-U88384854.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\peddireddylavanya_5187493_160247684_Lavanya Peddireddy Assignment 3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\purumrupesh_5097469_160278322_assignment3_complete.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\reddipogusravanth_5105857_160274328_Sravanth_U87860185_assignment_3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\sangoivineshnarendra_5120719_160030074_assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\satputeneha_5149461_160275402_neha_satpute_dataminning_3_U19160274.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\sunkarashwetha_5091285_160275192_Shwetha_Sunkara_Assignment_3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\tanuboddivenkatasaisatvikreddy_5080075_160240660_assignment3_starting_template.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\thotasrija_5101315_160265552_SrijaThota_U60390046_AS3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\tirumalasettyvarunkumar_5099545_160263772_assignment3_VARUN_KUMAR_TIRUMALASETTY_U35246156.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\upparajayanth_5009099_160251008_DM_Assignment3.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\vadlalarohit_4992045_160269432_Rohit_Vadlala_Assignment3_Datamining.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\vermasaurabhjagdishprasad_5120275_160271210_Saurabh Jagdish Prasad Verma U09956219 Logistic Regression.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\wayalabhijitvilas_5085657_160196312_ASSIG3_ABHIJIT_WAYAL_U09616725.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\yadavallisravani_5155133_160273100_Assignment3_Sravani_Yadavalli.ipynb",
        "D:\\AutoGrader\\src\\autograder\\submissions\\yalamarthikusuma_5007435_160266936_kusuma_yalamarthi_Assignment3.ipynb"
    ],
    "PROCESSED_FILE": [
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'/Users/poorna/Downloads/riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any typos, and if so, then fix this issue.\ndf['Ownership'].unique()\ndf = df.replace(['owners'], 'Owner' ) \ndf = df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'].unique()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \ndf = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf\n\n\ncol_names_2b_transformed = df.columns.drop('Ownership_Owner')\ncol_names_2b_transformed\ndf_scaled = df[col_names_2b_transformed]\n\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_scaled),\n                       index=df_scaled.index, columns=df_scaled.columns)\ndf_scaled['Ownership_Owner'] = df['Ownership_Owner']\ndf = df_scaled\ndf\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nX = df[[\"Income\", \"Lot_Size\"]] \ny = df[\"Ownership_Owner\"] \n\nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\n# model = LogisticRegression()\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\nprint(accuracy_score(y, model.predict(X)))\nprint(precision_score(y, model.predict(X)))\nprint(recall_score(y, model.predict(X)))\nprint(f1_score(y, model.predict(X)))\n\n\n# ## Step 6: Discuss the Results\n# ...insert a discussion of the results from your model\n# ## The Accuracy of 83.33% suggests that approximately 83.33% of the instances in the dataset are predicted accurately.\n# ## The Precision of 83.33% suggests that approximately 83.33% of the instances in the dataset predicted as positive by the model were true positives.\n# ## The Recall of 83.33% suggests that model identified 83.33% of all the actual positives in the dataset.\n# ## The F1 score is the harmonic mean of precision and recall, providing a balance between precision and recall. It's also approximately 83.33%.\n# ## The consistent values of 83.33% across all these metrics suggest that the model is performing consistently well on data. However, evaluating the model on the same data it was trained on can lead to optimistic performance estimates. \n",
        "# ### HARI KRISHNA AGEER\n# ### UID:-U18952413\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib inline\n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(\"riding_mowers.csv\")\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=100,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\n# display the confusion matrix\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n# ## Step 6: Discuss the Results\n# The observed accuracy of 83 percent is expected when evaluating the model on the same training data, as it measures how well the model fits the data it has already seen during training. Both precision and recall also exhibit high values because there is no separation of data for training and testing.The model can predict training data accurately, leading to consistent metrics. And the F1 score depends on precision and recall .The need for an independent test dataset to be perfect model for predectionn and with more data observations may predict with better accuracy.\n",
        "# # Data Mining Assignment 3\n# Vamsikrishna_Ala_U91394366\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib inline \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('C:/Users/Vamsi/Documents/Datamining Assignment 3/riding_mowers.csv')\n\ndf.head(20)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\n\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n\n\n\n# ## Step 6: Discuss the Results\n# Accuracy, Precision, Recall, and F1 Score are a few metrics that are frequently used to assess a model's performance. Due to the similar rates of true positives, true negatives, false positives, and false negatives, we were able to conclude from our analysis that these metrics produced results that were comparable. Precision evaluates the accuracy of predictions for the \"Owner\" class while Accuracy provides a more general gauge of correctness. By striking a balance between Precision and Recall, the F1 Score quantifies the model's capacity to correctly identify every instance of the \"Owner\" class. These standards are essential for assessing how accurately the model categorizes data because false positives and false negatives affect the evaluation in different ways.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom sklearn.linear_model import *\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n\n\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nrows = df.shape[0]\ncols = df.shape[1]\nprint('Rows:'+str(rows)+'\\nCols:'+str(cols))\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum() # There are no missing values\n\n# check to see if there are any typos\ndf['Ownership'].unique()\n\n#Dummy Encoding.\n#Determine or check classes of variable\ndf['Ownership'] = df['Ownership'].replace({'nonowner': 'Nonowner', 'owners': 'Owner'})\nprint(df['Ownership'].unique())\n\n#Encode the variable.\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nP = df[['Income', 'Lot_Size']]  \nr = df['Ownership']\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \npredictive_model = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\npredictive_model.fit(P, r)\n\nresults = pd.DataFrame()\nresults['actual_values'] = r\nresults['predicted_values'] = predictive_model.predict(P)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual_values'], results['predicted_values'])\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual_values'], results['predicted_values'], display_labels=predictive_model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Reds\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n\n# - Accuracy\nprint(accuracy_score(r, predictive_model.predict(P)))\n# - Precision\nprint(precision_score(r, predictive_model.predict(P), pos_label='Owner'))\n# - Recall\nprint(recall_score(r, predictive_model.predict(P), pos_label='Owner'))\n# - F1 Score\nf1_score(r, predictive_model.predict(P), pos_label='Owner')\n# - Error Rate\nprint(1-accuracy_score(r,predictive_model.predict(P)))\n\n# ## Step 6: Discuss the Results\n# - We have 24 rows in the dataset. \n# - Out of 24, 10 are true negatives: model predicted them to be negative and they are negative in reality also.\n# - 10 are true positives: model predicted them to be positive and they are indeed positive in reality also.\n# - 2 are false negatives: model predicted them to be negatives but they are positive in reality.\n# - 2 are false positives: model predicted them to be positives but they are negative in reality.\n# - The accuracy rate of our classifier model is 83.3% and error rate is 16.6%\n# - Since we got accuracy, precision, recall and f1 score the same. This shows our model is balanced and has equal ability to estimate true positives, true negatives, false positives and false negatives.\n# - We could prefer any of the metrics in our case as all our scores are high and identical.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136 \n# Jennifer Baker U05341559\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('https://raw.githubusercontent.com/prof-tcsmith/data/master/RidingMowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows = {rows} and Cols = {cols}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n#alternatively\n#df.isnull().sum()\n\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf[\"Ownership\"].unique()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\ndf_cleaned = pd.get_dummies(df, prefix_sep='_', dummy_na=False, drop_first=False, columns=['Ownership'])\n\ndf_cleaned.head()\n\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LinearRegression()\nX_dataset = df_cleaned[['Lot_Size','Ownership_Nonowner','Ownership_Owner']].copy()\ny_dataset = df[['Income']].copy()\nmodel.fit(X_dataset, y_dataset)\ny_dataset['predicted'] = model.predict(X_dataset)\n\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n\nN = y_dataset.shape[0]\npredictions = y_dataset['predicted'] \ntrue_values = y_dataset['Income']\nTP =  ((predictions == True) & (true_values == True)).count()\nFP = ((predictions == True) & (true_values == False)).count()\nTN = ((predictions == False) & (true_values == False)).count()\nFN = ((predictions == False) & (true_values == True)).count()\n\n\n# - Accuracy\naccuracy = (predictions == true_values).count() / N \nprint(\"Accuracy is \" +str(accuracy))\n# - Precision\nprecision = TP / (TP+FP)\nprint(\"Precision is \" +str(precision))\n# - Recall\nrecall = TP / (TP + FN)\nprint(\"Recall is \" +str(recall))\n# - F1 Score\nF1 = (2*precision*recall) / (precision + recall)\nprint(\"F1 Score is \" + str(F1))\n\n\n# ## Step 6: Discuss the Results\n# According to the Confusion Matrix statistics, the model is very accurate, but only half time time has correct precision/recall and the harmonic mean is 0.5 which means that the linear model would not be the best fit case.  It would be best to try other models than base linear regression to get better predictions. \n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n%matplotlib inline\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('riding_mowers (1).csv')\n\ndf.head(11)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nn = df.shape[0]\nb = df.shape[1]\nprint(f\"Number of rows={n} and Number of Columns={b}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='purple')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='red')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income in Dollars ($000s)')\nax.set_ylabel('Lot Size in Sq ft (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nr = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\np = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(r, p)\n\nresults = pd.DataFrame()\nresults['existing_real'] = p\nresults['forcasted'] = model.predict(r)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['existing_real'], results['forcasted'])\n\nfig, ax = plt.subplots(figsize=(9, 3))\n\nConfusionMatrixDisplay.from_predictions(\n    results['existing_real'], results['forcasted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Greens\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(p, model.predict(r)))\n# - Precision\nprint(precision_score(p, model.predict(r), pos_label='Owner'))\n# - Recall\nprint(recall_score(p, model.predict(r), pos_label='Owner'))\n# - F1 Score\nf1_score(p, model.predict(r), pos_label='Owner')\n\n\n\n# ## Step 6: Discuss the Results\n# The result shows the basic steps in data preparation, including evaluating data dimensions, dealing with missing values, normalizing categorical data, and using a scatter plot to see the structure and patterns of the dataset.\n# The provided material also describes how to build a logistic regression classifier using the available dataset. Using 'Owner' and 'Nonowner' as the class labels, the performance of the classifier is evaluated using a confusion matrix. This matrix is a crucial tool for determining how accurately the model predicts categories that fall under the topic \"Ownership.\"\n# Considerably, the model shows superior scores for accuracy, precision, recall, and F1 score when \"Owner\" is the positive label. The model appears to be particularly good at predicting instances that fall under the \"Owner\" class, according to these metrics taken together.\n",
        "# # Assignment 3\n# # RAKESHWARANJANEYULU CHITRALA - U96703037\n# ## Step 1: Import libraries what we use \n# import neccessary libraries\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib inline \n\n# ## Step 2: Load the Data\ndf = pd.read_csv('C:/Users/Dell/Downloads/riding_mowers.csv')\n\ndf.head(20)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='pink')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='violet')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\n\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyse the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n\n\n\n# ## Step 6: SUMMARY\n# A scatter plot is made to show the relationship between Income and Lot_Size depending on the Ownership type after importing the necessary libraries, changing the necessary path, and pre-processing the data. After the dataframe is checked for missing values, it is seen that there are none. Then, in order to present the confusion matrix, we developed a classifier. The accuracy, precision, recall, and F-1 Score are used to measure the performance of linear regression models. The model has the same score (about 0.8333) for all metrics, showing consistent performance.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nimport matplotlib.pylab as plt\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable.\n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('/Users/Administrator/Downloads/riding_mowers.csv')\n\ndf.head(8)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\n# check to see if there are any typos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\n# Correcting the typos errors\n\ndf = df.replace(['owners'], 'Owner' ) \ndf = df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'].unique()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nX = df[['Income', 'Lot_Size']]  \ny = df['Ownership'] \n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(X)\n\nclassifier = RandomForestClassifier(random_state=42)\n\nclassifier.fit(X_scaled, y)\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\n\n\n# display the confusion matrix\n\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# - Accuracy\naccuracy = (accuracy_score(y, model.predict(X)))\n# - Precision\nprecision = (precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nrecall = (recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1 = (f1_score(y, model.predict(X), pos_label='Owner'))\n\nprint(f'Accuracy: {accuracy:.3f}')\nprint(f'Precision: {precision:.3f}')\nprint(f'Recall: {recall:.3f}')\nprint(f'F1 Score: {f1:.3f}')\n\n# ## Step 6: Discuss the Results\n# 1) Recall, accuracy, precision, and F1 score all have the same value of 0.83, which is significant. This implies that the performance of the model is constant across different criteria, showing a well-balanced and trustworthy model.\n# 2) The model has not produced any false positives or false negatives, only right predictions.\n# 3) The model's performance has been better understood thanks to the confusion matrix, particularly in terms of false positives and false negatives. \n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n\n%matplotlib inline\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nimport matplotlib.pylab as plt\n# setting up the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('C:/Users/sasan/Downloads/riding_mowers.csv')\n\ndf.head(12)\n\n# ## Step 3: Prepare the Data\n# lets find number of rows and columns respectively\nrows = df.shape[0]\ncolumns = df.shape[1]\nprint(rows,columns)\n\n# fixing null values and replace them with sum of other values\n\ndf.isna().sum()\n\n# checking the data frame to see if there are any unique values.\ndf['Ownership'].unique()\n\n# replacing nonowner with Nonowner and owners with owner hence before we got 4 values now after replacing we get only two\ndf['Ownership'] = df['Ownership'].replace({'nonowner': 'Nonowner', 'owners': 'Owner'})\nprint(df['Ownership'].unique())\n\ndummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualizing data with scatter plot\n \nplot = plt.figure()\nsc = plot.add_subplot()\nsc.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nsc.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nsc.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nsc.set_xlim(10, 140)\nsc.set_ylim(10, 30)\nsc.set_xlabel('Income ($s)')\nsc.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since there are few observations, all of the data will be allocated to training. Typically, the data would be divided into training and testing sets.\n# standardizing the input data \n\nf = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\nt = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(f, t)\n\nresults = pd.DataFrame()\nresults['actual'] = t\nresults['predicted'] = model.predict(f)\nresults\n\n# displaying the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n#lets use some important metrics to analyze the model performance\n#  Accuracy\nprint(accuracy_score(t, model.predict(f)))\n# Precision\nprint(precision_score(t, model.predict(f), pos_label='Owner'))\n# Recall\nprint(recall_score(t, model.predict(f), pos_label='Owner'))\n# F1 Score\nf1_score(t, model.predict(f), pos_label='Owner')\n\n\n\n# ## Step 6: Discuss the Results\n#  model's performance is quite consistent across multiple evaluation metrics, with each metric, including accuracy, precision, recall, and F1 Score, showing a value of approximately 0.8333. This means that the model correctly predicts outcomes, particularly when it classifies something as 'Owner,' about 83.33% of the time. It also effectively captures around 83.33% of the actual 'Owner' cases. In simple terms,  model is doing a reasonably good job in its predictions, but it's essential to keep in mind the specific context of  problem and the level of performance required for the application. Additionally, considering factors like class imbalances in  data when interpreting these metrics.\n\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable.\n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('/Users/gangupamusaisunil/Downloads/riding_mowers.csv')\n\ndf.head(15)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\n\nnum_rows, num_columns = df.shape\n\nprint(f\"Number of rows: {num_rows}\")\nprint(f\"Number of columns: {num_columns}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\nmissing_values = df.isnull().sum()\n\nprint(\"Missing Values:\")\nprint(missing_values)\n\n# check to see if there are any typos, and if so, then fix this issue.\n\n\n\ncolumns_to_check = ['Ownership']\nfor column in columns_to_check:\n    unique_values = df[column].unique()\n    print(f\"Unique values in '{column}':\")\n    print(unique_values)\n\n\n# Correcting the typos errors\ndf.loc[df['Ownership'] == 'owners', 'Ownership'] = 'Owner'\ndf.loc[df['Ownership'] == 'nonowner', 'Ownership'] = 'Nonowner'\n\ncolumns_to_check = ['Ownership']\nfor column in columns_to_check:\n    unique_values = df[column].unique()\n    print(f\"Unique values in '{column}':\")\n    print(unique_values)\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nX = df[['Income', 'Lot_Size']]  \ny = df['Ownership'] \n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(X)\n\nclassifier = RandomForestClassifier(random_state=42)\n\nclassifier.fit(X_scaled, y)\n\n\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# Displaying the confusion matrix\n\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\naccuracy = (accuracy_score(y, model.predict(X)))\n# - Precision\nprecision = (precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nrecall = (recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1 = (f1_score(y, model.predict(X), pos_label='Owner'))\n\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n\n\n\n# ## Step 6: Discuss the Results\n# ...insert a discussion of the results from your model\n#Accuracy: The model was 83% accurate, meaning that it accurately predicted ownership status for the vast majority of occurrences.\n\n#Precision: The precision is 83%, indicating that the model's positive predictions are credible.\n\n#The recall is 83%, indicating that the model efficiently captures positive cases.\n\n#F1 Score: The F1 score is 83%, effectively balancing precision and recall.\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\n#shows that all columns have values, no nulls hence not dropping anything here and I am proceeding with this dataset.\ndf.isnull()\n\n# check to see if there are any typos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#fixed above issue by renaming owners & nonowner to Owner & Nonowner respectively.\ndf['Ownership'] = df['Ownership'].replace('owners', 'Owner')\ndf['Ownership'] = df['Ownership'].replace('nonowner', 'Nonowner')\n#updated dataframe is shown below\ndf['Ownership'].unique()\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data\nfeatures = df.drop(columns=['Ownership'])\ntarget = df['Ownership']\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\nmodel.fit(features, target)\n\n# ## Step 5: Analyze the linear regression model's performance\nresults = pd.DataFrame()\nresults['actual'] = target\nresults['predicted'] = model.predict(features)\nresults\n\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nprint('Accuracy', accuracy_score(results['actual'], results['predicted']))\nprint()\npos_label = 'Owner'\nprint('For Owners: ')\nprint('Precision',precision_score(results['actual'], results['predicted'],pos_label=pos_label))\nprint('Recall Score',recall_score(results['actual'], results['predicted'], pos_label=pos_label))\nprint('F1 SCore',f1_score(results['actual'], results['predicted'], pos_label=pos_label))\nprint()\npos_label = 'Nonowner'\nprint('For Non Owners: ')\nprint('Precision',precision_score(results['actual'], results['predicted'],pos_label=pos_label))\nprint('Recall Score',recall_score(results['actual'], results['predicted'], pos_label=pos_label))\nprint('F1 SCore',f1_score(results['actual'], results['predicted'], pos_label=pos_label))\n\n\n\n# ## Step 6: Discuss the Results\n# Given that the dataset contains information about the owner's characteristics (income and lot size) and their ownership status (owner or non-owner), we applied the logistics regression to predict whether someone is an owner or a non-owner based on their income and lot size.\n# Recall (Owner Class): A high recall (0.833) for the 'Owner' class means that the model correctly identifies about 83.3% of actual owners.\n# F1-Score (Owner Class): The F1-Score (0.83) is the balance between precision and recall for the 'Owner' class. It indicates a good trade-off between precision and recall for identifying owners.\n# Precision (Non-Owner Class): A high precision (0.83) for the 'Non-Owner' class means that when the model predicts someone as a non-owner, it is correct about 83.3% of the time.\n# Recall (Non-Owner Class): Recall measures the ability of the model to correctly identify non-owners. A high recall (0.90) for the 'Non-Owner' class means that the model correctly identifies about 90% of actual non-owners.\n# F1-Score (Non-Owner Class): The F1-Score (0.90) is the balance between precision and recall for the 'Non-Owner' class. It indicates a good trade-off between precision and recall for identifying non-owners.\n# Accuracy: The overall accuracy of the model is 0.833, which means that it correctly predicts the ownership status (owner or non-owner) for approximately 83% of the samples in the dataset.\n\n\n",
        "# # Assignment 03\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib inline \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\sidda\\OneDrive\\Desktop\\Data Mining\\riding_mowers.csv')\n\ndf.head(20)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ### Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\n\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n\n\n\n# ## Step 6: Discuss the Results\n# Several metrics, such as Accuracy, Precision, Recall, and F1 Score, are commonly used to evaluate a model's performance. In our study, these metrics yielded similar results because the occurrences of false positives, false negatives, true positives, and true negatives were roughly equal. While Accuracy gives an overall measure of the model's correctness, Precision emphasizes the correctness of positive predictions. The F1 Score provides a balanced measure of a model's capability to correctly identify positive instances by harmonizing Precision and Recall. Evaluating these metrics is crucial since the impact of false positives and false negatives varies in model assessment.\n\n\n",
        "# # Predictive Modeling Example\n# ### Nikhil Kacham\n# ### U99023428\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n\n\n# import neccessary libraries\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 1: Import the libraries we will use in this notebook\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\nikhi\\Downloads\\riding_mowers.csv')\n\ndf.head(24)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nfeature = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ntarget = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1)\n\nmodel.fit(feature, target)\n\nresults = pd.DataFrame()\nresults['actual'] = target\nresults['predicted'] = model.predict(feature)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(target, model.predict(feature)))\n# - Precision\nprint(precision_score(target, model.predict(feature), pos_label='Owner'))\n# - Recall\nprint(recall_score(target, model.predict(feature), pos_label='Owner'))\n# - F1 Score\nf1_score(target, model.predict(feature), pos_label='Owner')\n\n# ## Step 6: Discuss the Results\n# The logistic regression model demonstrated strong performance on the dataset, achieving an accuracy, precision, and recall rate of 83.33%. This implies that it correctly predicted ownership status for a significant portion of the individuals. The F1 score of 0.8333, which balances precision and recall, further confirms the model's effectiveness. However, it's important to note that model performance is contingent on data quality, and bias or incompleteness in the data could lead to biased predictions. Additionally, the relatively small dataset may limit the model's ability to generalize to new data. To enhance its generalizability, collecting a more extensive and diverse dataset would be beneficial.The confusion matrix shows that majority of the data is under the true negatives and true positives which is supported by the accuracy of the model, which is 83% indicating a good prediction model.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib inline \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\suhit\\OneDrive\\Documents\\riding_mowers.csv')\n\ndf.head(8)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\nrs = df.shape[0]\ncs = df.shape[1]\nprint(f\"Rows={rs} and Cols={cs}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\ndf.isnull().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\ndf.replace(['owners'], 'Owner', inplace= True)\ndf.replace(['nonowner'], 'Nonowner', inplace= True)\ndf.head(25)\n\ndf['Income'].unique()\n\ndf['Lot_Size'].unique()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='orange')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='red')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nX = df[['Income','Lot_Size']]\ny = df[['Ownership']]\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX=scaler.fit_transform(X)\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(multi_class='ovr', solver='lbfgs')\ny.head(5)\n\n# trianing model\nmodel.fit(X,y)\n\ny_pred=model.predict(X)\n\ny_pred\n\ndff = pd.DataFrame()\ndff['y_pred'] = y_pred\ndff['y'] = y\n\ndff\n\nmodel.predict_proba(X)\n\n# ## Step 5: Analyze the linear regression model's performance\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,ConfusionMatrixDisplay\n\n\ncm = confusion_matrix(y, y_pred, labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot(cmap=plt.cm.Purples)\nplt.show()\n\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\nprint(confusion_matrix(y_pred,y))\nprint(accuracy_score(y_pred,y))\nprint(classification_report(y_pred,y))\n\n# ## Step 6: Discuss the Results\n## converting target datatype from categorical to numerical\n\ndf = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf\n\n\nXO = df[['Income','Lot_Size']]\nyO = df[['Ownership_Owner']]\n\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nXO=scaler.fit_transform(XO)\n\n# trianing model\nmodel.fit(XO,yO)\n\ny_predO=model.predict(XO)\n\ny_predO\n\ncmO = confusion_matrix(yO, y_predO, labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cmO, display_labels=model.classes_)\ndisp.plot(cmap=plt.cm.Greens)\nplt.show()\n\nprint(confusion_matrix(y_predO,yO))\nprint(accuracy_score(y_predO,yO))\nprint(classification_report(y_predO,yO))\n\n# ## Observation:\n# Based on income and lot size, the first model predicts ownership (either \"Owner\" or \"Nonowner\"), but the second model predicts ownership as a binary classification problem (0 for non-owners and 1 for owners). Both models' model performance measures (confusion matrices and accuracy) are displayed.\n# It's important to note that the second model (binary classification) appears to perform better, as demonstrated by the confused matrix and other metrics. The particular conclusions regarding the model's applicability and performance on the given dataset would be determined by the properties of the dataset as well as the project's goals. Furthermore, to minimize overfitting and for generalizability, the model's performance must be evaluated on a separate test dataset (not simply the training data).\n\n\n",
        "# # Assignment3\n# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# Sandeep Karanam U94742075\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom matplotlib import pyplot as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('C:/Users/sande/Downloads/data/riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n#  Dataframe contains 24 rows and 3 columns\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# No missing values in the data\n# check to see if there are any typos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n# identified couple of typos and replacing them with correct values.\ndf=df.replace(['owners'],'Owner')\ndf=df.replace(['nonowner'],'Nonowner')\n\n# checking if the values are updated.\ndf['Ownership'].unique()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \ndf = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf\n\n# let's create a list of column names, excluding the Ownership_Owner column (which is our dummy encoded column)\ncol_names_2b_transformed=df.columns.drop('Ownership_Owner')\ncol_names_2b_transformed\n\ndf_scaled=df[col_names_2b_transformed]\ndf_scaled.head()\n\n# More common scaling technique is to normalize the data (each variable value is scaled using the variables standard deviation)\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_scaled),\n                       index=df_scaled.index, columns=df_scaled.columns)\n\ndf_scaled['Ownership_Owner']=df['Ownership_Owner']\n\ndf_scaled.head()\n\nX=df_scaled.drop(columns=['Ownership_Owner'])\ny=df_scaled['Ownership_Owner']\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel=LogisticRegression()\nmodel.fit(X,y)\n\ny_predicted=model.predict(X)\n\nresult=pd.DataFrame()\nresult['y_actual']=y\nresult['y_predicted']=y_predicted\nresult\n\n# display the confusion matrix\ncm = confusion_matrix(result['y_actual'], result['y_predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    result['y_actual'], result['y_predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy \nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X)))\n# - Recall\nprint(recall_score(y, model.predict(X)))\n# - F1 Score\nprint(f1_score(y, model.predict(X)))\n\n\n# ## Step 6: Discuss the Results\n# Since we did not have much to do train/test split model we fit the model for all the existing data and got above results for how the model performed. Accuracy, precision, recall, f1 scores are 83.3% that means model predcited with accuracy of 83.3% and precision explains 83.3% of positives instances predicted are true positives. similarly for recall model correctly predicted 83.33% of all properties that are actually owned by an owner. F1 score is 83.33% which indicates good balance between precision and recall. Confusion matrix is also displayed above which shows there are 10 instances each of TP and TN and 2 instances each of FP and FN. which shows that model performed considerably well on the data and should be tested on test data to confirm the results.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# Naga Gayathiri Devi Kasturi\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\gayat\\OneDrive\\Documents\\DataMining_Assignments\\riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n# standardize the input data \nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n\n# ## Step 6: Discuss the Results\n# Since accuracy,precision,recall and f1 scores are same the model has given perfect results.Model made right predictions with no false positives or false negatives.\n\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(\"C:/Users/janha/Downloads/riding_mowers.csv\")\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#df.replace(['owners'],'Owner',regex=True,inplace=True)\ndf=df.replace(['owners'],'Owner')\n#df.replace(['nonowner'],'Nonowner',regex=True,inplace=True)\ndf=df.replace(['nonowner'],'Nonowner')\n\ndf['Ownership'].unique()\n\ndf = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership_Owner==1]['Income'], \n           df.loc[df.Ownership_Owner==1]['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership_Owner==0]['Income'], \n           df.loc[df.Ownership_Owner==0]['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nscaler1 = StandardScaler()\nscaler2 = StandardScaler()\ndf['Income'] = scaler1.fit_transform(df[['Income']])\ndf['Lot_Size'] = scaler1.fit_transform(df[['Lot_Size']])\n                                       \nX = df.drop(columns=['Ownership_Owner'])\ny = df['Ownership_Owner']\n\ndf\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\nmodel.fit(X,y)\n\n\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\n\n\n\n\ncm = confusion_matrix(results['actual'], results['predicted'])\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\nprint(\"Accuracy :\", accuracy_score(y, model.predict(X)))\nprint(\"Precision :\",precision_score(y, model.predict(X)))\nprint(\"Recall :\",recall_score(y, model.predict(X)))\nprint(\"F1 :\",f1_score(y, model.predict(X)))\n\n\n# ## Step 6: Discuss the Results\n# ...insert a discussion of the results from your model\n# Accuracy suggests that model predicts 83% of the predictions correctly. Since the dataset is small, it might be the reason of this good accuracy.\n# Precision tells us that 83.33% of the postive predictions i.e true positives are actually correct. This indicates that when the model predicted a positive outcome, it was correct 83.33% of the time. \n# Recall of 83.33% is suggesting that the model has correctly identified 83.33% of the positive instances\n# F1-score of 83.33% suggests a good balance between precision and recall. It provides a single metric that balances both precision and recall.\n",
        "# # Assignment 3: Predictive Modeling Example\n# ## Devan Kreitzer\n# ## 09/27/2023\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\nimport matplotlib.pyplot as plt\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('data/riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#fixing typos\ndf['Ownership'] = df['Ownership'].str.lower()\ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner', 'owner': 'Owner'})\n\ndf['Ownership'].unique()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf_dummy.head(10)\n\ndf_dummy['Ownership_Owner'].unique()\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nfeatures = df_dummy.drop(columns=['Ownership_Owner'])\ntarget = df_dummy['Ownership_Owner']\n\nX_train = features\ny_train = target\n\nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\nmodel.fit(X_train, y_train)\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y_train, model.predict(X_train)))\n\n# - Precision\nprint(precision_score(y_train, model.predict(X_train)))\n\n# - Recall\nprint(recall_score(y_train, model.predict(X_train)))\n\n# - F1 Score\nprint(f1_score(y_train, model.predict(X_train)))\n\n# ## Step 6: Discuss the Results\n# My model shows an accuracy, precision, recall, and F1 score of 83.33% on the training data, meaning I'm getting predictions right 83.33% of the time. The same numbers for precision and recall hint that my data is balanced, and I'm doing well both in spotting the right cases and making sure my positive picks are accurate. But, I have to remember, these scores come from the training set, so they might be a bit on the sunny side. For a clearer picture of how my model really performs, I should test it on some new data or try out cross-validation.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('riding_mowers.csv')\n\ndf.head(4)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\ndf.info()\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\ndf['Ownership'].unique()\ndf['Ownership'].value_counts()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf=df.replace(['owners'], 'Owner' ) \ndf=df.replace(['nonowner'],'Nonowner')\ndf['Ownership'].value_counts()\n\n# visualize the data\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf_dummy\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nobserv = df_dummy.drop(columns=['Ownership_Owner'])\ntarget = df_dummy['Ownership_Owner']\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1       \n)\nmodel.fit(observ, target)\n\nresults = pd.DataFrame()\nresults['actual'] = target\nresults['predicted'] = model.predict(observ)\nresults\n\n\n# ## Step 5: Analyze the linear regression model's performance\n\n\n# Record the models performance using the following metrics: for test data\n# - Accuracy\nprint(accuracy_score(target, model.predict(observ)))\n# - Precision\nprint(precision_score(target, model.predict(observ)))\n# - Recall\nprint(recall_score(target, model.predict(observ)))\n# - F1 Score\nprint(f1_score(target, model.predict(observ)))\n\n\n\n\n\n# By the above regression model's performance it achieves high accuracy, precision, recall, and F1 score of 83.8 percentage.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('C:/Users/maggi/Documents/Classes/MSAA/Data Mining/A3/riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\ndf.head()\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any typos, and if so, then fix this issue.\nxx = df['Ownership'].unique()\ndf = df.replace(['owners'], 'Owner' )\ndf = df.replace(['nonowner'], 'Nonowner' )\nxxx = df['Ownership'].unique()\nprint(xx)\nprint(xxx)\n\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\ntype(df[['Ownership']])\n\n\n# standardize the input data \ndf[['Income','Lot_Size']] = preprocessing.scale(df[['Income','Lot_Size']]) \nprint(df.head())\n\ndf = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int8'\n)\n\ndf\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nfeatures = df[['Income','Lot_Size']]\ntarget = df['Ownership_Owner']\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.4, random_state=42)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nresults = pd.DataFrame()\nresults['actual'] = y_test\nresults['predicted'] = model.predict(X_test)\nresults\n\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\ny_pred = model.predict(X_test)\n# - Accuracy\naccuracy = accuracy_score(y_test, y_pred)\n# - Precision\nprecision = precision_score(y_test, y_pred)\n# - Recall\nrecall = recall_score(y_test, y_pred)\n# - F1 Score\nf1 = f1_score(y_test, y_pred)\n\n# Print results\nprint(accuracy)\nprint(precision)\nprint(recall)\nprint(f1)\n\n\n# ## Step 6: Discuss the Results\n# ...insert a discussion of the results from your model\n# The accuracy metric is fairly high at 80%. This means that 80% of all preductions were accurate. Depending on the model, this may be more or less acceptable, however, this value may be skewed due to the small sample size in the current model. A precision on 1 means that for all predicted positive value, 100% were actually positive. This is important if the impact of a false positive is large. A recall value of 2/3 is not great but may have the same limitations from the small sample size. this means for the total population of values that should have been predicted true, only 2/3 were actually presicted to be true. This means that when the model preditcs true it is always right, but it will predict more false negatives. In a situation where failure to identify a trait is catastroic, this would not be good. Lastly, the f1 score shows that this model will be moderately successful when the effects of precision and recall are balanced. More data may be needed to produce a model that is more suitable to address Type I and II error.\n\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n\n# set the random seed\nnp.random.seed(88005599) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('riding_mowers.csv')\n\ndf.head(3)\n\n\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\ndf.info()\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\ndf['Ownership'].unique()\ndf['Ownership'].value_counts()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf=df.replace(['owners'], 'Owner' ) \ndf=df.replace(['nonowner'],'Nonowner')\ndf['Ownership'].value_counts()\n\n# visualize the data\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf_dummy\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nfeatures = df_dummy.drop(columns=['Ownership_Owner'])\ntarget = df_dummy['Ownership_Owner']\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1       \n)\nmodel.fit(features, target)\n\n# ## Step 5: Analyze the linear regression model's performance\nresults = pd.DataFrame()\nresults['actual'] = target\nresults['predicted'] = model.predict(features)\nresults\n\n\n# Record the models performance using the following metrics: for test data\n# - Accuracy\nprint(accuracy_score(target, model.predict(features)))\n# - Precision\nprint(precision_score(target, model.predict(features)))\n# - Recall\nprint(recall_score(target, model.predict(features)))\n# - F1 Score\nprint(f1_score(target, model.predict(X_t)))\n\n\n\n\n\n# # Discussion on results\n# 1.Accuracy: In this case, the model achieved an accuracy of 0.83, which means it correctly predicted the outcome for 83% of the train data,indicating that it has a good overall predictive performance.\n# 2.Precision: The precision score of 0.83 suggests that when the model predicts a positive class it is correct approximately 83% of the time. we can that out of all the samples it classified as positive, 83% were truly positive.\n# 3.Recall: The recall score of 0.83 indicates that the model identified approximately 83% of all actual positive samples. In other words, it has a good ability to find positive samples in the dataset.\n# 4.f1 Score: The F1 score of approximately 0.83 indicates that the model achieves a good balance between precision and recall\n# ...insert a discussion of the results from your model\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ### Revanth Mandadi- U16635207\npip install ipympl\n\n# import neccessary libraries\n\n\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 1: Import the libraries we will use in this notebook\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r\"C:\\Users\\revanths4\\Downloads\\riding_mowers.csv\")\n\ndf.head(24)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n# ## Step 6: Discuss the Results\n# Th Logistic Rgrssion modl achivd an accuracy of 83.33%, which mans that it corrctly prdictd th ownrship status of 83.33% of th individuals in th datast. Th modl also achivd a prcision of 83.33%, maning that 83.33% of th individuals prdictd to b ownrs by th modl wr actually ownrs. Th modl also achivd a rcall of 83.33%, maning that 83.33% of all ownrs in th datast wr corrctly prdictd by th modl. Finally, th modl achivd an F1 scor of 0.8333, which is a masur of both prcision and rcall.\n# Ovrall, th Logistic Rgrssion modl prformd wll on this datast. It was abl to accuratly prdict th ownrship status of a majority of th individuals in th datast. Howvr, it is important to not that th modl is only as good as th data it is traind on. If th data is biasd or incomplt, th modl will likly b biasd as wll.\n# Anothr important considration is th fact that th datast is rlativly small. This mans that th modl may not b abl to gnraliz wll to nw data. To improv th modl's gnralizability, it would b hlpful to collct mor data and train th modl on a largr datast.\n# Conclusion\n# Th Logistic Rgrssion modl is a simpl but ffctiv machin larning algorithm that can b usd for classification tasks. In this cas, th modl was abl to accuratly prdict th ownrship status of a majority of th individuals in th datast. Howvr, it is important to not that th modl is only as good as th data it is traind on, and it may not b abl to gnraliz wll to nw data. \n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Importing Libraries\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\n#from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom matplotlib import pyplot as plt\n\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib inline\n\n# ## Loading data \ndf = pd.read_csv('riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\ndf.shape\n\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'nonowner': 'Nonowner', 'owners': 'Owner'})\nprint(df['Ownership'].unique())\n\n# no typos\n\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data, dummy encoding the result\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int8'\n)\n\ndf_dummy.head(10)\n\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\n\n#X_train, y_train\n\nX = df_dummy[['Income','Lot_Size']]\n\ny = df_dummy[['Ownership_Owner']]\n\n\n\nmodel.fit(X,y)\n\n\n\n\n\n# ## Step 5: Analyze the linear regression model's performance\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nprint (results)\n\ncm = confusion_matrix(results['actual'], results['predicted'])\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n\n\n\n\n\nprint(cm)\n\nprint(accuracy_score(results['actual'], results['predicted']))\nprint(precision_score(results['actual'], results['predicted']))\nprint(recall_score(results['actual'], results['predicted']))\nprint(f1_score(results['actual'], results['predicted']))\n\n\n# ## Discussing the Results\n# my accuracy, precession, recall and f1 score are all equal to 0.83.\n# An accuracy of 0.83 implies that it predicted if the holder is an owner or not exactly 83.3% of the time.\n# A precession of 0.83 implies that it predicted correctly if the holder is an owner 83.3% of the time for all its preditions saying the holder was an owner.\n# A recall of 0.83 implies that the model predicted 83.3% of all  the relavant instances where he was actually an owner.\n# An F1 score of 0.83 implies an estimated perforamnce of the model is 83%.\n\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# Prajay Reddy Minuka\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('riding_mowers (1).csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n# There are 24 rows and 3 columns\ndf.info()\n\n# check to see if there is any missing values, and if so, then fix this issue.\n# No null values found\ndf.isnull()\n\n# check to see if there are any typos, and if so, then fix this issue.\n\nprint(df['Ownership'].unique())\ndf=df.replace(['owners'],'Owner')\ndf=df.replace(['nonowner'],'Nonowner')\nprint(df['Ownership'].unique())\n\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \ndf_dummy = pd.get_dummies(\n            df,\n            prefix_sep = '_', \n            dummy_na = False, \n            drop_first = True, \n            columns = ['Ownership'],\n            dtype = 'int32')\ndf_dummy.head()\n\nX = df_dummy.drop(columns=['Ownership_Owner'])\ny = df_dummy['Ownership_Owner']\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nfrom sklearn.linear_model import LogisticRegression\n\nModel = LogisticRegression(max_iter= 1000,  \n                           n_jobs  = -1)\n\nModel.fit(X, y)\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nprint(accuracy_score(y, Model.predict(X)))\nprint(precision_score(y, Model.predict(X)))\nprint(recall_score(y, Model.predict(X)))\nprint(f1_score(y, Model.predict(X)))\n\n\n# ## Step 6: Discuss the Results\n# ...insert a discussion of the results from your model\n# The model has an accuracy for 83.3% and has the same scores all the metrics. \n",
        "# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix, ConfusionMatrixDisplay\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\grees\\Downloads\\riding_mowers.csv')\n\ndf.head(5)\n\n# ## Step 3: Prepare the Data\n#determine the number of rows and columns\nr = df.shape[0]\nc = df.shape[1]\nprint(f\"No. of Rows={r} and No. of Columns={c}\")\n\n#check to see if there is any missing values, and if so, then fix this issue.\ndf.isnull().all()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\ndf.replace(['owners'], 'Owner', inplace= True)\ndf.replace(['nonowner'], 'Nonowner', inplace= True)\ndf.head(50)\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nP = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\nr = df['Ownership']# Target variable \n\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(P, r)\n\nresults = pd.DataFrame()\nresults['actual'] = r\nresults['predicted'] = model.predict(P)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Greens\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(r, model.predict(P)))\n# - Precision\nprint(precision_score(r, model.predict(P), pos_label='Owner'))\n# - Recall\nprint(recall_score(r, model.predict(P), pos_label='Owner'))\n# - F1 Score\nf1_score(r, model.predict(P), pos_label='Owner')\n\n\n# ## Step 6: Discuss the Results\n# We began with a dataset with 24 observations for this model. We next created a classifier and used dummy variables to prepare the target variable, which was categorical. Additionally, we used the StandardScaler function to standardise the dataset's numerical features. The next step was model fitting, which involved maximising the values of two distinct parameters: max_iter and n_jobs.\n# The model's performance was then assessed by looking at a number of parameters, including Accuracy, Precision, Recall, and F1 Score. I used the same dataset for both training and testing, which is why there is consistency in the results. The consistency in the metric scores is explained by the fact that we obtained identical findings for FN and FP, followed by the same results for TP and TN.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom matplotlib import pyplot as plt\n\n\n# set the random seed\nnp.random.seed(42) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('C:/Users/hrush/OneDrive/Desktop/DM/Assignment 3/riding_mowers.csv')\n\ndf.head(15)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\ndf = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nX = df[['Income', 'Lot_Size']]\ny = df['Ownership_Owner']\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label=1))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label=1))\n# - F1 Score\nprint(f1_score(y, model.predict(X), pos_label=1))\n\n\n\n# ## Step 6: Discuss the Results\n# This dataset has 24 rows and 3 columns . They are categorized based on their Income , Lot_Size and Ownership. No missing values were found and ownership status discrepancies were categorized as Owner and Nonowner for uniformity. The confusion matrix provides insights into the model's performance - True Positives(TP) = 10 , True Negatives(TN) = 2 , False Positives(FP) = 10 , False Negatives(FN) = 2.\n# With the following performance metrics Accuracy(83.33%) , Precision(83.33%), Recall(83.33%) , F1 score(83.33%) we can conclude that they provide a compelling picture of a model which can accurately categorize Owners and Nonowners.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom matplotlib import pyplot as plt\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('https://raw.githubusercontent.com/prof-tcsmith/data/master/RidingMowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\n#There are no null values\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\n# To check for typos I checked for unique values so that I can find any irrelavant observations about the data in a specific column.\nIncome_unique = df['Income'].unique()\nLot_Size_unique = df['Lot_Size'].unique()\nOwnership_unique = df['Ownership'].unique()\n\nprint(Income_unique)\nprint(Lot_Size_unique)\nprint(Ownership_unique)\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf_dummy\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nfeatures = df_dummy.drop(columns=['Ownership_Owner'])\ntarget = df_dummy['Ownership_Owner']\n\nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\nmodel.fit(features, target)\n\n\nresults = pd.DataFrame()\nresults['actual'] = target\nresults['predicted'] = model.predict(features)\nresults\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\n\nprint(accuracy_score(target, model.predict(features)))\nprint(precision_score(target, model.predict(features)))\nprint(recall_score(target, model.predict(features)))\nprint(f1_score(target, model.predict(features)))\n\n# ## Step 6: Discuss the Results\n# ...insert a discussion of the results from your model\n# Accuracy : The accuracy score of 83.33 percent states that the model correctly predicted the target variable(Ownership_Owner) for approximately 83 out of every 100 instances in your dataset.\n# Precision : True Positives/(True Positives + False Positives) which in our case is 20/20 + 4  = 0.8333, which says that, 83 percent were correctly classified\n# recall_score : True Positives/True Positives + False Negatives which in our case is 20/20+4 = 0.8333, which says, 83 percent were correctly classified\n# f1_score : 2 * (Precision * recall_score)/(Precision + recall_score) which in our case is 0.8333, which says, 83 percent were correctly classified\n",
        "# ## PAVAN KUMAR - ASSIGNMENT 03 :\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n%matplotlib inline\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('C:/Users/pabba/Downloads/riding_mowers.csv')\n\ndf.head(15)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# As we can see in the data, There are some typos in the Ownership column that to be fixed.\ndf['Ownership'].unique()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'] = df['Ownership'].replace({'owners':'Owner','nonowner':'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\ndf.info()\n\ndf.head(20)\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nX = df[['Income','Lot_Size']] \ny = df['Ownership'] \n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,  # number of iterations taken as 1000\n    n_jobs=-1       # use all processors\n)\nmodel.fit(X, y)\n\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# Displaying the Confusion Matrix from predictions\n\nCM = confusion_matrix(results['actual'],results['predicted'])\nfig, ax = plt.subplots(figsize=(5,5))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'],results['predicted'], display_labels = model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\naccuracy = (accuracy_score(y,model.predict(X)))\nprint(f\"Accuracy: {accuracy}\")\n\n# - Precision\nprecision = (precision_score(y,model.predict(X), pos_label ='Owner'))\nprint(f\"Precision: {precision}\")\n\n# - Recall\nrecall = (recall_score(y,model.predict(X), pos_label ='Owner'))\nprint(f\"Recall: {recall}\")\n\n# - F1 Score\nf1_Score = (f1_score(y,model.predict(X), pos_label ='Owner'))\nprint(f\"F1 Score: {f1_Score}\")\n\n\n\n# ## Step 6: Discuss the Results\n# Analyzed how well the linear regression model worked by using different measures like Accuracy, Precision, Recall, and F1 Score. Surprisingly, all these measures showed the same results i.e 83.33%. This means the model consistently made accurate predictions, showing that it's really good at predicting ownership based on income and lot size.\n\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nfrom random import randint\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('./riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\ndf=df.replace(['owners'], 'Owner' ) \ndf=df.replace(['nonowner'], 'Nonowner' ) \n\ndf['Ownership'].unique()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nprint(df.head())\n\ndf1 = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf1.head()\n\nx = df1.drop(columns=['Ownership_Owner'])\ny = df1['Ownership_Owner']\n\nprint(x.shape, y.shape)\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\nmodel.fit(x, y)\n\n# ## Step 5: Analyze the linear regression model's performance\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ny_pred = model.predict(x)\ny_pred\n\n# display the confusion matrix\ncm = confusion_matrix(y, y_pred)\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    y, y_pred, display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# Record the models performance using the following metrics:\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# - Accuracy\nprint(accuracy_score(y, y_pred))\n\n# - Precision\nprint(precision_score(y, y_pred))\n\n# - Recall\nprint(recall_score(y, y_pred))\n\n# - F1 Score\nprint(f1_score(y, y_pred))\n\n# ## Step 6: Discuss the Results\n# 1. An accuracy of 0.8333 indicates that the model correctly predicted the class labels for approximately 83.33% of all data points in the dataset. This means that the model is making correct predictions for a significant portion of the instances.\n# 2. A precision of 0.8333 suggests that when the model predicts the \"Owner\" class, it is correct approximately 83.33% of the time.\n# 3. A recall score of 0.8333 indicates that the model correctly identifies approximately 83.33% of all individuals who are actual \"Owners.\"\n# 4. An F1 score of 0.8333 indicates a good balance between precision and recall. This suggests that the model achieves a balance between correctly identifying \"Owners\" and minimizing incorrect \"Owner\" predictions.\n# In summary, the logistic regression model performs consistently across multiple evaluation metrics, with high values for accuracy, precision, recall, and F1 Score. This indicates that the model is reliable in making predictions for both \"Owner\" and \"Nonowner\" classes.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Lavanya Peddireddy\n# ## U00142531\n# ## Module 3 Assignment\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n# import neccessary libraries\n\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(\"C:/Users/lavan/Downloads/riding_mowers.csv\")\n\ndf.head(30)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf.isna().sum()\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model.\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Reds\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n\n\n# ## Step 6: Discuss the Results\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n\n\n\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\n# df = pd.read_csv('https://raw.githubusercontent.com/prof-tcsmith/data/master/RidingMowers.csv')\ndf = pd.read_csv(\"C:\\\\Users\\\\rupesh\\\\Documents\\\\DataMining\\\\week05\\\\riding_mowers.csv\")\n\n\ndf\n\n\ndf.columns\ndf['Ownership'].unique()\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf.columns\ndf['Ownership'].unique()\n\ndf = df.replace(['nonowner'],'Nonowner')\ndf = df.replace(['owners'],'Owner')\ndf['Ownership'].unique()\n\n\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\ndf.columns\n\ndf = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf\n\n\n\n\n\nfeatures = df.drop(columns=['Ownership_Owner'])\ntarget = df['Ownership_Owner']\ntarget\n\n# standardize the input data \n\nscaler = StandardScaler()\nfeatures_scaled = pd.DataFrame(scaler.fit_transform(features),\n                       index=features.index, columns=features.columns)\nfeatures_scaled\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\nmodel.fit(features_scaled, target)\n\nresults = pd.DataFrame()\nresults['actual'] = target\nresults['predicted'] = model.predict(features_scaled)\nresults\n\n#display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(target, model.predict(features_scaled)))\n\n# - Precision\nprint(precision_score(target, model.predict(features_scaled)))\n\n# - Recall\nprint(recall_score(target, model.predict(features_scaled)))\n\n# - F1 Score\nprint(f1_score(target, model.predict(features_scaled)))\n\n\n# ## Step 6: Discuss the Results\n# #### Values obtained for accuracy_score, precision_score, recall_score, f1_score is same and found as 0.8333333333333334. This is because of using the same dataset for both training and predicting.\n\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nimport matplotlib.pylab as plt\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(\"E://MASTER'S - USF//Fall 23'//Data Mining//riding_mowers.csv\")\n\ndf.head(10)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nrows = df.shape[0]\ncolumns = df.shape[1]\nprint(f\"Number of Rows={rows} and Number of Columns={columns}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'nonowner': 'Nonowner', 'owners': 'Owner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='Red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='Indigo')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(40, 140)\nax.set_ylim(15, 30)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\ninput_data = df[['Income', 'Lot_Size']]\n\ntarget_variable = df['Ownership']\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(input_data, target_variable)\n\nresults = pd.DataFrame()\nresults['actual'] = target_variable\nresults['expected'] = model.predict(input_data)\nresults\n\n# display the confusion matrix\nconf_matrix = confusion_matrix(results['actual'], results['expected'])\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['expected'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Purples\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(target_variable, model.predict(input_data)))\n# - Precision\nprint(precision_score(target_variable, model.predict(input_data), pos_label='Owner'))\n# - Recall\nprint(recall_score(target_variable, model.predict(input_data), pos_label='Owner'))\n# - F1 Score\nf1_score(target_variable, model.predict(input_data), pos_label='Owner')\n\n\n# ## Step 6: Discuss the Results\n# We started with a dataset containing 24 observations. To build a classifier, we transformed the categorical target variable using dummy variables and standardized the numerical features using the StandardScaler function. We then proceeded to fit the model by optimizing two key parameters: max_iter and n_jobs.\n# To evaluate the model's performance, we considered several metrics, including Accuracy, Precision, Recall, and F1 Score. Notably, we used the same dataset for both training and testing, which is why the results showed consistent metrics. This consistency in the metric scores can be attributed to obtaining identical results for False Negatives (FN) and False Positives (FP), followed by the same outcomes for True Positives (TP) and True Negatives (TN).\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport random\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('C:/Users/vines/Downloads/riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isnull().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\ndf.replace(['owners'], 'Owner', inplace= True)\ndf.replace(['nonowner'], 'Nonowner', inplace= True)\ndf.head(100)\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nfeatures = df.drop(columns=['Ownership'])\ntarget = df['Ownership']\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(features,target)\n\nresults = pd.DataFrame()\nresults['actual'] = target\nresults['predicted'] = model.predict(features)\nresults\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nprint('Acuuracy Score: ',accuracy_score(results['actual'], results['predicted']))\nprint(classification_report(results['actual'], results['predicted']))\nprint(confusion_matrix(results['actual'], results['predicted']))\n\n# ## Step 6: Discuss the Results\n# The values of accuracy, precision, recall and f1 score is same because of the small dataset and hence there is no difference in them. From the confusion matrix it is evident that most of the values are in True negatives and True positives supporting accuracy of the model which is 83%. Hence it is a good prediction model.\n",
        "# # Assignment 3\n# ISM6136\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable.\n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(\"C:/Users/User/Downloads/riding_mowers.csv\")\n\ndf.head(11)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\n\nnum_rows, num_columns = df.shape\n\nprint(f\"No of rows: {num_rows}\")\nprint(f\"No of columns: {num_columns}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\nmissing_values = df.isnull().sum()\n\nprint(\"Missing Values:\")\nprint(missing_values)\n\n# check to see if there are any typos, and if so, then fix this issue.\n\n\n\ncolumns_to_check = ['Ownership']\nfor column in columns_to_check:\n    unique_values = df[column].unique()\n    print(f\"Unique values in '{column}':\")\n    print(unique_values)\n\n\n# Correcting the typos errors\ndf.loc[df['Ownership'] == 'owners', 'Ownership'] = 'Owner'\ndf.loc[df['Ownership'] == 'nonowner', 'Ownership'] = 'Nonowner'\n\ncolumns_to_check = ['Ownership']\nfor column in columns_to_check:\n    unique_values = df[column].unique()\n    print(f\"Unique values in '{column}':\")\n    print(unique_values)\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nX = df[['Income', 'Lot_Size']]  \ny = df['Ownership'] \n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(X)\n\nclassifier = RandomForestClassifier(random_state=42)\n\nclassifier.fit(X_scaled, y)\n\n\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# Displaying the confusion matrix\n\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\naccuracy = (accuracy_score(y, model.predict(X)))\n# - Precision\nprecision = (precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nrecall = (recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1 = (f1_score(y, model.predict(X), pos_label='Owner'))\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n\n\n\n# ## Step 6: Discuss the Results\n# Accuracy: This measures how often the model is correct overall. An accuracy of 83% means it's right for most cases.\n# Precision: Precision gauges how often the model's positive predictions are accurate. With a precision of 83%, the model is usually correct when it says something is positive.\n# Recall: Recall assesses how well the model finds all positive cases. At 83%, it's good at finding the majority of positive cases.\n# F1 Score: The F1 score strikes a balance between precision and recall. An F1 score of 83% means the model is reliable in both finding positives and being accurate about them.\n# In short, model is accurate and trustworthy in identifying positive cases, like finding red marbles in a bag.\n",
        "# # Predictive Modeling Example\n# Logistic Example Model Fitting**\n# ## Step 1: Importing the libraries \n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n%matplotlib inline\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('C:/Users/ShwethaS/Downloads/riding_mowers.csv')\n\ndf\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\nprint(f\"Rows : {df.shape[0]} and Columns : {df.shape[1]}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\ndf = df.dropna(axis=0, thresh=int(0.25*(df.shape[1]-1)))\n\ndf.shape\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\ndf['Ownership'] = df['Ownership'].replace({'nonowner': 'Nonowner', 'owners': 'Owner'})\ndf['Ownership'].unique()\n\ndf_dummies = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='*', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='*', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Creating a Classifier\n# Since the number of observations is low, we just allocate all the data into training set.\n# standardize the input data \n\nInd_Var_Set=df[['Income','Lot_Size']]\nTarget = df['Ownership']\n\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(Ind_Var_Set, Target)\n\nResult = pd.DataFrame()\nResult['Actual']= Target\nResult['Predicted'] = model.predict(Ind_Var_Set)\nResult\n\n# display the confusion matrix\ncm = confusion_matrix(Result['Actual'], Result['Predicted'])\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nConfusionMatrixDisplay.from_predictions(\n    Result['Actual'], Result['Predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Reds\n)\n\nplt.show()\n\n# ## Step 5: Analyzing the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(Target, model.predict(Ind_Var_Set)))\n# - Precision\nprint(precision_score(Target, model.predict(Ind_Var_Set), pos_label='Owner'))\n# - Recall\nprint(recall_score(Target, model.predict(Ind_Var_Set), pos_label='Owner'))\n# - F1 Score\nf1_score(Target, model.predict(Ind_Var_Set), pos_label='Owner')\n\n\n\n# ## Step 6: Results\n# For this model, The given dataset consists of 24 observations. Then we checked if there are any missing values or misspelled words and corrected them. We created a Classifier to prepare the target variable 'Ownership',which is categorical. The next step was model fitting.\n# The effectiveness of the model was then evaluated by taking into account a number of factors, such as Accuracy, Precision, Recall, and F1 Score. All of them are observed to be 0.834. As we used the whole data for training and tested the same, results are observed to be consistent.We found the same results for FN and FP, then the same findings for TP and TN, which explains the consistency in the metricscores. \n# In simple terms, We predicted the probability of certain class( Ownership) using the dependent variables (Income and Lot size).\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\sathw\\Downloads\\riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isnull().sum()\ndf.Ownership.value_counts()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'] = df['Ownership'].replace('owners', 'Owner')\ndf['Ownership'] = df['Ownership'].str.lower()  \ndf['Ownership'] = df['Ownership'].str.capitalize()\n\n\n#creating target and assigning Nonowner as 0 and owner/owners as 1\ndf['target'] = df.Ownership.map(lambda x: 0 if x == \"Nonowner\" else 1)\n\ndf.Ownership.value_counts()\n\ndf.head()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf['Income'] = scaler.fit_transform(df[['Income']])\nscaler = StandardScaler()\ndf['Lot_Size'] = scaler.fit_transform(df[['Lot_Size']])\n\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nfrom sklearn.linear_model import LogisticRegression\nX = df[['Income','Lot_Size']]\ny = df[['target']]\nmodel = LogisticRegression(random_state=42, max_iter=1000,penalty='l2',C=0.01)\nmodel.fit(X, y)\n\n\ny_pred = model.predict(X)\n\ny_pred\n\nresults = pd.DataFrame({\"y_true\":y.values.ravel(),\"y_pred\":y_pred})\nresults['pred'] = results.y_true + results.y_pred\nresults['pred'] = results.pred.map(lambda x: False if x==1 else True)\nresults\n\n# ## Step 5: Analyze the logistic regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\n\n\n\n# Actual and predicted labels\nactual_labels = results.y_true.values\npredicted_labels = results.y_pred.values\n\n# Accuracy\naccuracy = accuracy_score(actual_labels, predicted_labels)\n\n# Precision\nprecision = precision_score(actual_labels, predicted_labels)\n\n# Recall\nrecall = recall_score(actual_labels, predicted_labels)\n\n# F1 Score\nf1 = f1_score(actual_labels, predicted_labels)\n\n# Print the metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n\n\n# Calculate the confusion matrix\nconfusion = confusion_matrix(actual_labels, predicted_labels)\n\n# Create a heatmap to visualize the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"Actual Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\ny_values = model.coef_[0][0] * df.Income.values + model.coef_[0][1] * df.Lot_Size.values + model.intercept_[0]\n\ny_values\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n         color='blue')\n\n\nplt.plot(np.arange(df.Income.min(),df.Income.max(),abs(df.Income.min()-df.Income.max())/24),y_values, label=\"Regression Line\", color=\"red\")\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(-2,2)\n#ax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n\n\n\n\n# ## Step 6: Discuss the Results\n# ...insert a discussion of the results from your model\n# ### Model Creation\n# A logistic regression model was created using the standardized features (\"Income\" and \"Lot_Size\").\n# The model was trained on the entire dataset due to the low number of observations, although typically data would be split into training and testing sets to evaluate generalization performance.\n# ### Model Performance\n# The performance of the logistic regression model was evaluated using several metrics:\n# Accuracy: 0.875\n# Precision: 0.846\n# Recall: 0.917\n# F1 Score: 0.880\n# These metrics provide a good overall view of the model's performance. The high accuracy, precision, and recall values suggest that the model is performing well in predicting ownership of riding mowers based on income and lot size.\n# ### Confusion Matrix\n# The confusion matrix shows that the model  have less false positives and false negatives which is indicating good performance\n# ### Regression line\n# The regression line plotted over the data points in the next figure. The red line represents the logistic regression decision boundary\n# The line visually separates the \"Owner\" and \"Nonowner\" data points, showing how the model makes predictions based on the input features.\n# ### Conclusion\n# It successfully predicts ownership of riding mowers based on income and lot size, with high accuracy, precision, recall, and F1 score.\n",
        "#Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n%matplotlib inline \n\n#Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\thota\\Downloads\\riding_mowers.csv')\n\ndf.head(20)\n\n#Step 3: Prepare the Data\n# determine the number of rows and columns\n\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n#Step 4: Create a Classifier\n\n#Since there are few observations, all of the data will be allocated to training. Typically, the data would be divided into training and testing sets.\n\n\n# standardize the input data \n\nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n#Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n\n\n\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n%matplotlib inline\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('assg3_data.csv')\n\ndf.head(14)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nr = df.shape[0]\nc = df.shape[1]\nprint(f\"Rows={r} and Cols={c}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'nonowner': 'Nonowner', 'owners': 'Owner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nx = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(x, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predict'] = model.predict(x)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predict'])\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predict'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Reds\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(x)))\n# - Precision\nprint(precision_score(y, model.predict(x), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(x), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(x), pos_label='Owner')\n\n\n\n# ## Step 6: Discuss the Results\n# The content outlines the creation and development of a logistic regression classifier using the given dataset. An evaluation is performed using a confusion matrix, which assesses the classifiers performance as 'Owner' and 'Nonowner' categorical values. The accuracy rate of the model is high i.e, greater than 80% and error rate is less than 20%. The model has good accuracy, precision and f1 scores and balanced in estimating all 4 quadrations of the confusion matrix,which shows that it is ideal for making predictions.\n",
        "# ### Jayanth Uppara| U46027839\n# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\n# import neccessary libraries\n\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(\"/Users/jay/Documents/Untitled Folder/riding_mowers.csv\")\n\ndf.head()\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf.isna().sum()\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model.\nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Reds\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n\n\n# ## Step 6: Discuss the Results\n\n\n\n\n",
        "# # Assignment 03\n# ## Step 1: Import the libraries we will use in this notebook\n# import necessary libraries\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pylab as plt\n# set the random seed\nnp.random.seed(1)  # set this to ensure the results are repeatable.\n\n%matplotlib inline \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\vadla\\Downloads\\riding_mowers.csv')\n\ndf.head(20)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\ndf\n\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \n\nX = df[['Income', 'Lot_Size']]  # Features (all columns except the target)\ny = df['Ownership']# Target variable\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\n\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label='Owner'))\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label='Owner'))\n# - F1 Score\nf1_score(y, model.predict(X), pos_label='Owner')\n\n\n\n\n# ## Step 6: Discuss the Results\n# ### Here, we have used many metrics such as accuracy, precision, recall, and F1 score in order to assess the performance of the model. During analysis, we have found that the aforementioned tests produced comparable results due to the equal rates at which false positives, false negatives, true positives, and true negatives have occurred. Precision metric focuses on the accuracy of forecasts produced by \"Owner\" class members whereas Accuracy metric provides a measure of overall correctness. By balancing out the metrics of  Precision and Recall, the F1 Score evaluates the capacity of the model to correctly identify each instance of the \"Owner\" class. Since false positives and false negatives have different implications on the evaluation, these criteria are crucial for determining the efficiency of categorization.\n",
        "# # Predictive Modeling Example -  Logistic Example Model Fitting\n# ## Step 1: Import the libraries we will use in this notebook\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# import neccessary libraries\n\n# set the random seed\nnp.random.seed(10) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\Saurabh Verma\\OneDrive\\Desktop\\Data Mining\\Assignment - 3\\riding_mowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\n\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\n\ndf.isna().sum()\n\ndf.isnull().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\n\ndf['Ownership'].unique()\n\ndf.replace(['owners'], 'Owner', inplace= True)\ndf.replace(['nonowner'], 'Nonowner', inplace= True)\ndf.head(50)\n\ndf['Income'].unique()\n\ndf['Lot_Size'].unique()\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n## independent and dependent variable\n\nX = df[['Income','Lot_Size']]\ny = df[['Ownership']]\n\n# standardize the input data \n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(multi_class = 'ovr', solver = 'lbfgs')\n\ny.head(3)\n\n## model.fit(X, y.ravel())\n\n# trianing model\n\nmodel.fit(X,y)\n\ny_pred = model.predict(X)\n\ny_pred\n\ndfff = pd.DataFrame()\ndfff['y_pred'] = y_pred\ndfff['y'] = y\n\ndfff\n\nmodel.predict_proba(X)\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\n\n\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,ConfusionMatrixDisplay\n\ncm = confusion_matrix(y, y_pred, labels = model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = model.classes_)\ndisp.plot()\nplt.show()\n\nprint(confusion_matrix(y_pred,y))\nprint(accuracy_score(y_pred,y))\nprint(classification_report(y_pred,y))\n\n# ## Step 6: Discuss the Results\n# ...insert a discussion of the results from your model\n# ### Accuracy:\n# Accuracy, within the context of a confusion matrix, serves as a metric to gauge the proportion of correct predictions made by a classification model out of the total predictions. To break it down further:\n# True Positives (TP) are instances where the model correctly predicted the positive class.\n# True Negatives (TN) are instances where the model correctly predicted the negative class.\n# False Positives (FP) are instances where the model incorrectly predicted the positive class.\n# False Negatives (FN) are instances where the model incorrectly predicted the negative class.\n# The accuracy is calculated as:\n# Accuracy = (TP + TN) / (TP + TN + FP + FN)\n# In this particular model, it is correct approximately 83.33% of the time, effectively distinguishing between owners and non-owners with a high degree of accuracy.\n# ### Precision:\n# Precision, when dealing with a confusion matrix, is a measure that evaluates the accuracy of positive predictions made by a classification model. It specifically quantifies the proportion of true positive predictions (instances correctly predicted as positive) out of all instances predicted as positive. The formula for precision is as follows:\n# Precision = TP / (TP + FP)\n# In the context of the mentioned model, it achieves a precision score of approximately 0.8333, signifying that it accurately identifies positive instances about 83.33% of the time.\n# ### Recall:\n# Recall, also referred to as Sensitivity or True Positive Rate, is a metric within a confusion matrix that assesses the model's capability to correctly identify all relevant instances of the positive class. \n# The Recall is calculated as:\n# Recall = TP / (TP + FN)\n# In the given model, the recall score is approximately 0.8333, indicating that it effectively captures about 83.33% of all relevant positive instances.\n# ### F1 Score:\n# The F1 Score, alternatively known as the F1 measure, is a composite metric in a confusion matrix that amalgamates both precision and recall into a single value. It proves particularly valuable when dealing with imbalanced datasets or when there's a need to strike a balance between making precise positive predictions (precision) and ensuring the capture of all pertinent positive instances (recall). \n# The F1 Score is calculated as:\n# F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n# For the model in question, the F1 score is approximately 0.8333, signifying a well-balanced performance that is correct about 83.33% of the time, taking into account both precision and recall.\n",
        "# ## ASSIGNMENT_3_DATA_MINING\n# ### ABHIJIT VILAS WAYAL U09616725\n# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv(r'C:\\Users\\wayal\\OneDrive\\Desktop\\ABHIJIT\\USF\\DATA MINING\\w5\\ASSIG3\\data\\riding_mowers.csv')\n\ndf.head(5)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\ndf.info()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Income'].unique()\ndf['Lot_Size'].unique()\ndf['Ownership'].unique()\n\ndf\n\ndf.iloc[3].Ownership\n\ndf = df.replace(['owners'], 'Owner')\n\ndf\n\ndf = df.replace(['nonowner'], 'Nonowner')\n\ndf\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \ndf_dummy = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'],\n    dtype='int32'\n)\n\ndf_dummy.head(24)\n\nX = df_dummy[['Income', 'Lot_Size']]\ny = df_dummy['Ownership_Owner']\n\nprint(X)\n\nprint(y)\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model.\nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\nmodel.fit(X, y)\n\n# ## Step 5: Analyze the linear regression model's performance\nresults = pd.DataFrame()\nresults['Actual'] = y\nresults['Predicted'] = model.predict(X)\nresults\n\n# display the confusion matrix\ncm = confusion_matrix(results['Actual'], results['Predicted'])\n\nfig, ax = plt.subplots(figsize=(9, 9))\n\nConfusionMatrixDisplay.from_predictions(\n    results['Actual'], results['Predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Reds\n)\n\nplt.show()\n\n# Record the models performance using the following metrics:\n\nAccuracy = accuracy_score(y, model.predict(X))\nPrecision = precision_score(y, model.predict(X), average='weighted')\nRecall = recall_score(y, model.predict(X), average='weighted')\nF1_score = f1_score(y, model.predict(X), average='weighted')\n\nprint(Accuracy)\nprint(Precision)\nprint(Recall)\nprint(F1_score)\n\n# ## Step 6: Discuss the Results\n# Accuracy:\n# The accuracy of model is 0.8333, which means that the model correctly predicted 83.33% times whether it was a owner or nonowner.\n# Precision:\n# Precision tells us how well the model does at correctly classifying something as positive(Owner or Nonowner).The precision of model is 0.8333.\n# Recall:\n# The recall measures the model ability to detect Positive samples(Owner or Nonowner).The recall of model is 83.33%\n# F1_score:\n# F1 score is the harmonic mean of a models precision and recall.A F1 score of 83.33% shows that there is a good balance between the ability to identify positive events and classify them correctly.\n\n\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ## Step 1: Import the libraries we will use in this notebook\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom matplotlib import pyplot as plt\n\n# set the random seed\nnp.random.seed(1) # set this to ensure the results are repeatable. \n\n%matplotlib widget \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('https://raw.githubusercontent.com/prof-tcsmith/data/master/RidingMowers.csv')\n\ndf.head(3)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\ndf.shape\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n#There are no missing values\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#Fixing them by using the dummy encoding\ndf = pd.get_dummies(df,\n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf.head(5)\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership_Owner==1]['Income'], \n           df.loc[df.Ownership_Owner==1]['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership_Owner==0]['Income'], \n           df.loc[df.Ownership_Owner==0]['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \n\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \ncol_names_2b_transformed = df.columns.drop('Ownership_Owner')\ncol_names_2b_transformed\n\n\ntrain_data_scaled = df[col_names_2b_transformed]\nvalid_data_scaled = df[col_names_2b_transformed]\n\ntrain_data_scaled.head()\n\nscaler = StandardScaler()\ntrain_data_scaled = pd.DataFrame(scaler.fit_transform(train_data_scaled),\n                       index=train_data_scaled.index, columns=train_data_scaled.columns)\n\ntrain_data_scaled['Ownership_Owner'] = df['Ownership_Owner']\n\ntrain_data_scaled.head()\n\nfeatures = df.drop(columns=['Ownership_Owner'])\ntarget = df['Ownership_Owner']\n\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,  # increase the number of iterations\n    n_jobs=-1       # use all processors\n)\nmodel.fit(features, target)\n\nresults = pd.DataFrame()\nresults['actual'] = target\nresults['predicted'] = model.predict(features)\n\n# display the confusion matrix\ncm = confusion_matrix(results['actual'], results['predicted'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\n# - Precision\n# - Recall\n# - F1 Score\nprint(accuracy_score(target, model.predict(features)))\nprint(precision_score(target, model.predict(features)))\nprint(recall_score(target, model.predict(features))) \nprint(f1_score(target, model.predict(features)))\n\n\n# ## Step 6: Discuss the Results\n# Here the accuracy,precision,recall and f1 scores are same. From the confusion matrix the true positive and true negative values are same and the false positive,false negative values are same. It suggests that model is good at identifying the true positive and true negative and equally bad at the false positive and false negative.\n",
        "# # Predictive Modeling Example\n# **Week04, Logistic Example Model Fitting**\n# ISM6136\n# &copy; 2023 Dr. Tim Smith\n# ---\n# ## Step 1: Import the libraries we will use in this notebook\n# import neccessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom matplotlib import pyplot as plt\n\n# set the random seed\nnp.random.seed(42) # set this to ensure the results are repeatable. \n\n# ## Step 2: Load the given data that we will model\ndf = pd.read_csv('/Users/Dell/Desktop/Datamining/riding_mowers.csv')\n\ndf.head(20)\n\n# ## Step 3: Prepare the Data\n# determine the number of rows and columns\nrows = df.shape[0]\ncols = df.shape[1]\nprint(f\"Rows={rows} and Cols={cols}\")\n\n\n# check to see if there is any missing values, and if so, then fix this issue.\ndf.isna().sum()\n\n# check to see if there are any topos, and if so, then fix this issue.\ndf['Ownership'].unique()\n\n#df.replace(['nonowner'], 'Nonowner' ) \ndf['Ownership'] = df['Ownership'].replace({'owners': 'Owner', 'nonowner': 'Nonowner'})\nprint(df['Ownership'].unique())\n\n# visualize the data\n \nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(df.loc[df.Ownership=='Owner']['Income'], \n           df.loc[df.Ownership=='Owner']['Lot_Size'], \n           marker='o', \n           color='red')\nax.scatter(df.loc[df.Ownership=='Nonowner']['Income'], \n           df.loc[df.Ownership=='Nonowner']['Lot_Size'], \n           marker='o', \n           color='blue')\n\nax.legend([\"Owner\", \"Nonowner\"], framealpha=0.5)\n                                         \nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\n\ndf = pd.get_dummies(\n    df, \n    prefix_sep='_', \n    dummy_na=False, \n    drop_first=True, \n    columns=['Ownership'], \n    dtype='int32'\n)\n\ndf\n\n# ## Step 4: Create a Classifier\n# Since the number of observations is low, you will alocate all of the data to training. Normally, you would split the data into training and testing sets.\n# standardize the input data \nX = df[['Income', 'Lot_Size']]\ny = df['Ownership_Owner']\n\n# Use LogisticRegression from sklearn.linear_model to create a predictive model. \nmodel = LogisticRegression(\n    max_iter=1000,\n    n_jobs=-1\n)\n\nmodel.fit(X, y)\n\nresults = pd.DataFrame()\nresults['actual'] = y\nresults['predicted'] = model.predict(X)\nresults\n\ncm = confusion_matrix(results['actual'], results['predicted'])# display the confusion matrix\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nConfusionMatrixDisplay.from_predictions(\n    results['actual'], results['predicted'], display_labels=model.classes_, ax=ax, colorbar=False, cmap=plt.cm.Blues\n)\n\nplt.show()\n\n# ## Step 5: Analyze the linear regression model's performance\n# Record the models performance using the following metrics:\n# - Accuracy\nprint(accuracy_score(y, model.predict(X)))\n\n# - Precision\nprint(precision_score(y, model.predict(X), pos_label=1))\n\n# - Recall\nprint(recall_score(y, model.predict(X), pos_label=1))\n\n# - F1 Score\nprint(f1_score(y, model.predict(X), pos_label=1))\n\n\n\n\n# ## Step 6: Discuss the Results\n# I have imported necessary libraries and the dataset, 'riding_mowers.csv', is loaded into a Data Frame (df) from a CSV file, containing 24 rows and 6 columns, and no missing values are found.\n# The 'Ownership' column is transformed, renaming 'owners' to 'Owner' and 'nonowner' to 'Nonowner'. Data is visualized using a scatter plot and one-hot encoding applied to the 'Ownership' column. A logistic regression model predicts 'Ownership' based on income and Lot Size, using the entire dataset for trained and testing.\n# Using the number of True Positives (TP) at 10, True Negatives (TN) at 2, False Positives (FP) at 10, and False Negatives (FN) at 2, the Confusion Matrix indicates the model's results.\n# The linear regression model's performance metrics include accuracy, precision, recall, and F1 score, which are approximately 83.33%.\n"
    ],
    "PERCENTAGE_GRADE": [
        60.06641365502413,
        60.97059931342359,
        99.59416724306058,
        65.54842407599764,
        87.725748851777,
        77.70207707316395,
        70.79404797456908,
        74.73969528585346,
        68.79805282825097,
        93.3750123533523,
        79.10158936861808,
        66.22031041716295,
        72.20626862221987,
        92.65151952772729,
        80.36132961593626,
        74.43033924132992,
        95.56510685893606,
        84.81116793579407,
        88.98272439968082,
        76.92041735043148,
        67.24909204502212,
        79.56681900670672,
        65.59459388943463,
        84.39327788574624,
        83.29700460368954,
        72.51431994921444,
        90.41162115522744,
        78.50923124198836,
        71.48951948802662,
        81.97140719748091,
        62.8975786972081,
        70.631466424269,
        65.85939057204652,
        73.23576387037856,
        67.30580751897591,
        75.32215667721873,
        91.09261448289341,
        68.18505603089118,
        94.06463632366624,
        81.5481871844536,
        66.49122481504246,
        68.44825676625743,
        87.38242216215161,
        64.77864650454626
    ],
    "COMMENTS": [
        "Random comment for Student ID: 5007507",
        "Random comment for Student ID: 160259344",
        "Random comment for Student ID: 160253344",
        "Random comment for Student ID: 160254036",
        "Random comment for Student ID: 160270318",
        "Random comment for Student ID: 160263268",
        "Random comment for Student ID: 4882785",
        "Random comment for Student ID: 160189646",
        "Random comment for Student ID: 160276660",
        "Random comment for Student ID: 160274116",
        "Random comment for Student ID: 160266292",
        "Random comment for Student ID: 160272512",
        "Random comment for Student ID: 160261604",
        "Random comment for Student ID: 160271550",
        "Random comment for Student ID: 160271062",
        "Random comment for Student ID: 160270806",
        "Random comment for Student ID: 160196434",
        "Random comment for Student ID: 160273704",
        "Random comment for Student ID: 160271780",
        "Random comment for Student ID: 160067740",
        "Random comment for Student ID: 160244266",
        "Random comment for Student ID: 160259148",
        "Random comment for Student ID: 4976197",
        "Random comment for Student ID: 160258444",
        "Random comment for Student ID: 160247166",
        "Random comment for Student ID: 160181742",
        "Random comment for Student ID: 5084803",
        "Random comment for Student ID: 160261394",
        "Random comment for Student ID: 159987052",
        "Random comment for Student ID: 5187493",
        "Random comment for Student ID: 160278322",
        "Random comment for Student ID: 160274328",
        "Random comment for Student ID: 5120719",
        "Random comment for Student ID: 160275402",
        "Random comment for Student ID: 160275192",
        "Random comment for Student ID: 160240660",
        "Random comment for Student ID: 160265552",
        "Random comment for Student ID: 160263772",
        "Random comment for Student ID: 160251008",
        "Random comment for Student ID: 160269432",
        "Random comment for Student ID: 5120275",
        "Random comment for Student ID: 160196312",
        "Random comment for Student ID: 160273100",
        "Random comment for Student ID: 160266936"
    ]
}